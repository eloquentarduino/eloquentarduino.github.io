{
    "version": "https://jsonfeed.org/version/1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://eloquentarduino.github.io/category/programming/feed/json/ -- and add it your reader.",
    "home_page_url": "https://eloquentarduino.github.io/category/programming/",
    "feed_url": "https://eloquentarduino.github.io/category/programming/feed/json/",
    "title": "Eloquent Arduino Blog",
    "description": "Machine learning on Arduino, programming &amp; electronics",
    "items": [
        {
            "id": "https://eloquentarduino.github.io/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board/",
            "url": "https://eloquentarduino.github.io/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board/",
            "title": "How to train a color classification Machine learning classifier directly on your Arduino board",
            "content_html": "<p>In the <a href=\"/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board\">previous post</a> we learnt it is possible to train a Machine learning classifier directly on a microcontroller. In this post we'll look into how to do it to classify colors.</p>\n<p><span id=\"more-988\"></span></p>\n<p>This will be an hands-on guide, so let's walk throughout each step you need to complete to run the example. </p>\n<p>I setup this very example as a basis for your future projects, so you can easily swap the color classification task for any other one you could think of.</p>\n<h2>Definitions</h2>\n<pre><code class=\"language-c\">#ifdef ESP32\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n#define abs(x) ((x) &gt; 0 ? (x) : -(x))\n#endif\n\n#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;RGB.h&quot;\n\n#define MAX_TRAINING_SAMPLES 20\n#define FEATURES_DIM 3\n\nint numSamples;\nRGB rgb(2, 3, 4);\nfloat X_train[MAX_TRAINING_SAMPLES][FEATURES_DIM];\nint y_train[MAX_TRAINING_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);</code></pre>\n<p>When training a classifier on your microcontroller there are some things that are mandatory:</p>\n<ol>\n<li><code>#include &lt;EloquentSVMSMO.h&gt;</code>: this is the library that implements the SVM learning algorithm</li>\n<li><code>X_train</code>: this is a matrix where each row represents a training sample. You will need to keep this data always with you, since it's required also during the inference</li>\n<li><code>y_train</code>: this array contains, for each training sample, the class it belongs to: 1 or -1</li>\n<li><code>linearKernel</code>: this is the kernel function for the SVM classifier (you can read more <a href=\"#\">here</a>). You can pass your own kernel other than linear (for example <code>poly</code> or <code>rbf</code>)</li>\n</ol>\n<p>In this specific example, we're using the <code>RGB</code> class to handle the TCS3200 sensor reading, but this will change based on the dataset you want to train on. Also, since our features are going to be the R, G and B components of a color, <code>FEATURES_DIM</code> is set to 3.</p>\n<h2>Setup</h2>\n<pre><code class=\"language-c\">void setup() {\n    Serial.begin(115200);\n    rgb.begin();\n\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}</code></pre>\n<p>The setup does not contain any logic really. You can use this part to configure the parameters of the classifier:</p>\n<ul>\n<li><code>C</code>: &quot;The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points&quot; (quoted from <a href=\"https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\">stackexchange</a>)</li>\n<li><code>tol</code>: &quot;The tol parameter is a setting for the SVM's tolerance in optimization. Recall that <code>yi(xi.w+b)-1 &gt;= 0</code>. For an SVM to be valid, all values must be greater than or equal to 0, and at least one value on each side needs to be &quot;equal&quot; to 0, which will be your support vectors. Since it is highly unlikely that you will actually get values equal perfectly to 0, you set tolerance to allow a bit of wiggle room.&quot; (quoted from <a href=\"https://pythonprogramming.net/support-vector-machine-parameters-machine-learning-tutorial/\">pythonprogramming</a>)</li>\n<li><code>maxIter</code>: set an upper bound to the number of iterations the algorithm can take to converge</li>\n<li><code>passes</code>: max # of times to iterate over \u03b1\u2019s without changing</li>\n<li><code>alphaTol</code>: alfpha coefficients determine which samples from the training set are to be considered support vectors and so be included during the inference procedure. This value discards support vectors with an alpha too small to be noticeable.</li>\n</ul>\n<h2>Fit</h2>\n<pre><code class=\"language-c\">else if (command == &quot;fit&quot;) {\n        Serial.print(&quot;How many samples will you record? &quot;);\n        numSamples = readSerialNumber();\n\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(i + 1);\n            Serial.print(&quot;/&quot;);\n            Serial.print(numSamples);\n            Serial.println(&quot; Which class does the sample belongs to, 1 or -1?&quot;);\n            y_train[i] = readSerialNumber() &gt; 0 ? 1 : -1;\n            getFeatures(X_train[i]);\n        }\n\n        Serial.print(&quot;Start training... &quot;);\n        classifier.fit(X_train, y_train, numSamples);\n        Serial.println(&quot;Done&quot;);\n    }</code></pre>\n<p>This is the core of the project. Here we are loading the samples to train our classifier &quot;live&quot; on the board.</p>\n<p>Since this is an interactive demo, the program prompts us to define how many samples we'll load and, one by one, which class they belong to.</p>\n<p>Now there are a few important things to keep in mind:</p>\n<ul>\n<li><code>numSamples</code>: sadly, C has no easy way to know the size of an array, so we have to be explicit about it. To train the classifier, it is mandatory that you do know I many samples you're passing to it</li>\n<li><code>getFeatures()</code> is the function that reads the training sample. It is actually a &quot;proxy&quot; to your own custom logic: in this example it reads the TCS3200, in your project it could read an accelerometer or the like.</li>\n<li><code>fit()</code>: this is where the magic happens. With this single line of code you're training the SVM on the training data; when the functions ends, the classifier will have updated its internal state with the coefficients it needs to classify new samples</li>\n</ul>\n<h2>Predict</h2>\n<p>ColorClassificationTrainingExample.ino</p>\n<pre><code class=\"language-c\">else if (command == &quot;predict&quot;) {\n        int label;\n        float x[FEATURES_DIM];\n\n        getFeatures(x);\n        Serial.print(&quot;Predicted label is &quot;);\n        Serial.println(classifier.predict(X_train, x));\n    }</code></pre>\n<p>Now that our classifier has been trained, we can finally make use of it to classify new samples.</p>\n<p>As easy as it can be, you just call its <code>predict</code> method.</p>\n<div class=\"infobox\">As you can see, the <code>predict</code> method requires the <code>X_train</code> matrix other than the new sample vector</div>\n<p>And that's it: you can now complete your Machine learning task on your microcontroller from start to end, without the need of a PC.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<hr />\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/ColorClassificationTrainingExample/ColorClassificationTrainingExample.ino\">Github</a></p>\n<hr />\n<h2>Full example</h2>\n<p>ColorClassificationTrainingExample.ino</p>\n<pre><code class=\"language-c\">#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;RGB.h&quot;\n\n#define MAX_TRAINING_SAMPLES 20\n#define FEATURES_DIM 3\n\nint numSamples;\nRGB rgb(2, 3, 4);\nfloat X_train[MAX_TRAINING_SAMPLES][FEATURES_DIM];\nint y_train[MAX_TRAINING_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);\n\nvoid setup() {\n    Serial.begin(115200);\n    rgb.begin();\n\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}\n\nvoid loop() {\n    if (!Serial.available()) {\n        delay(100);\n        return;\n    }\n\n    String command = Serial.readStringUntil(&#039;\\n&#039;);\n\n    if (command == &quot;help&quot;) {\n        Serial.println(&quot;Available commands:&quot;);\n        Serial.println(&quot;\\tfit: train the classifier on a new set of samples&quot;);\n        Serial.println(&quot;\\tpredict: classify a new sample&quot;);\n        Serial.println(&quot;\\tinspect: print X_train and y_train&quot;);\n    }\n    else if (command == &quot;fit&quot;) {\n        Serial.print(&quot;How many samples will you record? &quot;);\n        numSamples = readSerialNumber();\n\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(i + 1);\n            Serial.print(&quot;/&quot;);\n            Serial.print(numSamples);\n            Serial.println(&quot; Which class does the sample belongs to, 1 or -1?&quot;);\n            y_train[i] = readSerialNumber() &gt; 0 ? 1 : -1;\n            getFeatures(X_train[i]);\n        }\n\n        Serial.print(&quot;Start training... &quot;);\n        classifier.fit(X_train, y_train, numSamples);\n        Serial.println(&quot;Done&quot;);\n    }\n    else if (command == &quot;predict&quot;) {\n        int label;\n        float x[FEATURES_DIM];\n\n        getFeatures(x);\n        Serial.print(&quot;Predicted label is &quot;);\n        Serial.println(classifier.predict(X_train, x));\n    }\n    else if (command == &quot;inspect&quot;) {\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(&quot;[&quot;);\n            Serial.print(y_train[i]);\n            Serial.print(&quot;] &quot;);\n\n            for (int j = 0; j &lt; FEATURES_DIM; j++) {\n                Serial.print(X_train[i][j]);\n                Serial.print(&quot;, &quot;);\n            }\n\n            Serial.println();\n        }\n    }\n}\n\n/**\n *\n * @return\n */\nint readSerialNumber() {\n    while (!Serial.available()) delay(1);\n\n    return Serial.readStringUntil(&#039;\\n&#039;).toInt();\n}\n\n/**\n * Get features for new sample\n * @param x\n */\nvoid getFeatures(float x[FEATURES_DIM]) {\n    rgb.read(x);\n\n    for (int i = 0; i &lt; FEATURES_DIM; i++) {\n        Serial.print(x[i]);\n        Serial.print(&quot;, &quot;);\n    }\n\n    Serial.println();\n}</code></pre>\n<p>RGB.h</p>\n<pre><code class=\"language-c\">#pragma once\n\n/**\n * Wrapper for RGB color sensor\n */\nclass RGB {\n    public:\n        RGB(uint8_t s2, uint8_t s3, uint8_t out) :\n            _s2(s2),\n            _s3(s3),\n            _out(out) {\n\n        }\n\n        /**\n         *\n         */\n        void begin() {\n            pinMode(_s2, OUTPUT);\n            pinMode(_s3, OUTPUT);\n            pinMode(_out, INPUT);\n        }\n\n        /**\n         *\n         * @param x\n         */\n        void read(float x[3]) {\n            x[0] = readComponent(LOW, LOW);\n            x[1] = readComponent(HIGH, HIGH);\n            x[2] = readComponent(LOW, HIGH);\n        }\n\n    protected:\n        uint8_t _s2;\n        uint8_t _s3;\n        uint8_t _out;\n\n        /**\n         *\n         * @param s2\n         * @param s3\n         * @return\n         */\n        int readComponent(bool s2, bool s3) {\n            delay(10);\n            digitalWrite(_s2, s2);\n            digitalWrite(_s3, s3);\n\n            return pulseIn(_out, LOW);\n        }\n};</code></pre>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board/\">How to train a color classification Machine learning classifier directly on your Arduino board</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In the previous post we learnt it is possible to train a Machine learning classifier directly on a microcontroller. In this post we'll look into how to do it to classify colors.\n\nThis will be an hands-on guide, so let's walk throughout each step you need to complete to run the example. \nI setup this very example as a basis for your future projects, so you can easily swap the color classification task for any other one you could think of.\nDefinitions\n#ifdef ESP32\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n#define abs(x) ((x) &gt; 0 ? (x) : -(x))\n#endif\n\n#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;RGB.h&quot;\n\n#define MAX_TRAINING_SAMPLES 20\n#define FEATURES_DIM 3\n\nint numSamples;\nRGB rgb(2, 3, 4);\nfloat X_train[MAX_TRAINING_SAMPLES][FEATURES_DIM];\nint y_train[MAX_TRAINING_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);\nWhen training a classifier on your microcontroller there are some things that are mandatory:\n\n#include &lt;EloquentSVMSMO.h&gt;: this is the library that implements the SVM learning algorithm\nX_train: this is a matrix where each row represents a training sample. You will need to keep this data always with you, since it's required also during the inference\ny_train: this array contains, for each training sample, the class it belongs to: 1 or -1\nlinearKernel: this is the kernel function for the SVM classifier (you can read more here). You can pass your own kernel other than linear (for example poly or rbf)\n\nIn this specific example, we're using the RGB class to handle the TCS3200 sensor reading, but this will change based on the dataset you want to train on. Also, since our features are going to be the R, G and B components of a color, FEATURES_DIM is set to 3.\nSetup\nvoid setup() {\n    Serial.begin(115200);\n    rgb.begin();\n\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}\nThe setup does not contain any logic really. You can use this part to configure the parameters of the classifier:\n\nC: &quot;The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points&quot; (quoted from stackexchange)\ntol: &quot;The tol parameter is a setting for the SVM's tolerance in optimization. Recall that yi(xi.w+b)-1 &gt;= 0. For an SVM to be valid, all values must be greater than or equal to 0, and at least one value on each side needs to be &quot;equal&quot; to 0, which will be your support vectors. Since it is highly unlikely that you will actually get values equal perfectly to 0, you set tolerance to allow a bit of wiggle room.&quot; (quoted from pythonprogramming)\nmaxIter: set an upper bound to the number of iterations the algorithm can take to converge\npasses: max # of times to iterate over \u03b1\u2019s without changing\nalphaTol: alfpha coefficients determine which samples from the training set are to be considered support vectors and so be included during the inference procedure. This value discards support vectors with an alpha too small to be noticeable.\n\nFit\nelse if (command == &quot;fit&quot;) {\n        Serial.print(&quot;How many samples will you record? &quot;);\n        numSamples = readSerialNumber();\n\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(i + 1);\n            Serial.print(&quot;/&quot;);\n            Serial.print(numSamples);\n            Serial.println(&quot; Which class does the sample belongs to, 1 or -1?&quot;);\n            y_train[i] = readSerialNumber() &gt; 0 ? 1 : -1;\n            getFeatures(X_train[i]);\n        }\n\n        Serial.print(&quot;Start training... &quot;);\n        classifier.fit(X_train, y_train, numSamples);\n        Serial.println(&quot;Done&quot;);\n    }\nThis is the core of the project. Here we are loading the samples to train our classifier &quot;live&quot; on the board.\nSince this is an interactive demo, the program prompts us to define how many samples we'll load and, one by one, which class they belong to.\nNow there are a few important things to keep in mind:\n\nnumSamples: sadly, C has no easy way to know the size of an array, so we have to be explicit about it. To train the classifier, it is mandatory that you do know I many samples you're passing to it\ngetFeatures() is the function that reads the training sample. It is actually a &quot;proxy&quot; to your own custom logic: in this example it reads the TCS3200, in your project it could read an accelerometer or the like.\nfit(): this is where the magic happens. With this single line of code you're training the SVM on the training data; when the functions ends, the classifier will have updated its internal state with the coefficients it needs to classify new samples\n\nPredict\nColorClassificationTrainingExample.ino\nelse if (command == &quot;predict&quot;) {\n        int label;\n        float x[FEATURES_DIM];\n\n        getFeatures(x);\n        Serial.print(&quot;Predicted label is &quot;);\n        Serial.println(classifier.predict(X_train, x));\n    }\nNow that our classifier has been trained, we can finally make use of it to classify new samples.\nAs easy as it can be, you just call its predict method.\nAs you can see, the predict method requires the X_train matrix other than the new sample vector\nAnd that's it: you can now complete your Machine learning task on your microcontroller from start to end, without the need of a PC.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n\nCheck the full project code on Github\n\nFull example\nColorClassificationTrainingExample.ino\n#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;RGB.h&quot;\n\n#define MAX_TRAINING_SAMPLES 20\n#define FEATURES_DIM 3\n\nint numSamples;\nRGB rgb(2, 3, 4);\nfloat X_train[MAX_TRAINING_SAMPLES][FEATURES_DIM];\nint y_train[MAX_TRAINING_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);\n\nvoid setup() {\n    Serial.begin(115200);\n    rgb.begin();\n\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}\n\nvoid loop() {\n    if (!Serial.available()) {\n        delay(100);\n        return;\n    }\n\n    String command = Serial.readStringUntil(&#039;\\n&#039;);\n\n    if (command == &quot;help&quot;) {\n        Serial.println(&quot;Available commands:&quot;);\n        Serial.println(&quot;\\tfit: train the classifier on a new set of samples&quot;);\n        Serial.println(&quot;\\tpredict: classify a new sample&quot;);\n        Serial.println(&quot;\\tinspect: print X_train and y_train&quot;);\n    }\n    else if (command == &quot;fit&quot;) {\n        Serial.print(&quot;How many samples will you record? &quot;);\n        numSamples = readSerialNumber();\n\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(i + 1);\n            Serial.print(&quot;/&quot;);\n            Serial.print(numSamples);\n            Serial.println(&quot; Which class does the sample belongs to, 1 or -1?&quot;);\n            y_train[i] = readSerialNumber() &gt; 0 ? 1 : -1;\n            getFeatures(X_train[i]);\n        }\n\n        Serial.print(&quot;Start training... &quot;);\n        classifier.fit(X_train, y_train, numSamples);\n        Serial.println(&quot;Done&quot;);\n    }\n    else if (command == &quot;predict&quot;) {\n        int label;\n        float x[FEATURES_DIM];\n\n        getFeatures(x);\n        Serial.print(&quot;Predicted label is &quot;);\n        Serial.println(classifier.predict(X_train, x));\n    }\n    else if (command == &quot;inspect&quot;) {\n        for (int i = 0; i &lt; numSamples; i++) {\n            Serial.print(&quot;[&quot;);\n            Serial.print(y_train[i]);\n            Serial.print(&quot;] &quot;);\n\n            for (int j = 0; j &lt; FEATURES_DIM; j++) {\n                Serial.print(X_train[i][j]);\n                Serial.print(&quot;, &quot;);\n            }\n\n            Serial.println();\n        }\n    }\n}\n\n/**\n *\n * @return\n */\nint readSerialNumber() {\n    while (!Serial.available()) delay(1);\n\n    return Serial.readStringUntil(&#039;\\n&#039;).toInt();\n}\n\n/**\n * Get features for new sample\n * @param x\n */\nvoid getFeatures(float x[FEATURES_DIM]) {\n    rgb.read(x);\n\n    for (int i = 0; i &lt; FEATURES_DIM; i++) {\n        Serial.print(x[i]);\n        Serial.print(&quot;, &quot;);\n    }\n\n    Serial.println();\n}\nRGB.h\n#pragma once\n\n/**\n * Wrapper for RGB color sensor\n */\nclass RGB {\n    public:\n        RGB(uint8_t s2, uint8_t s3, uint8_t out) :\n            _s2(s2),\n            _s3(s3),\n            _out(out) {\n\n        }\n\n        /**\n         *\n         */\n        void begin() {\n            pinMode(_s2, OUTPUT);\n            pinMode(_s3, OUTPUT);\n            pinMode(_out, INPUT);\n        }\n\n        /**\n         *\n         * @param x\n         */\n        void read(float x[3]) {\n            x[0] = readComponent(LOW, LOW);\n            x[1] = readComponent(HIGH, HIGH);\n            x[2] = readComponent(LOW, HIGH);\n        }\n\n    protected:\n        uint8_t _s2;\n        uint8_t _s3;\n        uint8_t _out;\n\n        /**\n         *\n         * @param s2\n         * @param s3\n         * @return\n         */\n        int readComponent(bool s2, bool s3) {\n            delay(10);\n            digitalWrite(_s2, s2);\n            digitalWrite(_s3, s3);\n\n            return pulseIn(_out, LOW);\n        }\n};\nL'articolo How to train a color classification Machine learning classifier directly on your Arduino board proviene da Eloquent Arduino Blog.",
            "date_published": "2020-03-28T20:02:53+01:00",
            "date_modified": "2020-03-29T13:37:01+02:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning",
                "Programming"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/",
            "url": "https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/",
            "title": "How to train a IRIS classification Machine learning classifier directly on your Arduino board",
            "content_html": "<p>In this hands-on guide about <a href=\"/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board\">on-board SVM training</a> we're going to see a classifier in action, training it on the Iris dataset and evaluating its performance.</p>\n<p><span id=\"more-1008\"></span></p>\n<h2>What we'll make</h2>\n<p>In this demo project we're going to take a know dataset (<a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">iris flowers</a>) and interactively train an SVM classifier on it, adjusting the number of samples to see the effects on both training time, inference time and accuracy.</p>\n<h2>Definitions</h2>\n<pre><code class=\"language-c\">#ifdef ESP32\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n#define abs(x) ((x) &gt; 0 ? (x) : -(x))\n#endif\n\n#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;iris.h&quot;\n\n#define TOTAL_SAMPLES (POSITIVE_SAMPLES + NEGATIVE_SAMPLES)\n\nfloat X_train[TOTAL_SAMPLES][FEATURES_DIM];\nfloat X_test[TOTAL_SAMPLES][FEATURES_DIM];\nint y_train[TOTAL_SAMPLES];\nint y_test[TOTAL_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);</code></pre>\n<p>First of all we need to include a couple files, namely <code>EloquentSVMSMO.h</code> for the SVM classifier and <code>iris.h</code> for the dataset.</p>\n<p><code>iris.h</code> defines a couple constants:</p>\n<ul>\n<li><code>FEATURES_DIM</code>: the number of features each sample has (4 in this case)</li>\n<li><code>POSITIVE_SAMPLES</code>: the number of samples that belong to the positive class (50)</li>\n<li><code>NEGATIVE_SAMPLES</code>: the number of samples that belong to the negative class (50)</li>\n</ul>\n<p>The we declare the array that hold the data: <code>X_train</code> and <code>y_train</code> for the training process, <code>X_test</code> and <code>y_test</code> for the inference process.</p>\n<h2>Setup</h2>\n<pre><code class=\"language-c\">void setup() {\n    Serial.begin(115200);\n    delay(5000);\n\n    // configure classifier\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}</code></pre>\n<p>Here we just set a few parameters for the classifier. You could actually skip this step in this demo, since the defaults will work well. Those lines are there so you know you can tweak them, if needed.</p>\n<p>Please refer to the <a href=\"/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board\">demo for color classification</a> for an explanation of each parameter.</p>\n<h2>Interactivity</h2>\n<pre><code class=\"language-c\">void loop() {\n    int positiveSamples = readSerialNumber(&quot;How many positive samples will you use for training? &quot;, POSITIVE_SAMPLES);\n\n    if (positiveSamples &gt; POSITIVE_SAMPLES - 1) {\n        Serial.println(&quot;Too many positive samples entered. All but one will be used instead&quot;);\n        positiveSamples = POSITIVE_SAMPLES - 1;\n    }\n\n    int negativeSamples = readSerialNumber(&quot;How many negative samples will you use for training? &quot;, NEGATIVE_SAMPLES);\n\n    if (negativeSamples &gt; NEGATIVE_SAMPLES - 1) {\n        Serial.println(&quot;Too many negative samples entered. All but one will be used instead&quot;);\n        negativeSamples = NEGATIVE_SAMPLES - 1;\n    }\n\n    loadDataset(positiveSamples, negativeSamples);\n\n    // ...\n}\n\n/**\n * Ask the user to enter a numeric value\n */\nint readSerialNumber(String prompt, int maxAllowed) {\n    Serial.print(prompt);\n    Serial.print(&quot; (&quot;);\n    Serial.print(maxAllowed);\n    Serial.print(&quot; max) &quot;);\n\n    while (!Serial.available()) delay(1);\n\n    int n = Serial.readStringUntil(&#039;\\n&#039;).toInt();\n\n    Serial.println(n);\n\n    return n;\n}\n\n/**\n * Divide training and test data\n */\nvoid loadDataset(int positiveSamples, int negativeSamples) {\n    int positiveTestSamples = POSITIVE_SAMPLES - positiveSamples;\n\n    for (int i = 0; i &lt; positiveSamples; i++) {\n        memcpy(X_train[i], X_positive[i], FEATURES_DIM);\n        y_train[i] = 1;\n    }\n\n    for (int i = 0; i &lt; negativeSamples; i++) {\n        memcpy(X_train[i + positiveSamples], X_negative[i], FEATURES_DIM);\n        y_train[i + positiveSamples] = -1;\n    }\n\n    for (int i = 0; i &lt; positiveTestSamples; i++) {\n        memcpy(X_test[i], X_positive[i + positiveSamples], FEATURES_DIM);\n        y_test[i] = 1;\n    }\n\n    for (int i = 0; i &lt; NEGATIVE_SAMPLES - negativeSamples; i++) {\n        memcpy(X_test[i + positiveTestSamples], X_negative[i + negativeSamples], FEATURES_DIM);\n        y_test[i + positiveTestSamples] = -1;\n    }\n}</code></pre>\n<p>The code above is a preliminary step where you're asked to enter how many samples you will use for training of both positive and negative classes.</p>\n<p>This way you can have multiple run of benchmarking without the need to re-compile and re-upload the sketch.</p>\n<p>It also shows that the training process can be &quot;dynamic&quot;, in the sense that you can tweak it at runtime as per your need.</p>\n<h2>Training</h2>\n<pre><code class=\"language-c\">time_t start = millis();\nclassifier.fit(X_train, y_train, positiveSamples + negativeSamples);\nSerial.print(&quot;It took &quot;);\nSerial.print(millis() - start);\nSerial.print(&quot;ms to train on &quot;);\nSerial.print(positiveSamples + negativeSamples);\nSerial.println(&quot; samples&quot;);</code></pre>\n<p>Training is actually a one line operation. Here we'll also logging how much time it takes to train.</p>\n<h3>Predicting</h3>\n<pre><code class=\"language-c\">void loop() {\n    // ...\n\n    int tp = 0;\n    int tn = 0;\n    int fp = 0;\n    int fn = 0;\n\n    start = millis();\n\n    for (int i = 0; i &lt; TOTAL_SAMPLES - positiveSamples - negativeSamples; i++) {\n        int y_pred = classifier.predict(X_train, X_test[i]);\n        int y_true = y_test[i];\n\n        if (y_pred == y_true &amp;&amp; y_pred ==  1) tp += 1;\n        if (y_pred == y_true &amp;&amp; y_pred == -1) tn += 1;\n        if (y_pred != y_true &amp;&amp; y_pred ==  1) fp += 1;\n        if (y_pred != y_true &amp;&amp; y_pred == -1) fn += 1;\n    }\n\n    Serial.print(&quot;It took &quot;);\n    Serial.print(millis() - start);\n    Serial.print(&quot;ms to test on &quot;);\n    Serial.print(TOTAL_SAMPLES - positiveSamples - negativeSamples);\n    Serial.println(&quot; samples&quot;);\n\n    printConfusionMatrix(tp, tn, fp, fn);\n}\n\n/**\n * Dump confusion matrix to Serial monitor\n */\nvoid printConfusionMatrix(int tp, int tn, int fp, int fn) {\n    Serial.print(&quot;Overall accuracy &quot;);\n    Serial.print(100.0 * (tp + tn) / (tp + tn + fp + fn));\n    Serial.println(&quot;%&quot;);\n    Serial.println(&quot;Confusion matrix&quot;);\n    Serial.print(&quot;          | Predicted 1 | Predicted -1 |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n&quot;);\n    Serial.print(&quot;Actual  1 |      &quot;);\n    Serial.print(tp);\n    Serial.print(&quot;     |      &quot;);\n    Serial.print(fn);\n    Serial.print(&quot;       |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n&quot;);\n    Serial.print(&quot;Actual -1 |      &quot;);\n    Serial.print(fp);\n    Serial.print(&quot;      |      &quot;);\n    Serial.print(tn);\n    Serial.print(&quot;       |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n\\n\\n&quot;);\n}</code></pre>\n<p>Finally we can run the classification on our test set and get the overall accuracy.</p>\n<p>We also print the <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> to double-check each class accuracy.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<hr />\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/IrisClassificationTrainingExample/IrisClassificationTrainingExample.ino\">Github</a> where you'll also find another dataset to test, which is characterized by a number of features much higher (30 instead of 4).</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/\">How to train a IRIS classification Machine learning classifier directly on your Arduino board</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this hands-on guide about on-board SVM training we're going to see a classifier in action, training it on the Iris dataset and evaluating its performance.\n\nWhat we'll make\nIn this demo project we're going to take a know dataset (iris flowers) and interactively train an SVM classifier on it, adjusting the number of samples to see the effects on both training time, inference time and accuracy.\nDefinitions\n#ifdef ESP32\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n#define abs(x) ((x) &gt; 0 ? (x) : -(x))\n#endif\n\n#include &lt;EloquentSVMSMO.h&gt;\n#include &quot;iris.h&quot;\n\n#define TOTAL_SAMPLES (POSITIVE_SAMPLES + NEGATIVE_SAMPLES)\n\nfloat X_train[TOTAL_SAMPLES][FEATURES_DIM];\nfloat X_test[TOTAL_SAMPLES][FEATURES_DIM];\nint y_train[TOTAL_SAMPLES];\nint y_test[TOTAL_SAMPLES];\nEloquent::TinyML::SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);\nFirst of all we need to include a couple files, namely EloquentSVMSMO.h for the SVM classifier and iris.h for the dataset.\niris.h defines a couple constants:\n\nFEATURES_DIM: the number of features each sample has (4 in this case)\nPOSITIVE_SAMPLES: the number of samples that belong to the positive class (50)\nNEGATIVE_SAMPLES: the number of samples that belong to the negative class (50)\n\nThe we declare the array that hold the data: X_train and y_train for the training process, X_test and y_test for the inference process.\nSetup\nvoid setup() {\n    Serial.begin(115200);\n    delay(5000);\n\n    // configure classifier\n    classifier.setC(5);\n    classifier.setTol(1e-5);\n    classifier.setMaxIter(10000);\n}\nHere we just set a few parameters for the classifier. You could actually skip this step in this demo, since the defaults will work well. Those lines are there so you know you can tweak them, if needed.\nPlease refer to the demo for color classification for an explanation of each parameter.\nInteractivity\nvoid loop() {\n    int positiveSamples = readSerialNumber(&quot;How many positive samples will you use for training? &quot;, POSITIVE_SAMPLES);\n\n    if (positiveSamples &gt; POSITIVE_SAMPLES - 1) {\n        Serial.println(&quot;Too many positive samples entered. All but one will be used instead&quot;);\n        positiveSamples = POSITIVE_SAMPLES - 1;\n    }\n\n    int negativeSamples = readSerialNumber(&quot;How many negative samples will you use for training? &quot;, NEGATIVE_SAMPLES);\n\n    if (negativeSamples &gt; NEGATIVE_SAMPLES - 1) {\n        Serial.println(&quot;Too many negative samples entered. All but one will be used instead&quot;);\n        negativeSamples = NEGATIVE_SAMPLES - 1;\n    }\n\n    loadDataset(positiveSamples, negativeSamples);\n\n    // ...\n}\n\n/**\n * Ask the user to enter a numeric value\n */\nint readSerialNumber(String prompt, int maxAllowed) {\n    Serial.print(prompt);\n    Serial.print(&quot; (&quot;);\n    Serial.print(maxAllowed);\n    Serial.print(&quot; max) &quot;);\n\n    while (!Serial.available()) delay(1);\n\n    int n = Serial.readStringUntil(&#039;\\n&#039;).toInt();\n\n    Serial.println(n);\n\n    return n;\n}\n\n/**\n * Divide training and test data\n */\nvoid loadDataset(int positiveSamples, int negativeSamples) {\n    int positiveTestSamples = POSITIVE_SAMPLES - positiveSamples;\n\n    for (int i = 0; i &lt; positiveSamples; i++) {\n        memcpy(X_train[i], X_positive[i], FEATURES_DIM);\n        y_train[i] = 1;\n    }\n\n    for (int i = 0; i &lt; negativeSamples; i++) {\n        memcpy(X_train[i + positiveSamples], X_negative[i], FEATURES_DIM);\n        y_train[i + positiveSamples] = -1;\n    }\n\n    for (int i = 0; i &lt; positiveTestSamples; i++) {\n        memcpy(X_test[i], X_positive[i + positiveSamples], FEATURES_DIM);\n        y_test[i] = 1;\n    }\n\n    for (int i = 0; i &lt; NEGATIVE_SAMPLES - negativeSamples; i++) {\n        memcpy(X_test[i + positiveTestSamples], X_negative[i + negativeSamples], FEATURES_DIM);\n        y_test[i + positiveTestSamples] = -1;\n    }\n}\nThe code above is a preliminary step where you're asked to enter how many samples you will use for training of both positive and negative classes.\nThis way you can have multiple run of benchmarking without the need to re-compile and re-upload the sketch.\nIt also shows that the training process can be &quot;dynamic&quot;, in the sense that you can tweak it at runtime as per your need.\nTraining\ntime_t start = millis();\nclassifier.fit(X_train, y_train, positiveSamples + negativeSamples);\nSerial.print(&quot;It took &quot;);\nSerial.print(millis() - start);\nSerial.print(&quot;ms to train on &quot;);\nSerial.print(positiveSamples + negativeSamples);\nSerial.println(&quot; samples&quot;);\nTraining is actually a one line operation. Here we'll also logging how much time it takes to train.\nPredicting\nvoid loop() {\n    // ...\n\n    int tp = 0;\n    int tn = 0;\n    int fp = 0;\n    int fn = 0;\n\n    start = millis();\n\n    for (int i = 0; i &lt; TOTAL_SAMPLES - positiveSamples - negativeSamples; i++) {\n        int y_pred = classifier.predict(X_train, X_test[i]);\n        int y_true = y_test[i];\n\n        if (y_pred == y_true &amp;&amp; y_pred ==  1) tp += 1;\n        if (y_pred == y_true &amp;&amp; y_pred == -1) tn += 1;\n        if (y_pred != y_true &amp;&amp; y_pred ==  1) fp += 1;\n        if (y_pred != y_true &amp;&amp; y_pred == -1) fn += 1;\n    }\n\n    Serial.print(&quot;It took &quot;);\n    Serial.print(millis() - start);\n    Serial.print(&quot;ms to test on &quot;);\n    Serial.print(TOTAL_SAMPLES - positiveSamples - negativeSamples);\n    Serial.println(&quot; samples&quot;);\n\n    printConfusionMatrix(tp, tn, fp, fn);\n}\n\n/**\n * Dump confusion matrix to Serial monitor\n */\nvoid printConfusionMatrix(int tp, int tn, int fp, int fn) {\n    Serial.print(&quot;Overall accuracy &quot;);\n    Serial.print(100.0 * (tp + tn) / (tp + tn + fp + fn));\n    Serial.println(&quot;%&quot;);\n    Serial.println(&quot;Confusion matrix&quot;);\n    Serial.print(&quot;          | Predicted 1 | Predicted -1 |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n&quot;);\n    Serial.print(&quot;Actual  1 |      &quot;);\n    Serial.print(tp);\n    Serial.print(&quot;     |      &quot;);\n    Serial.print(fn);\n    Serial.print(&quot;       |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n&quot;);\n    Serial.print(&quot;Actual -1 |      &quot;);\n    Serial.print(fp);\n    Serial.print(&quot;      |      &quot;);\n    Serial.print(tn);\n    Serial.print(&quot;       |\\n&quot;);\n    Serial.print(&quot;----------------------------------------\\n\\n\\n&quot;);\n}\nFinally we can run the classification on our test set and get the overall accuracy.\nWe also print the confusion matrix to double-check each class accuracy.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n\nCheck the full project code on Github where you'll also find another dataset to test, which is characterized by a number of features much higher (30 instead of 4).\nL'articolo How to train a IRIS classification Machine learning classifier directly on your Arduino board proviene da Eloquent Arduino Blog.",
            "date_published": "2020-03-28T19:02:09+01:00",
            "date_modified": "2020-03-29T13:33:45+02:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "svm",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board/",
            "url": "https://eloquentarduino.github.io/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board/",
            "title": "So you want to train an ML classifier directly on an Arduino board?",
            "content_html": "<p>As of now, we know it is possible to run Machine learning inference on tiny microcontrollers thanks to <a href=\"https://www.tensorflow.org/lite/microcontrollers\">Tensorflow for Micro</a> and my very own library <a href=\"https://github.com/eloquentarduino/micromlgen\">MicroML</a>. What if you could <strong>train</strong> a classifier directly on the microcontroller, too?</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/03/Onboard-IRIS-dataset-training-time.png\" alt=\"Onboard IRIS dataset training time\" /></p>\n<p><span id=\"more-982\"></span></p>\n<p>When I first started this journey in the world of Machine learning on microcontrollers, one fact was set in stone for me: you train your classifier once and for all on a PC, then deploy it to your microcontroller.</p>\n<p>As simple as this.</p>\n<p>Training is a heavy process, requires lots of computations and memory. You just want a machine as powerful as possible to carry out this task as fast as possible.</p>\n<p>Moreover, it is a one-time task: once your classifier has been trained, it needs not to be updated anymore.</p>\n<p>And this yield true until now.</p>\n<p>Until my reader <a href=\"https://github.com/joaocarvalhoopen\">Joao Carvalho</a>, in the comments on the post about <a href=\"/2020/02/even-smaller-machine-learning-models-for-your-mcu/\">an alternative to SVM which produces much smaller models</a> that I strongly invite you to read, challenged me with this idea of running the SVM training directly on the microcontroller. </p>\n<p>In the past I replied <em>&quot;No way&quot;</em> to people asking about this topic on forums, but Joao was so kind to link me a <a href=\"https://github.com/karpathy/svmjs/blob/master/lib/svm.js\">Javascript implementation</a> of the <a href=\"https://github.com/karpathy/svmjs/blob/master/test/smo.pdf\">simplified SVM SMO (Sequential Minimal Optimization) algorithm</a>.</p>\n<p>At a first glance it looked quite easy to port from Javascript to C, so I gave it a try.</p>\n<p>And in fact it only took me 30 minutes to get it working on my PC.</p>\n<p>Then I deployed it to my ESP32 and... <strong>it worked</strong>!</p>\n<p>My first try was with 10 samples from the the IRIS dataset: it only took almost no time to train.</p>\n<p>But I know SVM training and inferencing time grows rapidly with the number of training samples.</p>\n<p>Execution time will be the most limiting factor for this kind of task, so I created a benchmarking setup to evaluate the performance of the algorithm on different dataset sizes and features dimensions.<br />\nThe results are summarized in the following table and plots.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">Features size</th>\n<th style=\"text-align: center;\">Training size</th>\n<th style=\"text-align: center;\">Training time (ms)</th>\n<th style=\"text-align: center;\">Unit inference time (ms)</th>\n<th style=\"text-align: center;\">Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">0,011</td>\n<td style=\"text-align: center;\">80</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">0,013</td>\n<td style=\"text-align: center;\">85</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">11</td>\n<td style=\"text-align: center;\">0,014</td>\n<td style=\"text-align: center;\">90</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: center;\">38</td>\n<td style=\"text-align: center;\">0,017</td>\n<td style=\"text-align: center;\">91</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">50</td>\n<td style=\"text-align: center;\">47</td>\n<td style=\"text-align: center;\">0,020</td>\n<td style=\"text-align: center;\">86</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">80</td>\n<td style=\"text-align: center;\">0,025</td>\n<td style=\"text-align: center;\">87</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">22</td>\n<td style=\"text-align: center;\">0,014</td>\n<td style=\"text-align: center;\">71</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">1000</td>\n<td style=\"text-align: center;\">0,017</td>\n<td style=\"text-align: center;\">61</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">3500</td>\n<td style=\"text-align: center;\">0,020</td>\n<td style=\"text-align: center;\">66</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: center;\">20000</td>\n<td style=\"text-align: center;\">0,025</td>\n<td style=\"text-align: center;\">85</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">50</td>\n<td style=\"text-align: center;\">32800</td>\n<td style=\"text-align: center;\">0,033</td>\n<td style=\"text-align: center;\">83</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">71400</td>\n<td style=\"text-align: center;\">0,050</td>\n<td style=\"text-align: center;\">80</td>\n</tr>\n</tbody>\n</table>\n<p><small>* all benchmark are obtained on an ESP32 board</small><br />\n<small>** the inference took actually sometimes less than 1ms to run for all the test samples, so it was rounded to 1ms and divided by the number of samples</small></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/03/Onboard-IRIS-dataset-training-time.png\" alt=\"Onboard IRIS dataset training time. Features dim = 4\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/03/Onboard-Breast-cancer-dataset-training-time.png\" alt=\"Onboard Breast cancer dataset training time. Features dim = 30\" /></p>\n<p>We can see from the table that for the Iris dataset, which has 4-dimensional features, the training process is quite fast: 80ms to train on 60 samples.</p>\n<p>Things become much different when training on the Breast cancer dataset, with its 30 features per sample. Now we're talking abouts <em>seconds</em> to train and even minutes when increasing the number of samples to 60.</p>\n<p>Fortunately the inference time stays almost flat, so you will have real-time predictions.</p>\n<div class=\"watchout\">I run the Iris benchmark on a Seeedstudio Xiao M0 board (32bit 48MHz processor), too. It took 7s to train on 60 samples vs 80ms it took on the ESP32. It is clear some boards are better than others for this task</div>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2>Downsides</h2>\n<p>Of course there're downsides: it's not a perfect world.</p>\n<h3>Convergence</h3>\n<div class=\"watchout\">\nAs <a href=\"http://cs229.stanford.edu/materials/smo.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a> reported: \"there is one thing to note, <b>the algorithm (the simplified version) is not guaranteed to converge\".</b>\n</div>\n<p>I actually don't know, in practice, what this means. But it sounds like a bad thing.</p>\n<h3>Binary classification only</h3>\n<p>As of now this algorithm can only do binary classification.</p>\n<p>I hope to implement multi-class classification in the future with the one-vs-all approach, but I don't really know if it would be too inefficient for a microcontroller to run.</p>\n<h3>Declining accuracy</h3>\n<p>If you look at the benchmark table above, also, you'll notice the accuracy does not always increase linearly with the training samples size. It seems it reaches an optimum and then starts decreasing.</p>\n<p>If you're going to deploy your device in an autonomous scenario, you'll need to monitor your accucary every time you re-train it, or your results are going to go poor.</p>\n<p>You should keep track of the optimum you achieved and roll-back to its training set when you register a declining accuracy.</p>\n<h3>Memory</h3>\n<p>You will need to keep all your training set in memory for the classifier to both learn and predict. This means RAM will be a limiting factor and we know RAM is an expensive resource on microcontrollers.</p>\n<p>You will have to find a good enough compromise between the number of features, the number of samples and the accuracy.</p>\n<h2>Time to get your hands on</h2>\n<p>I created a sample project for you to <a href=\"/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board\">train a color classifier</a> with a super simple setup: you will only need a TCS3200 (color sensor) to follow along.</p>\n<p>Dont' have a TCS3200? No problem, you can <a href=\"/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board\">train a classifier on the IRIS dataset</a></p>\n<p>If you've read so far, please consider letting me know in the comments some useful applications you can think about this new tool. It's a brand new topic for me and I'll appreciate any of your suggestion.</p>\n<hr />\n<p>Check the project repo on <a href=\"https://github.com/eloquentarduino/EloquentMicroML\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board/\">So you want to train an ML classifier directly on an Arduino board?</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "As of now, we know it is possible to run Machine learning inference on tiny microcontrollers thanks to Tensorflow for Micro and my very own library MicroML. What if you could train a classifier directly on the microcontroller, too?\n\n\nWhen I first started this journey in the world of Machine learning on microcontrollers, one fact was set in stone for me: you train your classifier once and for all on a PC, then deploy it to your microcontroller.\nAs simple as this.\nTraining is a heavy process, requires lots of computations and memory. You just want a machine as powerful as possible to carry out this task as fast as possible.\nMoreover, it is a one-time task: once your classifier has been trained, it needs not to be updated anymore.\nAnd this yield true until now.\nUntil my reader Joao Carvalho, in the comments on the post about an alternative to SVM which produces much smaller models that I strongly invite you to read, challenged me with this idea of running the SVM training directly on the microcontroller. \nIn the past I replied &quot;No way&quot; to people asking about this topic on forums, but Joao was so kind to link me a Javascript implementation of the simplified SVM SMO (Sequential Minimal Optimization) algorithm.\nAt a first glance it looked quite easy to port from Javascript to C, so I gave it a try.\nAnd in fact it only took me 30 minutes to get it working on my PC.\nThen I deployed it to my ESP32 and... it worked!\nMy first try was with 10 samples from the the IRIS dataset: it only took almost no time to train.\nBut I know SVM training and inferencing time grows rapidly with the number of training samples.\nExecution time will be the most limiting factor for this kind of task, so I created a benchmarking setup to evaluate the performance of the algorithm on different dataset sizes and features dimensions.\nThe results are summarized in the following table and plots.\n\n\n\nFeatures size\nTraining size\nTraining time (ms)\nUnit inference time (ms)\nAccuracy\n\n\n\n\n4\n10\n0\n0,011\n80\n\n\n4\n20\n4\n0,013\n85\n\n\n4\n30\n11\n0,014\n90\n\n\n4\n40\n38\n0,017\n91\n\n\n4\n50\n47\n0,020\n86\n\n\n4\n60\n80\n0,025\n87\n\n\n30\n10\n22\n0,014\n71\n\n\n30\n20\n1000\n0,017\n61\n\n\n30\n30\n3500\n0,020\n66\n\n\n30\n40\n20000\n0,025\n85\n\n\n30\n50\n32800\n0,033\n83\n\n\n30\n60\n71400\n0,050\n80\n\n\n\n* all benchmark are obtained on an ESP32 board\n** the inference took actually sometimes less than 1ms to run for all the test samples, so it was rounded to 1ms and divided by the number of samples\n\n\nWe can see from the table that for the Iris dataset, which has 4-dimensional features, the training process is quite fast: 80ms to train on 60 samples.\nThings become much different when training on the Breast cancer dataset, with its 30 features per sample. Now we're talking abouts seconds to train and even minutes when increasing the number of samples to 60.\nFortunately the inference time stays almost flat, so you will have real-time predictions.\nI run the Iris benchmark on a Seeedstudio Xiao M0 board (32bit 48MHz processor), too. It took 7s to train on 60 samples vs 80ms it took on the ESP32. It is clear some boards are better than others for this task\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nDownsides\nOf course there're downsides: it's not a perfect world.\nConvergence\n\nAs the paper reported: \"there is one thing to note, the algorithm (the simplified version) is not guaranteed to converge\".\n\nI actually don't know, in practice, what this means. But it sounds like a bad thing.\nBinary classification only\nAs of now this algorithm can only do binary classification.\nI hope to implement multi-class classification in the future with the one-vs-all approach, but I don't really know if it would be too inefficient for a microcontroller to run.\nDeclining accuracy\nIf you look at the benchmark table above, also, you'll notice the accuracy does not always increase linearly with the training samples size. It seems it reaches an optimum and then starts decreasing.\nIf you're going to deploy your device in an autonomous scenario, you'll need to monitor your accucary every time you re-train it, or your results are going to go poor.\nYou should keep track of the optimum you achieved and roll-back to its training set when you register a declining accuracy.\nMemory\nYou will need to keep all your training set in memory for the classifier to both learn and predict. This means RAM will be a limiting factor and we know RAM is an expensive resource on microcontrollers.\nYou will have to find a good enough compromise between the number of features, the number of samples and the accuracy.\nTime to get your hands on\nI created a sample project for you to train a color classifier with a super simple setup: you will only need a TCS3200 (color sensor) to follow along.\nDont' have a TCS3200? No problem, you can train a classifier on the IRIS dataset\nIf you've read so far, please consider letting me know in the comments some useful applications you can think about this new tool. It's a brand new topic for me and I'll appreciate any of your suggestion.\n\nCheck the project repo on Github\nL'articolo So you want to train an ML classifier directly on an Arduino board? proviene da Eloquent Arduino Blog.",
            "date_published": "2020-03-28T18:03:35+01:00",
            "date_modified": "2020-03-29T18:43:27+02:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning",
                "Programming"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/02/easy-arduino-thermal-camera-with-ascii-video-streaming/",
            "url": "https://eloquentarduino.github.io/2020/02/easy-arduino-thermal-camera-with-ascii-video-streaming/",
            "title": "Easy Arduino thermal camera with (ASCII) video streaming",
            "content_html": "<p>Ever wanted to use your thermal camera with Arduino but found it difficult to go beyond the tutorials code? Let's see the easiest possible way to view your thermal camera streaming without an LCD display!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/thermal-image-rgb-vs-ascii.jpg\" alt=\"Arduino thermal image rgb vs ascii\" /></p>\n<p><span id=\"more-956\"></span></p>\n<h2>MLX90640 thermal camera</h2>\n<p>For Arduino there are essentially two thermal camera available: the AMG8833 and the MLX90640.</p>\n<p>The AMG8833 is 8x8 and the MLX90640 is 32x24.</p>\n<p>They're not cheap, it is true.</p>\n<p>But if you have to spend money, I strongly advise you to buy the MLX90640: I have one and it's not that accurate. I can't imagine how low definition would be the AMG8833.</p>\n<p>If you want to actually get something meaningful from the camera, the AMG8833 won't give you any good results.</p>\n<p>Sure, you can do interpolation: interpolation would give you <em>the impression</em> you have a better definition, but you're just &quot;inventing&quot; values you don't actually have.</p>\n<p>For demo projects it could be enough. But for any serious application, spend 20$ more and buy an MLX90640.</p>\n<h2>MLX90640 eloquent library</h2>\n<p>As you may know if you read <a href=\"/2019/11/how-to-write-clean-arduino-code/\">my previous posts</a>, I strongly believe in &quot;eloquent&quot; code, that is code that's as easy as possible to read.</p>\n<p>How many lines do you think you need to read a MLX90640 camera? Well, not that much in fact.</p>\n<pre><code class=\"language-cpp\">#include &quot;EloquentMLX90640.h&quot;\n\nusing namespace Eloquent::Sensors;\n\nfloat buffer[768];\nMLX90640 camera;\n\nvoid setup() {\n  Serial.begin(115200);\n\n  if (!camera.begin()) {\n    Serial.println(&quot;Init error&quot;);\n    delay(50000);\n  }\n}\n\nvoid loop() {\n  camera.read(buffer);\n  delay(3000);\n}</code></pre>\n<p>If you skip the declaration lines, you only need a <code>begin()</code> and <code>read()</code> call.</p>\n<p>That's it.</p>\n<p>What <code>begin()</code> does is to run all of the boilerplate code I mentioned earlier (checking the connection and initializing the parameters).</p>\n<p><code>read()</code> populates the buffer you pass as argument with the temperature readings.</p>\n<p>From now on, you're free to handle that array as you may like: this is the most flexible way for the library to handle any use-case. It simply does not pose any restriction.</p>\n<p>You can find the camera code <a href=\"#anchor-camera-code\">at the end of the page</a> or <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ThermalCameraToAsciiArtExample/EloquentMLX90640.h\">on Github</a>.</p>\n<h2>Printing as ASCII Art</h2>\n<p>Now that you have this data, you may want to actually &quot;view&quot; it. Well, that's not an easy task as one may hope.</p>\n<p>You will need an LCD if you want to create a standalone product. If you have one, it'll be the best, it's a really cute project to build.</p>\n<p>Here's a video from Adafruit that showcases even a 3D-printed case.</p>\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/ZjQEykbvb5w\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n<p>If you don't have an LCD, though, it is less practical to access your image.</p>\n<p>I did this in the past, and it meant creating a Python script reading the serial port every second and updating a plot.<br />\nIt works, sure, but it's not the most convenient way to handle it.</p>\n<p>This is the reason I thought about ASCII art: it is used to draw images in plain text, so you can view them directly in the serial monitor.</p>\n<p>Of course they will not be as accurate or representative as RGB images, but can give you an idea of what you're framing in realtime.</p>\n<p>I wrote a class to do this. Once imported in your sketch, it is super easy to get it working.</p>\n<pre><code class=\"language-cpp\">#include &quot;EloquentAsciiArt.h&quot;\n\nusing namespace Eloquent::ImageProcessing;\n\nfloat buffer[768];\nuint8_t bufferBytes[768];\nMLX90640 camera;\n// we need to specify width and height of the image\nAsciiArt&lt;32, 24&gt; art(bufferBytes);\n\nvoid loop() {\n  camera.read(buffer);\n\n  // convert float image to uint8\n  for (size_t i = 0; i &lt; 768; i++) {\n    // assumes readings are in the range 0-40 degrees\n    // change as per your need\n    bufferBytes[i] = map(buffer[i], 0, 40, 0, 255);\n  }\n\n  // print to Serial with a border of 2 characters, to distinguish one image from the next\n  art.print(&amp;Serial, 2);\n  delay(2000);\n}</code></pre>\n<p>As you can see, you need to create an <code>AsciiArt</code> object, map the image pixels in the range <code>0-255</code> and call the <code>print()</code> method: easy peasy!</p>\n<p>You can find the ASCII art generator code <a href=\"#anchor-ascii-art-code\">at the end of the page</a> or <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ThermalCameraToAsciiArtExample/EloquentAsciiArt.h\">on Github</a>.</p>\n<p>Here's the result of the sketch. It's a video of me putting my arms at the top of my head, once at a time, then standing up.</p>\n<div class=\"watchout\">Resize the Serial Monitor as only a single frame at a time is visble to have a \"video streaming\" effect</div>\n<div style=\"width: 342px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-956-1\" width=\"342\" height=\"636\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/Thermal-ascii-speedup.mp4?_=1\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/Thermal-ascii-speedup.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/02/Thermal-ascii-speedup.mp4</a></video></div>\n<p>Of course the visual effect won't be as impressive as an RGB image, but you can clearly see my figure moving.</p>\n<p>The real bad part is the &quot;glitch&quot; you see between each frame when the scrolling happens: this is something I don't know if it's possible to mitigate.</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ThermalCameraToAsciiArtExample\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<hr />\n<div id=\"anchor-camera-code\"></div>\n<pre><code class=\"language-cpp\">#pragma once\n\n#include &quot;Wire.h&quot;\n#include &quot;MLX90640_API.h&quot;\n#include &quot;MLX90640_I2C_Driver.h&quot;\n\n#ifndef TA_SHIFT\n//Default shift for MLX90640 in open air\n#define TA_SHIFT 8\n#endif\n\nnamespace Eloquent {\n    namespace Sensors {\n\n        enum class MLX90640Status {\n            OK,\n            NOT_CONNECTED,\n            DUMP_ERROR,\n            PARAMETER_ERROR,\n            FRAME_ERROR\n        };\n\n        class MLX90640 {\n        public:\n            /**\n             *\n             * @param address\n             */\n            MLX90640(uint8_t address = 0x33) :\n                _address(address),\n                _status(MLX90640Status::OK) {\n\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool begin() {\n                Wire.begin();\n                Wire.setClock(400000);\n\n                return isConnected() &amp;&amp; loadParams();\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool read(float result[768]) {\n                for (byte x = 0 ; x &lt; 2 ; x++) {\n                    uint16_t frame[834];\n                    int status = MLX90640_GetFrameData(_address, frame);\n\n                    if (status &lt; 0)\n                        return fail(MLX90640Status::FRAME_ERROR);\n\n                    float vdd = MLX90640_GetVdd(frame, &amp;_params);\n                    float Ta = MLX90640_GetTa(frame, &amp;_params);\n                    float tr = Ta - TA_SHIFT;\n                    float emissivity = 0.95;\n\n                    MLX90640_CalculateTo(frame, &amp;_params, emissivity, tr, result);\n                }\n            }\n\n        protected:\n            uint8_t _address;\n            paramsMLX90640 _params;\n            MLX90640Status _status;\n\n            /**\n             * Test if device is connected\n             * @return\n             */\n            bool isConnected() {\n                Wire.beginTransmission(_address);\n\n                if (Wire.endTransmission() == 0) {\n                    return true;\n                }\n\n                return fail(MLX90640Status::NOT_CONNECTED);\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool loadParams() {\n                uint16_t ee[832];\n                int status = MLX90640_DumpEE(_address, ee);\n\n                if (status != 0)\n                    return fail(MLX90640Status::DUMP_ERROR);\n\n                status = MLX90640_ExtractParameters(ee, &amp;_params);\n\n                if (status != 0)\n                    return fail(MLX90640Status::PARAMETER_ERROR);\n\n                return true;\n            }\n\n            /**\n             * Mark a failure\n             * @param status\n             * @return\n             */\n            bool fail(MLX90640Status status) {\n                _status = status;\n\n                return false;\n            }\n        };\n    }\n}</code></pre>\n<div id=\"anchor-ascii-art-code\"></div>\n<pre><code class=\"language-cpp\">#pragma once\n\n#include &quot;Stream.h&quot;\n\nnamespace Eloquent {\n    namespace ImageProcessing {\n\n        /**\n         *\n         * @tparam width\n         * @tparam height\n         */\n        template&lt;size_t width, size_t height&gt;\n        class AsciiArt {\n        public:\n            AsciiArt(const uint8_t *data) {\n                _data = data;\n            }\n\n            /**\n             * Get pixel at given coordinates\n             * @param x\n             * @param y\n             * @return\n             */\n            uint8_t at(size_t x, size_t y) {\n                return _data[y * width + x];\n            }\n\n            /**\n             * Print as ASCII art picture\n             * @param stream\n             */\n            void print(Stream *stream, uint8_t frameSize = 0) {\n                const char glyphs[] = &quot; .,:;xyYX&quot;;\n                const uint8_t glyphsCount = 9;\n\n                printAsciiArtHorizontalFrame(stream, frameSize);\n\n                for (size_t y = 0; y &lt; height; y++) {\n                    // vertical frame\n                    for (uint8_t k = 0; k &lt; frameSize; k++)\n                        Serial.print(&#039;|&#039;);\n\n                    for (size_t x = 0; x &lt; width; x++) {\n                        const uint8_t glyph = floor(((uint16_t) at(x, y)) * glyphsCount / 256);\n\n                        stream-&gt;print(glyphs[glyph]);\n                    }\n\n                    // vertical frame\n                    for (uint8_t k = 0; k &lt; frameSize; k++)\n                        Serial.print(&#039;|&#039;);\n\n                    stream-&gt;print(&#039;\\n&#039;);\n                }\n\n                printAsciiArtHorizontalFrame(stream, frameSize);\n                stream-&gt;flush();\n            }\n\n        protected:\n            const uint8_t *_data;\n\n            /**\n             *\n             * @param stream\n             * @param frameSize\n             */\n            void printAsciiArtHorizontalFrame(Stream *stream, uint8_t frameSize) {\n                for (uint8_t i = 0; i &lt; frameSize; i++) {\n                    for (size_t j = 0; j &lt; width + 2 * frameSize; j++)\n                        stream-&gt;print(&#039;-&#039;);\n                    stream-&gt;print(&#039;\\n&#039;);\n                }\n            }\n        };\n    }\n}</code></pre>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/02/easy-arduino-thermal-camera-with-ascii-video-streaming/\">Easy Arduino thermal camera with (ASCII) video streaming</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Ever wanted to use your thermal camera with Arduino but found it difficult to go beyond the tutorials code? Let's see the easiest possible way to view your thermal camera streaming without an LCD display!\n\n\nMLX90640 thermal camera\nFor Arduino there are essentially two thermal camera available: the AMG8833 and the MLX90640.\nThe AMG8833 is 8x8 and the MLX90640 is 32x24.\nThey're not cheap, it is true.\nBut if you have to spend money, I strongly advise you to buy the MLX90640: I have one and it's not that accurate. I can't imagine how low definition would be the AMG8833.\nIf you want to actually get something meaningful from the camera, the AMG8833 won't give you any good results.\nSure, you can do interpolation: interpolation would give you the impression you have a better definition, but you're just &quot;inventing&quot; values you don't actually have.\nFor demo projects it could be enough. But for any serious application, spend 20$ more and buy an MLX90640.\nMLX90640 eloquent library\nAs you may know if you read my previous posts, I strongly believe in &quot;eloquent&quot; code, that is code that's as easy as possible to read.\nHow many lines do you think you need to read a MLX90640 camera? Well, not that much in fact.\n#include &quot;EloquentMLX90640.h&quot;\n\nusing namespace Eloquent::Sensors;\n\nfloat buffer[768];\nMLX90640 camera;\n\nvoid setup() {\n  Serial.begin(115200);\n\n  if (!camera.begin()) {\n    Serial.println(&quot;Init error&quot;);\n    delay(50000);\n  }\n}\n\nvoid loop() {\n  camera.read(buffer);\n  delay(3000);\n}\nIf you skip the declaration lines, you only need a begin() and read() call.\nThat's it.\nWhat begin() does is to run all of the boilerplate code I mentioned earlier (checking the connection and initializing the parameters).\nread() populates the buffer you pass as argument with the temperature readings.\nFrom now on, you're free to handle that array as you may like: this is the most flexible way for the library to handle any use-case. It simply does not pose any restriction.\nYou can find the camera code at the end of the page or on Github.\nPrinting as ASCII Art\nNow that you have this data, you may want to actually &quot;view&quot; it. Well, that's not an easy task as one may hope.\nYou will need an LCD if you want to create a standalone product. If you have one, it'll be the best, it's a really cute project to build.\nHere's a video from Adafruit that showcases even a 3D-printed case.\n\nIf you don't have an LCD, though, it is less practical to access your image.\nI did this in the past, and it meant creating a Python script reading the serial port every second and updating a plot.\nIt works, sure, but it's not the most convenient way to handle it.\nThis is the reason I thought about ASCII art: it is used to draw images in plain text, so you can view them directly in the serial monitor.\nOf course they will not be as accurate or representative as RGB images, but can give you an idea of what you're framing in realtime.\nI wrote a class to do this. Once imported in your sketch, it is super easy to get it working.\n#include &quot;EloquentAsciiArt.h&quot;\n\nusing namespace Eloquent::ImageProcessing;\n\nfloat buffer[768];\nuint8_t bufferBytes[768];\nMLX90640 camera;\n// we need to specify width and height of the image\nAsciiArt&lt;32, 24&gt; art(bufferBytes);\n\nvoid loop() {\n  camera.read(buffer);\n\n  // convert float image to uint8\n  for (size_t i = 0; i &lt; 768; i++) {\n    // assumes readings are in the range 0-40 degrees\n    // change as per your need\n    bufferBytes[i] = map(buffer[i], 0, 40, 0, 255);\n  }\n\n  // print to Serial with a border of 2 characters, to distinguish one image from the next\n  art.print(&amp;Serial, 2);\n  delay(2000);\n}\nAs you can see, you need to create an AsciiArt object, map the image pixels in the range 0-255 and call the print() method: easy peasy!\nYou can find the ASCII art generator code at the end of the page or on Github.\nHere's the result of the sketch. It's a video of me putting my arms at the top of my head, once at a time, then standing up.\nResize the Serial Monitor as only a single frame at a time is visble to have a \"video streaming\" effect\n\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/02/Thermal-ascii-speedup.mp4\nOf course the visual effect won't be as impressive as an RGB image, but you can clearly see my figure moving.\nThe real bad part is the &quot;glitch&quot; you see between each frame when the scrolling happens: this is something I don't know if it's possible to mitigate.\n\r\nCheck the full project code on Github\n\n\n#pragma once\n\n#include &quot;Wire.h&quot;\n#include &quot;MLX90640_API.h&quot;\n#include &quot;MLX90640_I2C_Driver.h&quot;\n\n#ifndef TA_SHIFT\n//Default shift for MLX90640 in open air\n#define TA_SHIFT 8\n#endif\n\nnamespace Eloquent {\n    namespace Sensors {\n\n        enum class MLX90640Status {\n            OK,\n            NOT_CONNECTED,\n            DUMP_ERROR,\n            PARAMETER_ERROR,\n            FRAME_ERROR\n        };\n\n        class MLX90640 {\n        public:\n            /**\n             *\n             * @param address\n             */\n            MLX90640(uint8_t address = 0x33) :\n                _address(address),\n                _status(MLX90640Status::OK) {\n\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool begin() {\n                Wire.begin();\n                Wire.setClock(400000);\n\n                return isConnected() &amp;&amp; loadParams();\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool read(float result[768]) {\n                for (byte x = 0 ; x &lt; 2 ; x++) {\n                    uint16_t frame[834];\n                    int status = MLX90640_GetFrameData(_address, frame);\n\n                    if (status &lt; 0)\n                        return fail(MLX90640Status::FRAME_ERROR);\n\n                    float vdd = MLX90640_GetVdd(frame, &amp;_params);\n                    float Ta = MLX90640_GetTa(frame, &amp;_params);\n                    float tr = Ta - TA_SHIFT;\n                    float emissivity = 0.95;\n\n                    MLX90640_CalculateTo(frame, &amp;_params, emissivity, tr, result);\n                }\n            }\n\n        protected:\n            uint8_t _address;\n            paramsMLX90640 _params;\n            MLX90640Status _status;\n\n            /**\n             * Test if device is connected\n             * @return\n             */\n            bool isConnected() {\n                Wire.beginTransmission(_address);\n\n                if (Wire.endTransmission() == 0) {\n                    return true;\n                }\n\n                return fail(MLX90640Status::NOT_CONNECTED);\n            }\n\n            /**\n             *\n             * @return\n             */\n            bool loadParams() {\n                uint16_t ee[832];\n                int status = MLX90640_DumpEE(_address, ee);\n\n                if (status != 0)\n                    return fail(MLX90640Status::DUMP_ERROR);\n\n                status = MLX90640_ExtractParameters(ee, &amp;_params);\n\n                if (status != 0)\n                    return fail(MLX90640Status::PARAMETER_ERROR);\n\n                return true;\n            }\n\n            /**\n             * Mark a failure\n             * @param status\n             * @return\n             */\n            bool fail(MLX90640Status status) {\n                _status = status;\n\n                return false;\n            }\n        };\n    }\n}\n\n#pragma once\n\n#include &quot;Stream.h&quot;\n\nnamespace Eloquent {\n    namespace ImageProcessing {\n\n        /**\n         *\n         * @tparam width\n         * @tparam height\n         */\n        template&lt;size_t width, size_t height&gt;\n        class AsciiArt {\n        public:\n            AsciiArt(const uint8_t *data) {\n                _data = data;\n            }\n\n            /**\n             * Get pixel at given coordinates\n             * @param x\n             * @param y\n             * @return\n             */\n            uint8_t at(size_t x, size_t y) {\n                return _data[y * width + x];\n            }\n\n            /**\n             * Print as ASCII art picture\n             * @param stream\n             */\n            void print(Stream *stream, uint8_t frameSize = 0) {\n                const char glyphs[] = &quot; .,:;xyYX&quot;;\n                const uint8_t glyphsCount = 9;\n\n                printAsciiArtHorizontalFrame(stream, frameSize);\n\n                for (size_t y = 0; y &lt; height; y++) {\n                    // vertical frame\n                    for (uint8_t k = 0; k &lt; frameSize; k++)\n                        Serial.print(&#039;|&#039;);\n\n                    for (size_t x = 0; x &lt; width; x++) {\n                        const uint8_t glyph = floor(((uint16_t) at(x, y)) * glyphsCount / 256);\n\n                        stream-&gt;print(glyphs[glyph]);\n                    }\n\n                    // vertical frame\n                    for (uint8_t k = 0; k &lt; frameSize; k++)\n                        Serial.print(&#039;|&#039;);\n\n                    stream-&gt;print(&#039;\\n&#039;);\n                }\n\n                printAsciiArtHorizontalFrame(stream, frameSize);\n                stream-&gt;flush();\n            }\n\n        protected:\n            const uint8_t *_data;\n\n            /**\n             *\n             * @param stream\n             * @param frameSize\n             */\n            void printAsciiArtHorizontalFrame(Stream *stream, uint8_t frameSize) {\n                for (uint8_t i = 0; i &lt; frameSize; i++) {\n                    for (size_t j = 0; j &lt; width + 2 * frameSize; j++)\n                        stream-&gt;print(&#039;-&#039;);\n                    stream-&gt;print(&#039;\\n&#039;);\n                }\n            }\n        };\n    }\n}\nL'articolo Easy Arduino thermal camera with (ASCII) video streaming proviene da Eloquent Arduino Blog.",
            "date_published": "2020-02-29T17:20:15+01:00",
            "date_modified": "2020-03-02T20:19:00+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Computer vision",
                "Electronics",
                "Eloquent library"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/02/Thermal-ascii-speedup.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 479591
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/",
            "url": "https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/",
            "title": "Handwritten digit classification with Arduino and MicroML",
            "content_html": "<p>We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST.gif\" alt=\"Arduino handwritten digit classification\" /></p>\n<p><span id=\"more-931\"></span></p>\n<p>If this is the first time you're reading my blog, you may have missed that I'm on a journey to push the limits of Machine learning on embedded devices like the Arduino boards and ESP32.</p>\n<p>I started with <a href=\"/2019/12/how-to-do-gesture-identification-on-arduino/\">accelerometer data classification</a>, then did <a href=\"/2019/12/wifi-indoor-positioning-on-arduino/\">Wifi indoor positioning</a> as a proof of concept.</p>\n<p>In the last weeks, though, I undertook a more difficult path that is image classification.</p>\n<p>Image classification is where Convolutional Neural Networks really shine, but I'm here to <a href=\"/2020/01/image-recognition-with-esp32-and-arduino/\">question this settlement</a> and demostrate that it is possible to come up with much lighter alternatives.</p>\n<p>In this post we continue with the examples, replicating a &quot;benchmark&quot; dataset in Machine learning: the handwritten digits classification.</p>\n<div class=\"infobox\">\nIf you are curious about a specific image classification task you would like to see implemented, <b>let me know in the comments</b>: I'm always open to new ideas\n</div>\n<h2>The task</h2>\n<p>The objective of this example is to be able to tell what an handwritten digit is, taking as input a photo from the ESP32 camera.</p>\n<p>In particular, we have 3 handwritten numbers and the task of our model will be to distinguish which image is what number.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-examples.jpg\" alt=\"Handwritten digits example\" /></p>\n<p>I only have a single image per digit, but you're free to draw as many samples as you like: it should help improve the performance of you're classifier.</p>\n<h2>1. Feature extraction</h2>\n<p>When dealing with images, if you use a CNN this step is often overlooked: CNNs are made on purpose to handle raw pixel values, so you just throw the image in and it is handled properly.</p>\n<p>When using other types of classifiers, it could help add a bit of feature engineering to help the classifier doing its job and achieve high accuracy.</p>\n<p>But not this time.</p>\n<p>I wanted to be as &quot;light&quot; as possible in this demo, so I only took a couple steps during the feature acquisition:</p>\n<ol>\n<li>use a grayscale image</li>\n<li>downsample to a manageable size</li>\n<li>convert it to black/white with a threshold</li>\n</ol>\n<p>I would hardly call this feature engineering.</p>\n<p>This is an example of the result of this pipeline.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-feature-extraction.jpg\" alt=\"Handwritten digit feature extraction\" /></p>\n<p>The code for this pipeline is really simple and is almost the same from the example on <a href=\"/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">motion detection</a>.</p>\n<pre><code class=\"language-c\">#include &quot;esp_camera.h&quot;\n\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    15\n#define XCLK_GPIO_NUM     27\n#define SIOD_GPIO_NUM     22\n#define SIOC_GPIO_NUM     23\n#define Y9_GPIO_NUM       19\n#define Y8_GPIO_NUM       36\n#define Y7_GPIO_NUM       18\n#define Y6_GPIO_NUM       39\n#define Y5_GPIO_NUM        5\n#define Y4_GPIO_NUM       34\n#define Y3_GPIO_NUM       35\n#define Y2_GPIO_NUM       32\n#define VSYNC_GPIO_NUM    25\n#define HREF_GPIO_NUM     26\n#define PCLK_GPIO_NUM     21\n\n#define FRAME_SIZE FRAMESIZE_QQVGA\n#define WIDTH 160\n#define HEIGHT 120\n#define BLOCK_SIZE 5\n#define W (WIDTH / BLOCK_SIZE)\n#define H (HEIGHT / BLOCK_SIZE)\n#define THRESHOLD 127\n\ndouble features[H*W] = { 0 };\n\nvoid setup() {\n    Serial.begin(115200);\n    Serial.println(setup_camera(FRAME_SIZE) ? &quot;OK&quot; : &quot;ERR INIT&quot;);\n    delay(3000);\n}\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n        return;\n    }\n\n    print_features();\n    delay(3000);\n}\n\nbool setup_camera(framesize_t frameSize) {\n    camera_config_t config;\n\n    config.ledc_channel = LEDC_CHANNEL_0;\n    config.ledc_timer = LEDC_TIMER_0;\n    config.pin_d0 = Y2_GPIO_NUM;\n    config.pin_d1 = Y3_GPIO_NUM;\n    config.pin_d2 = Y4_GPIO_NUM;\n    config.pin_d3 = Y5_GPIO_NUM;\n    config.pin_d4 = Y6_GPIO_NUM;\n    config.pin_d5 = Y7_GPIO_NUM;\n    config.pin_d6 = Y8_GPIO_NUM;\n    config.pin_d7 = Y9_GPIO_NUM;\n    config.pin_xclk = XCLK_GPIO_NUM;\n    config.pin_pclk = PCLK_GPIO_NUM;\n    config.pin_vsync = VSYNC_GPIO_NUM;\n    config.pin_href = HREF_GPIO_NUM;\n    config.pin_sscb_sda = SIOD_GPIO_NUM;\n    config.pin_sscb_scl = SIOC_GPIO_NUM;\n    config.pin_pwdn = PWDN_GPIO_NUM;\n    config.pin_reset = RESET_GPIO_NUM;\n    config.xclk_freq_hz = 20000000;\n    config.pixel_format = PIXFORMAT_GRAYSCALE;\n    config.frame_size = frameSize;\n    config.jpeg_quality = 12;\n    config.fb_count = 1;\n\n    bool ok = esp_camera_init(&amp;config) == ESP_OK;\n\n    sensor_t *sensor = esp_camera_sensor_get();\n    sensor-&gt;set_framesize(sensor, frameSize);\n\n    return ok;\n}\n\nbool capture_still() {\n    camera_fb_t *frame = esp_camera_fb_get();\n\n    if (!frame)\n        return false;\n\n    // reset all the features\n    for (size_t i = 0; i &lt; H * W; i++)\n      features[i] = 0;\n\n    // for each pixel, compute the position in the downsampled image\n    for (size_t i = 0; i &lt; frame-&gt;len; i++) {\n      const uint16_t x = i % WIDTH;\n      const uint16_t y = floor(i / WIDTH);\n      const uint8_t block_x = floor(x / BLOCK_SIZE);\n      const uint8_t block_y = floor(y / BLOCK_SIZE);\n      const uint16_t j = block_y * W + block_x;\n\n      features[j] += frame-&gt;buf[i];\n    }\n\n    // apply threshold\n    for (size_t i = 0; i &lt; H * W; i++) {\n      features[i] = (features[i] / (BLOCK_SIZE * BLOCK_SIZE) &gt; THRESHOLD) ? 1 : 0;\n    }\n\n    return true;\n}\n\nvoid print_features() {\n    for (size_t i = 0; i &lt; H * W; i++) {\n        Serial.print(features[i]);\n\n        if (i != H * W - 1)\n          Serial.print(&#039;,&#039;);\n    }\n\n    Serial.println();\n}</code></pre>\n<h2>2. Samples recording</h2>\n<p>To create your own dataset, you need a collection of handwritten digits.</p>\n<p>You can do this part as you like, by using pieces of paper or a monitor. I used a tablet because it was well illuminated and I could open a bunch of tabs to keep a record of my samples.</p>\n<p>As in the <a href=\"/2020/01/image-recognition-with-esp32-and-arduino/\">apple vs orange</a>, keep in mind that you should be consistent during both the training phase and the inference phase.</p>\n<p>This is why I used tape to fix my ESP32 camera to the desk and kept the tablet in the exact same position.</p>\n<p>If you desire, you could experiment varying slightly the capturing setup during the training and see if your classifier still achieves good accuracy: this is a test I didn't make.</p>\n<h2>3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn/\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<h2>4. The result</h2>\n<p>Okay, at this point you should have all the working pieces to do handwritten digit image classification on your ESP32 camera. Include your model in the sketch and run the classification.</p>\n<pre><code class=\"language-c\">#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n\n        return;\n    }\n\n    Serial.print(&quot;Number: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(3000);\n}</code></pre>\n<p>Done.</p>\n<p>You can see a demo of my results in the video below.</p>\n<div style=\"width: 788px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-931-2\" width=\"788\" height=\"443\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4?_=2\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4</a></video></div>\n<h3>Project figures</h3>\n<p>My dataset is composed of 25 training samples in total and the SVM with linear kernel produced 17 support vectors.</p>\n<p>On my M5Stick camera board, the overhead for the model is 6.8 Kb of flash and the inference takes 7ms: not that bad!</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/HandwrittenDigitClassificationExample/HandwrittenDigitClassificationExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/\">Handwritten digit classification with Arduino and MicroML</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera.\n\n\nIf this is the first time you're reading my blog, you may have missed that I'm on a journey to push the limits of Machine learning on embedded devices like the Arduino boards and ESP32.\nI started with accelerometer data classification, then did Wifi indoor positioning as a proof of concept.\nIn the last weeks, though, I undertook a more difficult path that is image classification.\nImage classification is where Convolutional Neural Networks really shine, but I'm here to question this settlement and demostrate that it is possible to come up with much lighter alternatives.\nIn this post we continue with the examples, replicating a &quot;benchmark&quot; dataset in Machine learning: the handwritten digits classification.\n\nIf you are curious about a specific image classification task you would like to see implemented, let me know in the comments: I'm always open to new ideas\n\nThe task\nThe objective of this example is to be able to tell what an handwritten digit is, taking as input a photo from the ESP32 camera.\nIn particular, we have 3 handwritten numbers and the task of our model will be to distinguish which image is what number.\n\nI only have a single image per digit, but you're free to draw as many samples as you like: it should help improve the performance of you're classifier.\n1. Feature extraction\nWhen dealing with images, if you use a CNN this step is often overlooked: CNNs are made on purpose to handle raw pixel values, so you just throw the image in and it is handled properly.\nWhen using other types of classifiers, it could help add a bit of feature engineering to help the classifier doing its job and achieve high accuracy.\nBut not this time.\nI wanted to be as &quot;light&quot; as possible in this demo, so I only took a couple steps during the feature acquisition:\n\nuse a grayscale image\ndownsample to a manageable size\nconvert it to black/white with a threshold\n\nI would hardly call this feature engineering.\nThis is an example of the result of this pipeline.\n\nThe code for this pipeline is really simple and is almost the same from the example on motion detection.\n#include &quot;esp_camera.h&quot;\n\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    15\n#define XCLK_GPIO_NUM     27\n#define SIOD_GPIO_NUM     22\n#define SIOC_GPIO_NUM     23\n#define Y9_GPIO_NUM       19\n#define Y8_GPIO_NUM       36\n#define Y7_GPIO_NUM       18\n#define Y6_GPIO_NUM       39\n#define Y5_GPIO_NUM        5\n#define Y4_GPIO_NUM       34\n#define Y3_GPIO_NUM       35\n#define Y2_GPIO_NUM       32\n#define VSYNC_GPIO_NUM    25\n#define HREF_GPIO_NUM     26\n#define PCLK_GPIO_NUM     21\n\n#define FRAME_SIZE FRAMESIZE_QQVGA\n#define WIDTH 160\n#define HEIGHT 120\n#define BLOCK_SIZE 5\n#define W (WIDTH / BLOCK_SIZE)\n#define H (HEIGHT / BLOCK_SIZE)\n#define THRESHOLD 127\n\ndouble features[H*W] = { 0 };\n\nvoid setup() {\n    Serial.begin(115200);\n    Serial.println(setup_camera(FRAME_SIZE) ? &quot;OK&quot; : &quot;ERR INIT&quot;);\n    delay(3000);\n}\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n        return;\n    }\n\n    print_features();\n    delay(3000);\n}\n\nbool setup_camera(framesize_t frameSize) {\n    camera_config_t config;\n\n    config.ledc_channel = LEDC_CHANNEL_0;\n    config.ledc_timer = LEDC_TIMER_0;\n    config.pin_d0 = Y2_GPIO_NUM;\n    config.pin_d1 = Y3_GPIO_NUM;\n    config.pin_d2 = Y4_GPIO_NUM;\n    config.pin_d3 = Y5_GPIO_NUM;\n    config.pin_d4 = Y6_GPIO_NUM;\n    config.pin_d5 = Y7_GPIO_NUM;\n    config.pin_d6 = Y8_GPIO_NUM;\n    config.pin_d7 = Y9_GPIO_NUM;\n    config.pin_xclk = XCLK_GPIO_NUM;\n    config.pin_pclk = PCLK_GPIO_NUM;\n    config.pin_vsync = VSYNC_GPIO_NUM;\n    config.pin_href = HREF_GPIO_NUM;\n    config.pin_sscb_sda = SIOD_GPIO_NUM;\n    config.pin_sscb_scl = SIOC_GPIO_NUM;\n    config.pin_pwdn = PWDN_GPIO_NUM;\n    config.pin_reset = RESET_GPIO_NUM;\n    config.xclk_freq_hz = 20000000;\n    config.pixel_format = PIXFORMAT_GRAYSCALE;\n    config.frame_size = frameSize;\n    config.jpeg_quality = 12;\n    config.fb_count = 1;\n\n    bool ok = esp_camera_init(&amp;config) == ESP_OK;\n\n    sensor_t *sensor = esp_camera_sensor_get();\n    sensor-&gt;set_framesize(sensor, frameSize);\n\n    return ok;\n}\n\nbool capture_still() {\n    camera_fb_t *frame = esp_camera_fb_get();\n\n    if (!frame)\n        return false;\n\n    // reset all the features\n    for (size_t i = 0; i &lt; H * W; i++)\n      features[i] = 0;\n\n    // for each pixel, compute the position in the downsampled image\n    for (size_t i = 0; i &lt; frame-&gt;len; i++) {\n      const uint16_t x = i % WIDTH;\n      const uint16_t y = floor(i / WIDTH);\n      const uint8_t block_x = floor(x / BLOCK_SIZE);\n      const uint8_t block_y = floor(y / BLOCK_SIZE);\n      const uint16_t j = block_y * W + block_x;\n\n      features[j] += frame-&gt;buf[i];\n    }\n\n    // apply threshold\n    for (size_t i = 0; i &lt; H * W; i++) {\n      features[i] = (features[i] / (BLOCK_SIZE * BLOCK_SIZE) &gt; THRESHOLD) ? 1 : 0;\n    }\n\n    return true;\n}\n\nvoid print_features() {\n    for (size_t i = 0; i &lt; H * W; i++) {\n        Serial.print(features[i]);\n\n        if (i != H * W - 1)\n          Serial.print(&#039;,&#039;);\n    }\n\n    Serial.println();\n}\n2. Samples recording\nTo create your own dataset, you need a collection of handwritten digits.\nYou can do this part as you like, by using pieces of paper or a monitor. I used a tablet because it was well illuminated and I could open a bunch of tabs to keep a record of my samples.\nAs in the apple vs orange, keep in mind that you should be consistent during both the training phase and the inference phase.\nThis is why I used tape to fix my ESP32 camera to the desk and kept the tablet in the exact same position.\nIf you desire, you could experiment varying slightly the capturing setup during the training and see if your classifier still achieves good accuracy: this is a test I didn't make.\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\n4. The result\nOkay, at this point you should have all the working pieces to do handwritten digit image classification on your ESP32 camera. Include your model in the sketch and run the classification.\n#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n\n        return;\n    }\n\n    Serial.print(&quot;Number: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(3000);\n}\nDone.\nYou can see a demo of my results in the video below.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4\nProject figures\nMy dataset is composed of 25 training samples in total and the SVM with linear kernel produced 17 support vectors.\nOn my M5Stick camera board, the overhead for the model is 6.8 Kb of flash and the inference takes 7ms: not that bad!\n\r\nCheck the full project code on Github\nL'articolo Handwritten digit classification with Arduino and MicroML proviene da Eloquent Arduino Blog.",
            "date_published": "2020-02-23T11:53:03+01:00",
            "date_modified": "2020-02-23T13:11:37+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "microml",
                "Arduino Machine learning",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 6424809
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/",
            "url": "https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/",
            "title": "Even smaller Machine learning models for your MCU: up to -82% code size",
            "content_html": "<p>So far we've used SVM (Support Vector Machine) as our main classifier to port a Machine learning model to a microcontroller: but recently I found an interesting alternative which could be waaaay smaller, mantaining a similar accuracy.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/rvm.jpg\" alt=\"RVM vs SVM support vectors\" /></p>\n<p><span id=\"more-893\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocthe-current-state\">The current state</a><li><a href=\"#toca-new-algorithm-relevance-vector-machines\">A new algorithm: Relevance Vector Machines</a><li><a href=\"#toctraining-a-classifier\">Training a classifier</a><li><a href=\"#tocporting-to-c\">Porting to C</a><li><a href=\"#tocperformace-comparison\">Performace comparison</a><li><a href=\"#tocsize-comparison\">Size comparison</a><li><a href=\"#tocdisclaimer\">Disclaimer</a></ol></div></p>\n<h2 id=\"tocthe-current-state\">The current state</h2>\n<p>I chose SVM as my main focus of intereset for the MicroML framework because I knew the support vector encoding could be very memory efficient once ported to plain C. And it really is.</p>\n<p>I was able to port many real-world models (gesture identification, wake word detection) to tiny microcontrollers like the old Arduino Nano (32 kb flash, 2 kb RAM).</p>\n<p>The tradeoff of my implementation was to sacrifice the flash space (which is usually quite big) to save as much RAM as possible, which is usually the most limiting factor.</p>\n<p>Due to this implementation, if your model grows in size (highly dimensional data or not well separable data), the generated code will still fit in the RAM, but &quot;overflow&quot; the available flash.</p>\n<p>In a couple of my previous post I warned that model selection might be a required step before being able to deploy a model to a MCU, since you should first check if it fits. If not, you must train another model hoping to get fewer support vectors, since each of them contributes to the code size increase.</p>\n<h2 id=\"toca-new-algorithm-relevance-vector-machines\">A new algorithm: Relevance Vector Machines</h2>\n<p>It was by chance that I came across a new algorithm that I never heard of, called <a href=\"https://en.wikipedia.org/wiki/Relevance_vector_machine\">Relevance Vector Machine</a>. It was patented by Microsoft until last year (so maybe this is the reason you don't see it in the wild), but now it is free of use as far as I can tell.</p>\n<p>Here is the <a href=\"https://papers.nips.cc/paper/1719-the-relevance-vector-machine.pdf\">link</a> to the paper if you want to read it, it gives some insights into the development process.</p>\n<p>I'm not a mathematician, so I can't describe it accurately, but in a few words it uses the same formulation of SVM (a weightened sum of kernels), applying a Bayesan model.</p>\n<p>This serves in the first place to be able to get the probabilities of the classification results, which is something totally missing in SVM.</p>\n<p>In the second place, the algorithm tries to learn a much more sparse representation of the support vectors, as you can see in the following picture.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/rvm.jpg\" alt=\"RVM vs SVM support vectors\" /></p>\n<p>When I first read the paper my first tought was just &quot;wow&quot;! This is exactly what I need for my MicroML framework: a ultra-lightweight model which can still achieve high accuracy.</p>\n<h2 id=\"toctraining-a-classifier\">Training a classifier</h2>\n<p>Now that I knew this algorithm, I searched for it in the <code>sklearn</code> documentation: it was not there.</p>\n<p>It seems that, since it was patented, they didn't have an implementation.</p>\n<p>Fortunately, there is <a href=\"https://github.com/AmazaspShumik/sklearn_bayes/\">an implementation</a> which follows the sklearn paradigm. You have to install it:</p>\n<pre><code class=\"language-bash\">pip install Cython\npip install https://github.com/AmazaspShumik/sklearn_bayes/archive/master.zip</code></pre>\n<p>Since the interface is the usual <code>fit</code> <code>predict</code>, it is super easy to train a classifier.</p>\n<pre><code class=\"language-python\">from sklearn.datasets import load_iris\nfrom skbayes.rvm_ard_models import RVC\nimport warnings\n\n# I get tons of boring warnings during training, so turn it off\nwarnings.filterwarnings(&quot;ignore&quot;)\n\niris = load_iris()\nX = iris.data\ny = iris.target\nclf = RVC(kernel=&#039;rbf&#039;, gamma=0.001)\nclf.fit(X, y)\ny_predict = clf.predict(X)</code></pre>\n<p>The parameters for the constructor are similar to those of the <code>SVC</code> classifier from sklearn:</p>\n<ul>\n<li><code>kernel</code>: one of linear, poly, rbf</li>\n<li><code>degree</code>: if <code>kernel=poly</code></li>\n<li><code>gamma</code>: if <code>kernel=poly</code> or <code>kernel=rbf</code></li>\n</ul>\n<p>You can read <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">the docs</a> from sklearn to learn more.</p>\n<h2 id=\"tocporting-to-c\">Porting to C</h2>\n<p>Now that we have a trained classifier, we have to port it to plain C that compiles on our microcontroller of choice.</p>\n<p>I patched my package <code>micromlgen</code> to do the job for you, so you should install the latest version to get it working.</p>\n<pre><code class=\"language-bash\"> pip install --upgrade micromlgen</code></pre>\n<p>Now the export part is almost the same as with an SVM classifier.</p>\n<pre><code class=\"language-python\"> from micromlgen import port_rvm\n\n clf = get_rvm_classifier()\n c_code = port_rvm(clf)\n print(c_code)</code></pre>\n<p>And you're done: you have plain C code you can embed in any microcontroller.</p>\n<h2 id=\"tocperformace-comparison\">Performace comparison</h2>\n<p>To test the effectiveness of this new algorithm, I applied it to the datasets I built in my previous posts, comparing side by side the size and accuracy of both SVM and RVM.</p>\n<p>The results are summarized in the next table.</p>\n<style>\n.dataset th+th, .dataset td + td { text-align: center; }\n.dataset small { display: block; font-size: 0.8em; }\n.dataset .__h td {background: blanchedalmond !important}\n</style>\n<table class=\"dataset\">\n<thead>\n<tr>\n<th>Dataset</th>\n<th colspan=\"2\">SVM</th>\n<th colspan=\"2\">RVM</th>\n<th colspan=\"2\">Delta</th>\n</tr>\n<tr>\n<th></th>\n<th>Flash<small>(byte)</small></th>\n<th>Acc. <small>(%)</small></th>\n<th>Flash<small>(byte)</small></th>\n<th>Acc. <small>(%)</small></th>\n<th>Flash</th>\n<th>Acc.</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>RGB colors</td>\n<td>4584</td>\n<td>100</td>\n<td>3580</td>\n<td>100</td>\n<td>-22%</td>\n<td>-0%</td>\n</tr>\n<tr>\n<td>Accelerometer gestures<small>(linear kernel)</small></td>\n<td>36888</td>\n<td>92</td>\n<td>7056</td>\n<td>85</td>\n<td>-80%</td>\n<td>-7%</td>\n</tr>\n<tr class=\"__h\">\n<td>Accelerometer gestures<small>(gaussian kernel)</small></td>\n<td>45348</td>\n<td>95</td>\n<td>7766</td>\n<td>95</td>\n<td>-82%</td>\n<td>-0%</td>\n</tr>\n<tr>\n<td>Wifi positioning</td>\n<td>4641</td>\n<td>100</td>\n<td>3534</td>\n<td>100</td>\n<td>-24%</td>\n<td>-0%</td>\n</tr>\n<tr>\n<td>Wake word<small>(linear kernel)</small></td>\n<td>18098</td>\n<td>86</td>\n<td>3602</td>\n<td>53</td>\n<td>-80%</td>\n<td>-33%</td>\n</tr>\n<tr>\n<td>Wake word<small>(gaussian kernel)</small></td>\n<td>21788</td>\n<td>90</td>\n<td>4826</td>\n<td>62</td>\n<td>-78%</td>\n<td>-28%</td>\n</tr>\n</tbody>\n</table>\n<p><small style=\"font-style: italic; font-size: 0.8em;\">** the accuracy reported are with default parameters, without any tuning, averaged in 30 runs</small></p>\n<p>As you may see, the results are quite surpising:</p>\n<ul>\n<li>you can achieve <strong>up to 82% space reduction</strong> on highly dimensional dataset <strong>without any loss in accuracy</strong> (accelerometer gestures with gaussian kernel)</li>\n<li>sometimes you may not be able to achieve a decent accuracy (62% at most on the wake word dataset)</li>\n</ul>\n<p>As in any situation, you should test which one of the two algorithms works best for your use case, but there a couple of guidelines you may follow:</p>\n<ul>\n<li>if you need top accuracy, probably SVM can achieve slighter better performance if you have enough space</li>\n<li>if you need tiny space or top speed, test if RVM achieves a satisfiable accuracy</li>\n<li>if both SVM and RVM achieve comparable performace, go with RVM: it's much lighter than SVM in most cases and will run faster</li>\n</ul>\n<h2 id=\"tocsize-comparison\">Size comparison</h2>\n<p>As a reference, here is the codes generated for an SVM classifier and an RVM one to classify the IRIS dataset.</p>\n<pre><code class=\"language-c\">uint8_t predict_rvm(double *x) {\n    double decision[3] = { 0 };\n    decision[0] = -0.6190847299428206;\n    decision[1] = (compute_kernel(x,  6.3, 3.3, 6.0, 2.5) - 72.33233 ) * 0.228214 + -2.3609625;\n    decision[2] = (compute_kernel(x,  7.7, 2.8, 6.7, 2.0) - 81.0089166 ) * -0.29006 + -3.360963;\n    uint8_t idx = 0;\n    double val = decision[0];\n    for (uint8_t i = 1; i &lt; 3; i++) {\n        if (decision[i] &gt; val) {\n            idx = i;\n            val = decision[i];\n        }\n    }\n    return idx;\n}\n\nint predict_svm(double *x) {\n    double kernels[10] = { 0 };\n    double decisions[3] = { 0 };\n    int votes[3] = { 0 };\n        kernels[0] = compute_kernel(x,   6.7  , 3.0  , 5.0  , 1.7 );\n        kernels[1] = compute_kernel(x,   6.0  , 2.7  , 5.1  , 1.6 );\n        kernels[2] = compute_kernel(x,   5.1  , 2.5  , 3.0  , 1.1 );\n        kernels[3] = compute_kernel(x,   6.0  , 3.0  , 4.8  , 1.8 );\n        kernels[4] = compute_kernel(x,   7.2  , 3.0  , 5.8  , 1.6 );\n        kernels[5] = compute_kernel(x,   4.9  , 2.5  , 4.5  , 1.7 );\n        kernels[6] = compute_kernel(x,   6.2  , 2.8  , 4.8  , 1.8 );\n        kernels[7] = compute_kernel(x,   6.0  , 2.2  , 5.0  , 1.5 );\n        kernels[8] = compute_kernel(x,   4.8  , 3.4  , 1.9  , 0.2 );\n        kernels[9] = compute_kernel(x,   5.1  , 3.3  , 1.7  , 0.5 );\n        decisions[0] = 20.276395502\n                    + kernels[0] * 100.0\n                    + kernels[1] * 100.0\n                    + kernels[3] * -79.351629954\n                    + kernels[4] * -49.298850195\n                    + kernels[6] * -40.585178082\n                    + kernels[7] * -30.764341769\n        ;\n        decisions[1] = -0.903345464\n                    + kernels[2] * 0.743494115\n                    + kernels[9] * -0.743494115\n        ;\n        decisions[2] = -1.507856504\n                    + kernels[5] * 0.203695177\n                    + kernels[8] * -0.160020702\n                    + kernels[9] * -0.043674475\n        ;\n        votes[decisions[0] &gt; 0 ? 0 : 1] += 1;\n        votes[decisions[1] &gt; 0 ? 0 : 2] += 1;\n        votes[decisions[2] &gt; 0 ? 1 : 2] += 1;\n                int classVal = -1;\n        int classIdx = -1;\n        for (int i = 0; i &lt; 3; i++) {\n            if (votes[i] &gt; classVal) {\n                classVal = votes[i];\n                classIdx = i;\n            }\n        }\n        return classIdx;\n}</code></pre>\n<p>As you can see, RVM actually only computes 2 kernels and does 2 multiplications. SVM, on the other hand, computes 10 kernels and does 13 multiplications.</p>\n<p>This is a recurring pattern, so RVM is much much faster in the inference process.</p>\n<h2 id=\"tocdisclaimer\">Disclaimer</h2>\n<p><code>micromlgen</code> and in particular <code>port_rvm</code> are work in progress: you may experience some glitches or it may not work in your specific case. Please report any issue <a href=\"https://github.com/eloquentarduino/micromlgen\">on the Github repo</a>.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/\">Even smaller Machine learning models for your MCU: up to -82% code size</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "So far we've used SVM (Support Vector Machine) as our main classifier to port a Machine learning model to a microcontroller: but recently I found an interesting alternative which could be waaaay smaller, mantaining a similar accuracy.\n\n\nTable of contentsThe current stateA new algorithm: Relevance Vector MachinesTraining a classifierPorting to CPerformace comparisonSize comparisonDisclaimer\nThe current state\nI chose SVM as my main focus of intereset for the MicroML framework because I knew the support vector encoding could be very memory efficient once ported to plain C. And it really is.\nI was able to port many real-world models (gesture identification, wake word detection) to tiny microcontrollers like the old Arduino Nano (32 kb flash, 2 kb RAM).\nThe tradeoff of my implementation was to sacrifice the flash space (which is usually quite big) to save as much RAM as possible, which is usually the most limiting factor.\nDue to this implementation, if your model grows in size (highly dimensional data or not well separable data), the generated code will still fit in the RAM, but &quot;overflow&quot; the available flash.\nIn a couple of my previous post I warned that model selection might be a required step before being able to deploy a model to a MCU, since you should first check if it fits. If not, you must train another model hoping to get fewer support vectors, since each of them contributes to the code size increase.\nA new algorithm: Relevance Vector Machines\nIt was by chance that I came across a new algorithm that I never heard of, called Relevance Vector Machine. It was patented by Microsoft until last year (so maybe this is the reason you don't see it in the wild), but now it is free of use as far as I can tell.\nHere is the link to the paper if you want to read it, it gives some insights into the development process.\nI'm not a mathematician, so I can't describe it accurately, but in a few words it uses the same formulation of SVM (a weightened sum of kernels), applying a Bayesan model.\nThis serves in the first place to be able to get the probabilities of the classification results, which is something totally missing in SVM.\nIn the second place, the algorithm tries to learn a much more sparse representation of the support vectors, as you can see in the following picture.\n\nWhen I first read the paper my first tought was just &quot;wow&quot;! This is exactly what I need for my MicroML framework: a ultra-lightweight model which can still achieve high accuracy.\nTraining a classifier\nNow that I knew this algorithm, I searched for it in the sklearn documentation: it was not there.\nIt seems that, since it was patented, they didn't have an implementation.\nFortunately, there is an implementation which follows the sklearn paradigm. You have to install it:\npip install Cython\npip install https://github.com/AmazaspShumik/sklearn_bayes/archive/master.zip\nSince the interface is the usual fit predict, it is super easy to train a classifier.\nfrom sklearn.datasets import load_iris\nfrom skbayes.rvm_ard_models import RVC\nimport warnings\n\n# I get tons of boring warnings during training, so turn it off\nwarnings.filterwarnings(&quot;ignore&quot;)\n\niris = load_iris()\nX = iris.data\ny = iris.target\nclf = RVC(kernel=&#039;rbf&#039;, gamma=0.001)\nclf.fit(X, y)\ny_predict = clf.predict(X)\nThe parameters for the constructor are similar to those of the SVC classifier from sklearn:\n\nkernel: one of linear, poly, rbf\ndegree: if kernel=poly\ngamma: if kernel=poly or kernel=rbf\n\nYou can read the docs from sklearn to learn more.\nPorting to C\nNow that we have a trained classifier, we have to port it to plain C that compiles on our microcontroller of choice.\nI patched my package micromlgen to do the job for you, so you should install the latest version to get it working.\n pip install --upgrade micromlgen\nNow the export part is almost the same as with an SVM classifier.\n from micromlgen import port_rvm\n\n clf = get_rvm_classifier()\n c_code = port_rvm(clf)\n print(c_code)\nAnd you're done: you have plain C code you can embed in any microcontroller.\nPerformace comparison\nTo test the effectiveness of this new algorithm, I applied it to the datasets I built in my previous posts, comparing side by side the size and accuracy of both SVM and RVM.\nThe results are summarized in the next table.\n\n\n\n\nDataset\nSVM\nRVM\nDelta\n\n\n\nFlash(byte)\nAcc. (%)\nFlash(byte)\nAcc. (%)\nFlash\nAcc.\n\n\n\n\nRGB colors\n4584\n100\n3580\n100\n-22%\n-0%\n\n\nAccelerometer gestures(linear kernel)\n36888\n92\n7056\n85\n-80%\n-7%\n\n\nAccelerometer gestures(gaussian kernel)\n45348\n95\n7766\n95\n-82%\n-0%\n\n\nWifi positioning\n4641\n100\n3534\n100\n-24%\n-0%\n\n\nWake word(linear kernel)\n18098\n86\n3602\n53\n-80%\n-33%\n\n\nWake word(gaussian kernel)\n21788\n90\n4826\n62\n-78%\n-28%\n\n\n\n** the accuracy reported are with default parameters, without any tuning, averaged in 30 runs\nAs you may see, the results are quite surpising:\n\nyou can achieve up to 82% space reduction on highly dimensional dataset without any loss in accuracy (accelerometer gestures with gaussian kernel)\nsometimes you may not be able to achieve a decent accuracy (62% at most on the wake word dataset)\n\nAs in any situation, you should test which one of the two algorithms works best for your use case, but there a couple of guidelines you may follow:\n\nif you need top accuracy, probably SVM can achieve slighter better performance if you have enough space\nif you need tiny space or top speed, test if RVM achieves a satisfiable accuracy\nif both SVM and RVM achieve comparable performace, go with RVM: it's much lighter than SVM in most cases and will run faster\n\nSize comparison\nAs a reference, here is the codes generated for an SVM classifier and an RVM one to classify the IRIS dataset.\nuint8_t predict_rvm(double *x) {\n    double decision[3] = { 0 };\n    decision[0] = -0.6190847299428206;\n    decision[1] = (compute_kernel(x,  6.3, 3.3, 6.0, 2.5) - 72.33233 ) * 0.228214 + -2.3609625;\n    decision[2] = (compute_kernel(x,  7.7, 2.8, 6.7, 2.0) - 81.0089166 ) * -0.29006 + -3.360963;\n    uint8_t idx = 0;\n    double val = decision[0];\n    for (uint8_t i = 1; i &lt; 3; i++) {\n        if (decision[i] &gt; val) {\n            idx = i;\n            val = decision[i];\n        }\n    }\n    return idx;\n}\n\nint predict_svm(double *x) {\n    double kernels[10] = { 0 };\n    double decisions[3] = { 0 };\n    int votes[3] = { 0 };\n        kernels[0] = compute_kernel(x,   6.7  , 3.0  , 5.0  , 1.7 );\n        kernels[1] = compute_kernel(x,   6.0  , 2.7  , 5.1  , 1.6 );\n        kernels[2] = compute_kernel(x,   5.1  , 2.5  , 3.0  , 1.1 );\n        kernels[3] = compute_kernel(x,   6.0  , 3.0  , 4.8  , 1.8 );\n        kernels[4] = compute_kernel(x,   7.2  , 3.0  , 5.8  , 1.6 );\n        kernels[5] = compute_kernel(x,   4.9  , 2.5  , 4.5  , 1.7 );\n        kernels[6] = compute_kernel(x,   6.2  , 2.8  , 4.8  , 1.8 );\n        kernels[7] = compute_kernel(x,   6.0  , 2.2  , 5.0  , 1.5 );\n        kernels[8] = compute_kernel(x,   4.8  , 3.4  , 1.9  , 0.2 );\n        kernels[9] = compute_kernel(x,   5.1  , 3.3  , 1.7  , 0.5 );\n        decisions[0] = 20.276395502\n                    + kernels[0] * 100.0\n                    + kernels[1] * 100.0\n                    + kernels[3] * -79.351629954\n                    + kernels[4] * -49.298850195\n                    + kernels[6] * -40.585178082\n                    + kernels[7] * -30.764341769\n        ;\n        decisions[1] = -0.903345464\n                    + kernels[2] * 0.743494115\n                    + kernels[9] * -0.743494115\n        ;\n        decisions[2] = -1.507856504\n                    + kernels[5] * 0.203695177\n                    + kernels[8] * -0.160020702\n                    + kernels[9] * -0.043674475\n        ;\n        votes[decisions[0] &gt; 0 ? 0 : 1] += 1;\n        votes[decisions[1] &gt; 0 ? 0 : 2] += 1;\n        votes[decisions[2] &gt; 0 ? 1 : 2] += 1;\n                int classVal = -1;\n        int classIdx = -1;\n        for (int i = 0; i &lt; 3; i++) {\n            if (votes[i] &gt; classVal) {\n                classVal = votes[i];\n                classIdx = i;\n            }\n        }\n        return classIdx;\n}\nAs you can see, RVM actually only computes 2 kernels and does 2 multiplications. SVM, on the other hand, computes 10 kernels and does 13 multiplications.\nThis is a recurring pattern, so RVM is much much faster in the inference process.\nDisclaimer\nmicromlgen and in particular port_rvm are work in progress: you may experience some glitches or it may not work in your specific case. Please report any issue on the Github repo.\nL'articolo Even smaller Machine learning models for your MCU: up to -82% code size proviene da Eloquent Arduino Blog.",
            "date_published": "2020-02-15T17:37:32+01:00",
            "date_modified": "2020-02-17T18:29:21+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "rvm",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/easy-tinyml-on-esp32-and-arduino/",
            "url": "https://eloquentarduino.github.io/2020/01/easy-tinyml-on-esp32-and-arduino/",
            "title": "Easy TinyML on ESP32 and Arduino",
            "content_html": "<p>In this post I will show you how to easily deploy your Tensorflow Lite model to an ESP32 using the Arduino IDE <strong>without any compilation stuff</strong>.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/tf-arduino-esp.png\" alt=\"tf arduino esp\" /></p>\n<p><span id=\"more-864\"></span></p>\n<p>So I finally settled on giving a try to TinyML, which is a way to deploy Tensorflow Lite models to microcontrollers.<br />\nAs a first step, I downloaded the free chapters from <a href=\"https://tinymlbook.com/\">the TinyML book website</a> and rapidly skimmed through them.</p>\n<p>Let me say that, even if it starts from &quot;too beginner&quot; level for me (they explain why you need to use the arrow instead of the point to access a pointer's property), it is a very well written book. They uncover every single aspect you may encounter during your first steps and give a very sound introduction to the general topic of training, validating and testing a dataset on a model.</p>\n<p>If I will go on with this TinyML stuff, I'll probably buy a copy: I strongly recommend you to at least read the free sample.</p>\n<p>Once done reading the 6 chapters, I wanted to try the described tutorial on my ESP32. Sadly, it is not mentioned in the supported boards on the book, so I had to solve it by myself.</p>\n<p>In this post I'm going to make a sort of recap of my learnings about the steps you need to follow to implement TF models to a microcontroller and introduce you to a tiny library I wrote for the purpose of facilitating the deployment in the Arduino IDE: <a href=\"https://github.com/eloquentarduino/EloquentTinyML\">EloquentTinyML</a>.</p>\n<h2>Building our first model</h2>\n<p>First of all, we need a model to deploy.</p>\n<p>The book guides us on building a neural network capable of predicting the sine value of a given number, in the range from 0 to Pi (3.14).</p>\n<p>It's an easy model to get started (the &quot;Hello world&quot; of machine learning, according to the authors), so we'll stick with it.</p>\n<p>I won't go into too much details about generating data and training the classifier, because I suppose you already know that part if you want to port Tensorflow on a microcontroller.</p>\n<p>Here's the code from the book.</p>\n<pre><code class=\"language-python\">import math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef get_model():\n    SAMPLES = 1000\n    np.random.seed(1337)\n    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n    # shuffle and add noise\n    np.random.shuffle(x_values)\n    y_values = np.sin(x_values)\n    y_values += 0.1 * np.random.randn(*y_values.shape)\n\n    # split into train, validation, test\n    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n    # create a NN with 2 layers of 16 neurons\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, activation=&#039;relu&#039;, input_shape=(1,)))\n    model.add(layers.Dense(16, activation=&#039;relu&#039;))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=&#039;rmsprop&#039;, loss=&#039;mse&#039;, metrics=[&#039;mae&#039;])\n    model.fit(x_train, y_train, epochs=200, batch_size=16,\n                        validation_data=(x_validate, y_validate))\n    return model</code></pre>\n<h2>Exporting the model</h2>\n<p>Now that we have a model, we need to convert it into a form ready to be deployed on our microcontroller. This is actually just an array of bytes that the TF interpreter will read to recreate the model.</p>\n<pre><code class=\"language-python\">model = get_model()\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(&quot;sine_model_quantized.tflite&quot;, &quot;wb&quot;).write(tflite_model)</code></pre>\n<p>Then you have to convert to a C array in the command line.</p>\n<pre><code class=\"language-bash\">xxd -i sine_model_quantized.tflite &gt; sine_model_quantized.cc</code></pre>\n<p>This is copy-paste code that hardly would change, so, for ease my development cycle, I wrapped this little snippet in a tiny package you can use: it's called <code>tinymlgen</code>.</p>\n<pre><code class=\"language-bash\">pip install tinymlgen</code></pre>\n<pre><code class=\"language-python\">from tinymlgen import port\n\nmodel = get_model()\nc_code = port(model, pretty_print=True)\nprint(c_code)</code></pre>\n<p>I point you to the <a href=\"https://github.com/eloquentarduino/tinymlgen\">Github repo</a> for a couple more options you can configure. </p>\n<p>Using this package, you don't have to open a terminal and use the <code>xxd</code> program to get a usable result.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2>Use the model</h2>\n<p>Now it is finally the time we deploy the model on our microcontroller. </p>\n<p>This part can be tricky, actually, if you don't have one of the supported boards in the book (Arduino Nano 33, SparkFun Edge or STM32F746G Discovery kit). </p>\n<p>I tried just setting &quot;ESP32&quot; as my target in the Arduino IDE and I got tons of errors.</p>\n<p>Luckily for us, a man called Wezley Sherman wrote a tutorial on <a href=\"https://towardsdatascience.com/tensorflow-meet-the-esp32-3ac36d7f32c7\">how to get a TinyML project to compile using the PlatformIO environment</a>. He saved me the effort to try to fix all the broken import errors on my own.</p>\n<p>Since I could get the project to compile using PlatformIO (which I don't use in my everyday tinkering), I settled to get the project to compile in the Arduino IDE.</p>\n<p>Fortunately, it was not difficult at all, so I can finally bring you this library that does all the heavy lifting for you.</p>\n<p>Thanks to the library, you won't need to download the full Tensorflow Lite framework and compile it on your own machine: it has been already done for you.</p>\n<p>As an added bonus, I created a wrapper class that incapsulates all the boring repetitive stuff, so you can focus solely on the application logic.</p>\n<p>Install the library from the library manager in the Arduino IDE: search for &quot;EloquentTinyML&quot;, or from <a href=\"https://github.com/eloquentarduino/EloquentTinyML\">Github</a> first.</p>\n<pre><code class=\"language-bash\">git clone https://github.com/eloquentarduino/EloquentTinyML.git</code></pre>\n<hr /><p><em>#EloquentTinyML escapes you from compiling Tensforflow on your own machine</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Feasy-tinyml-on-esp32-and-arduino%2F&#038;text=%23EloquentTinyML%20escapes%20you%20from%20compiling%20Tensforflow%20on%20your%20own%20machine&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Here is an example on how you use it.</p>\n<pre><code class=\"language-cpp\">#include &quot;EloquentTinyML.h&quot;\n// sine_model.h contains the array you exported from the previous step\n// with either xxd or tinymlgen\n#include &quot;sine_model.h&quot;\n\n#define NUMBER_OF_INPUTS 1\n#define NUMBER_OF_OUTPUTS 1\n// in future projects you may need to tweak this value\n// it&#039;s a trial and error process\n#define TENSOR_ARENA_SIZE 2*1024\n\nEloquent::TinyML::TfLite\\&lt;NUMBER_OF_INPUTS, NUMBER_OF_OUTPUTS, TENSOR_ARENA_SIZE&gt; ml(sine_model);\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    // pick up a random x and predict its sine\n    float x = 3.14 * random(100) / 100;\n    float y = sin(x);\n    float input[1] = { x };\n    float predicted = ml.predict(input);\n\n    Serial.print(&quot;sin(&quot;);\n    Serial.print(x);\n    Serial.print(&quot;) = &quot;);\n    Serial.print(y);\n    Serial.print(&quot;\\t predicted: &quot;);\n    Serial.println(predicted);\n    delay(1000);\n}</code></pre>\n<p>Does it look easy to use? I bet so.</p>\n<p>For simple cases like this example where you have a single output, the <code>predict</code> method returns that output so you can esaily assign it to a variable.</p>\n<p>If this is not the case and you expect multiple output from your model, you have to declare an output array.</p>\n<pre><code class=\"language-cpp\">float input[10] = { ... };\nfloat output[5] = { 0 };\n\nml.predict(input, output);</code></pre>\n<h2>Wrapping up</h2>\n<p>I hoped this post helped you kickstart your next TinyML project on your ESP32.</p>\n<p>It served me as a foundation for the next experiments I'm willing to do on this platform which is really in its early stages, so needs a lot of investigation about its capabilities.</p>\n<p>I plan to do a comparison with my MicroML framework when I get more experience in both, so staty tuned for the upcoming updates.</p>\n<h2>Disclaimer</h2>\n<p>I tested the library on both Ubuntu 18.04 and Windows 10 64 bit: if you are on a different platform and get compiling errors, please let me know in the comments so I can fix them.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/easy-tinyml-on-esp32-and-arduino/\">Easy TinyML on ESP32 and Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this post I will show you how to easily deploy your Tensorflow Lite model to an ESP32 using the Arduino IDE without any compilation stuff.\n\n\nSo I finally settled on giving a try to TinyML, which is a way to deploy Tensorflow Lite models to microcontrollers.\nAs a first step, I downloaded the free chapters from the TinyML book website and rapidly skimmed through them.\nLet me say that, even if it starts from &quot;too beginner&quot; level for me (they explain why you need to use the arrow instead of the point to access a pointer's property), it is a very well written book. They uncover every single aspect you may encounter during your first steps and give a very sound introduction to the general topic of training, validating and testing a dataset on a model.\nIf I will go on with this TinyML stuff, I'll probably buy a copy: I strongly recommend you to at least read the free sample.\nOnce done reading the 6 chapters, I wanted to try the described tutorial on my ESP32. Sadly, it is not mentioned in the supported boards on the book, so I had to solve it by myself.\nIn this post I'm going to make a sort of recap of my learnings about the steps you need to follow to implement TF models to a microcontroller and introduce you to a tiny library I wrote for the purpose of facilitating the deployment in the Arduino IDE: EloquentTinyML.\nBuilding our first model\nFirst of all, we need a model to deploy.\nThe book guides us on building a neural network capable of predicting the sine value of a given number, in the range from 0 to Pi (3.14).\nIt's an easy model to get started (the &quot;Hello world&quot; of machine learning, according to the authors), so we'll stick with it.\nI won't go into too much details about generating data and training the classifier, because I suppose you already know that part if you want to port Tensorflow on a microcontroller.\nHere's the code from the book.\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef get_model():\n    SAMPLES = 1000\n    np.random.seed(1337)\n    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n    # shuffle and add noise\n    np.random.shuffle(x_values)\n    y_values = np.sin(x_values)\n    y_values += 0.1 * np.random.randn(*y_values.shape)\n\n    # split into train, validation, test\n    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n    # create a NN with 2 layers of 16 neurons\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, activation=&#039;relu&#039;, input_shape=(1,)))\n    model.add(layers.Dense(16, activation=&#039;relu&#039;))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=&#039;rmsprop&#039;, loss=&#039;mse&#039;, metrics=[&#039;mae&#039;])\n    model.fit(x_train, y_train, epochs=200, batch_size=16,\n                        validation_data=(x_validate, y_validate))\n    return model\nExporting the model\nNow that we have a model, we need to convert it into a form ready to be deployed on our microcontroller. This is actually just an array of bytes that the TF interpreter will read to recreate the model.\nmodel = get_model()\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(&quot;sine_model_quantized.tflite&quot;, &quot;wb&quot;).write(tflite_model)\nThen you have to convert to a C array in the command line.\nxxd -i sine_model_quantized.tflite &gt; sine_model_quantized.cc\nThis is copy-paste code that hardly would change, so, for ease my development cycle, I wrapped this little snippet in a tiny package you can use: it's called tinymlgen.\npip install tinymlgen\nfrom tinymlgen import port\n\nmodel = get_model()\nc_code = port(model, pretty_print=True)\nprint(c_code)\nI point you to the Github repo for a couple more options you can configure. \nUsing this package, you don't have to open a terminal and use the xxd program to get a usable result.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nUse the model\nNow it is finally the time we deploy the model on our microcontroller. \nThis part can be tricky, actually, if you don't have one of the supported boards in the book (Arduino Nano 33, SparkFun Edge or STM32F746G Discovery kit). \nI tried just setting &quot;ESP32&quot; as my target in the Arduino IDE and I got tons of errors.\nLuckily for us, a man called Wezley Sherman wrote a tutorial on how to get a TinyML project to compile using the PlatformIO environment. He saved me the effort to try to fix all the broken import errors on my own.\nSince I could get the project to compile using PlatformIO (which I don't use in my everyday tinkering), I settled to get the project to compile in the Arduino IDE.\nFortunately, it was not difficult at all, so I can finally bring you this library that does all the heavy lifting for you.\nThanks to the library, you won't need to download the full Tensorflow Lite framework and compile it on your own machine: it has been already done for you.\nAs an added bonus, I created a wrapper class that incapsulates all the boring repetitive stuff, so you can focus solely on the application logic.\nInstall the library from the library manager in the Arduino IDE: search for &quot;EloquentTinyML&quot;, or from Github first.\ngit clone https://github.com/eloquentarduino/EloquentTinyML.git\n#EloquentTinyML escapes you from compiling Tensforflow on your own machineClick To Tweet\nHere is an example on how you use it.\n#include &quot;EloquentTinyML.h&quot;\n// sine_model.h contains the array you exported from the previous step\n// with either xxd or tinymlgen\n#include &quot;sine_model.h&quot;\n\n#define NUMBER_OF_INPUTS 1\n#define NUMBER_OF_OUTPUTS 1\n// in future projects you may need to tweak this value\n// it&#039;s a trial and error process\n#define TENSOR_ARENA_SIZE 2*1024\n\nEloquent::TinyML::TfLite\\&lt;NUMBER_OF_INPUTS, NUMBER_OF_OUTPUTS, TENSOR_ARENA_SIZE&gt; ml(sine_model);\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    // pick up a random x and predict its sine\n    float x = 3.14 * random(100) / 100;\n    float y = sin(x);\n    float input[1] = { x };\n    float predicted = ml.predict(input);\n\n    Serial.print(&quot;sin(&quot;);\n    Serial.print(x);\n    Serial.print(&quot;) = &quot;);\n    Serial.print(y);\n    Serial.print(&quot;\\t predicted: &quot;);\n    Serial.println(predicted);\n    delay(1000);\n}\nDoes it look easy to use? I bet so.\nFor simple cases like this example where you have a single output, the predict method returns that output so you can esaily assign it to a variable.\nIf this is not the case and you expect multiple output from your model, you have to declare an output array.\nfloat input[10] = { ... };\nfloat output[5] = { 0 };\n\nml.predict(input, output);\nWrapping up\nI hoped this post helped you kickstart your next TinyML project on your ESP32.\nIt served me as a foundation for the next experiments I'm willing to do on this platform which is really in its early stages, so needs a lot of investigation about its capabilities.\nI plan to do a comparison with my MicroML framework when I get more experience in both, so staty tuned for the upcoming updates.\nDisclaimer\nI tested the library on both Ubuntu 18.04 and Windows 10 64 bit: if you are on a different platform and get compiling errors, please let me know in the comments so I can fix them.\nL'articolo Easy TinyML on ESP32 and Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-25T20:36:29+01:00",
            "date_modified": "2020-02-16T10:50:45+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "url": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "title": "Apple or Orange? Image recognition with ESP32 and Arduino",
            "content_html": "<p>Do you have an ESP32 camera? </p>\n<p>Want to do image recognition directly on your ESP32, without a PC?</p>\n<p>In this post we'll look into a very basic image recognition task: <strong>distinguish apples from oranges with machine learning</strong>.</p>\n<p><img src=\"/wp-content/uploads/2020/01/Apple-vs-Orange.gif\" alt=\"Apple vs Orange\" /></p>\n<p><span id=\"more-820\"></span></p>\n<p>Image recognition is a very hot topic these days in the AI/ML landscape. <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">Convolutional Neural Networks</a> really shines in this task and can achieve almost perfect accuracy on many scenarios.</p>\n<p>Sadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.</p>\n<p>Since in this series about <a href=\"/category/programming/arduino-machine-learning/\">Machine Learning on Microcontrollers</a> we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.</p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-were-going-to-do\">What we're going to do</a><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocextracting-rgb-components\">Extracting RGB components</a><li><a href=\"#tocrecord-samples-image\">Record samples image</a><li><a href=\"#toctraining-the-classifier\">Training the classifier</a><li><a href=\"#tocreal-world-example\">Real world example</a><ol><li><a href=\"#tocdisclaimer\">Disclaimer</a></ol></div></p>\n<h2 id=\"tocwhat-were-going-to-do\">What we're going to do</h2>\n<p>In a previous post about <a href=\"/2019/12/color-identification-on-arduino/\">color identification with Machine learning</a>, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.</p>\n<p>Of course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.</p>\n<p>Object inference, in that case, works only if you have exactly one object for a given color.</p>\n<p>The objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.</p>\n<p>This is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.</p>\n<div class=\"watchout\">\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. <b>This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame</b>. It will classify the current image based on the samples recorded during training.\n</div>\n<p>As any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.</p>\n<h2 id=\"tocfeatures-definition\">Features definition</h2>\n<p>I have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.</p>\n<p>Those few times I used CNN, I always used the whole image as input, <em>as-is</em>. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.</p>\n<p>I don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.</p>\n<p>I said we're using the RGB components of the image. But not all of them.</p>\n<p>Even at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.</p>\n<p>We need to reduce this number  to the bare minimum.</p>\n<p>How much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?</p>\n<p>You would be surprised to know that I got 90% accuracy with an RGB image of <strong>8x6</strong>!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Orange-and-Apple-pixelated.jpg\" alt=\"You actually need very few pixels to do image classification\" /></p>\n<p>Yes, that's all we really need to do a <em>good enough</em> classification.</p>\n<hr /><p><em>You can distinguish apples from oranges on ESP32 with 8x6 pixels only!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fimage-recognition-with-esp32-and-arduino%2F&#038;text=You%20can%20distinguish%20apples%20from%20oranges%20on%20ESP32%20with%208x6%20pixels%20only%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Of course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.</p>\n<p>You have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.</p>\n<p>So we'll stick to 8x6 images.</p>\n<p>Now, how do you compact a 160x120 image to 8x6? With <em>downsampling</em>.</p>\n<p>This is the same tecnique we've used in the post about <a href=\"/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">motion detection on ESP32</a>: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).</p>\n<p><img src=\"/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>This time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.</p>\n<p>This is the code excerpt that does the downsampling.</p>\n<pre><code class=\"language-cpp\">uint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}</code></pre>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2 id=\"tocextracting-rgb-components\">Extracting RGB components</h2>\n<p>The ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):</p>\n<ol>\n<li><strong>grayscale</strong>: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH</li>\n<li><strong>RGB565</strong>: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2</li>\n<li><strong>JPEG</strong>: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results</li>\n</ol>\n<p>For our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.</p>\n<p><img src=\"https://s2.www.theimagingsource.com/application-1.1.29/documentation/ic_imaging_control_class/en_US/images/rgb565.gif\" alt=\"taken from https://www.theimagingsource.com/support/documentation/ic-imaging-control-cpp/PixelformatRGB565.htm\" /></p>\n<pre><code class=\"language-cpp\">config.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}</code></pre>\n<h2 id=\"tocrecord-samples-image\">Record samples image</h2>\n<p>Now that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.</p>\n<p>Before doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.</p>\n<pre><code class=\"language-cpp\">#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}</code></pre>\n<p>Now you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.</p>\n<div class=\"watchout\">\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. <br />You need to be very accurate during this phase if you want to achieve good results.<br /> I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n</div>\n<h2 id=\"toctraining-the-classifier\">Training the classifier</h2>\n<p>To train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\">how to train a ML classifier for Arduino</a> to get the exported model.</p>\n<p>You can experiment with different classifier configurations. </p>\n<p>My features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).</p>\n<p>One odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.</p>\n<p>The model produced 13 support vectors.</p>\n<p>I did no features scaling: you could try it if classifying more than 2 classes and having poor results.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange-decision-boundaries.png\" alt=\"Apple vs Orange decision boundaries\" /></p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>If you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.</p>\n<div style=\"width: 788px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-820-3\" width=\"788\" height=\"443\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4?_=3\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4</a></video></div>\n<p></p>\n<p>The little white object you see at the bottom of the image is the camera, taped to the desk.</p>\n<p>Did you think it was possible to do simple image classification on your ESP32?</p>\n<h3 id=\"tocdisclaimer\">Disclaimer</h3>\n<p>This is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.</p>\n<p>You have to carefully craft your setup and be as consistent as possible between training and inferencing.</p>\n<p>Still, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.</p>\n<p>In the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.</p>\n<p>Now that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? </p>\n<p>Let me know in the comments, we could even try realize it together if you need some help.</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/Apple_vs_Orange/Apple_vs_Orange.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/\">Apple or Orange? Image recognition with ESP32 and Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? \nWant to do image recognition directly on your ESP32, without a PC?\nIn this post we'll look into a very basic image recognition task: distinguish apples from oranges with machine learning.\n\n\nImage recognition is a very hot topic these days in the AI/ML landscape. Convolutional Neural Networks really shines in this task and can achieve almost perfect accuracy on many scenarios.\nSadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.\nSince in this series about Machine Learning on Microcontrollers we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.\nTable of contentsWhat we're going to doFeatures definitionExtracting RGB componentsRecord samples imageTraining the classifierReal world exampleDisclaimer\nWhat we're going to do\nIn a previous post about color identification with Machine learning, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.\nOf course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.\nObject inference, in that case, works only if you have exactly one object for a given color.\nThe objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.\nThis is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.\n\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame. It will classify the current image based on the samples recorded during training.\n\nAs any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.\nFeatures definition\nI have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.\nThose few times I used CNN, I always used the whole image as input, as-is. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.\nI don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.\nI said we're using the RGB components of the image. But not all of them.\nEven at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.\nWe need to reduce this number  to the bare minimum.\nHow much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?\nYou would be surprised to know that I got 90% accuracy with an RGB image of 8x6!\n\nYes, that's all we really need to do a good enough classification.\nYou can distinguish apples from oranges on ESP32 with 8x6 pixels only!Click To Tweet\nOf course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.\nYou have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.\nSo we'll stick to 8x6 images.\nNow, how do you compact a 160x120 image to 8x6? With downsampling.\nThis is the same tecnique we've used in the post about motion detection on ESP32: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).\n\nThis time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.\nThis is the code excerpt that does the downsampling.\nuint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nExtracting RGB components\nThe ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):\n\ngrayscale: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH\nRGB565: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2\nJPEG: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results\n\nFor our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.\n\nconfig.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}\nRecord samples image\nNow that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.\nBefore doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.\n#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}\nNow you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.\n\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. You need to be very accurate during this phase if you want to achieve good results. I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n\nTraining the classifier\nTo train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on how to train a ML classifier for Arduino to get the exported model.\nYou can experiment with different classifier configurations. \nMy features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).\nOne odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.\nThe model produced 13 support vectors.\nI did no features scaling: you could try it if classifying more than 2 classes and having poor results.\n\nReal world example\nIf you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\n\nThe little white object you see at the bottom of the image is the camera, taped to the desk.\nDid you think it was possible to do simple image classification on your ESP32?\nDisclaimer\nThis is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.\nYou have to carefully craft your setup and be as consistent as possible between training and inferencing.\nStill, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.\nIn the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.\nNow that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? \nLet me know in the comments, we could even try realize it together if you need some help.\n\r\nCheck the full project code on Github\nL'articolo Apple or Orange? Image recognition with ESP32 and Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-12T11:32:08+01:00",
            "date_modified": "2020-02-10T13:32:21+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "microml",
                "Arduino Machine learning",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 1642079
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "url": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "title": "Motion detection with ESP32 cam only (Arduino version)",
            "content_html": "<p>Do you have an <strong>ESP32 camera</strong>? Do you want to do motion detection <em>WITHOUT ANY</em> external hardware?</p>\n<p>Here's a tutorial made just for you: <strong>30 lines of code</strong> and you will know when something changes in your video stream  \ud83c\udfa5</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.gif\" alt=\"ESP32 camera motion detection example\" /></p>\n<p><span id=\"more-779\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</a><li><a href=\"#toccant-i-use-an-external-pir\">Can't I use an external PIR?</a><ol><li><a href=\"#tocexternal-hardware\">External hardware</a><li><a href=\"#tocfield-of-view\">Field of View</a><li><a href=\"#toccold-objects\">Cold objects</a></li></ol><li><a href=\"#tocwhat-do-you-need\">What do you need?</a><li><a href=\"#tochow-does-it-work\">How does it work?</a><ol><li><a href=\"#tocdownsampling\">Downsampling</a><li><a href=\"#tocblocks-difference-threshold\">Blocks difference threshold</a><li><a href=\"#tocimage-difference-threshold\">Image difference threshold</a><li><a href=\"#toccombining-all-together\">Combining all together</a></li></ol><li><a href=\"#tocreal-world-example\">Real world example</a></ol></div></p>\n<h2 id=\"tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</h2>\n<p>Quoting from Wikipedia</p>\n<blockquote>\n<p>Motion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object</p>\n</blockquote>\n<p>In this project, we're implementing what I call <em>naive</em> motion detection: that is, we're not focusing on a particular object and following its motion.</p>\n<p>We'll only detect if any considerable portion of the image changed from one frame to the next.</p>\n<p>We won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.</p>\n<h2 id=\"toccant-i-use-an-external-pir\">Can't I use an external PIR?</h2>\n<p>Several projects on the internet about motion detection with an ESP32 cam use an external <a href=\"https://en.wikipedia.org/wiki/Passive_infrared_sensor\">PIR sensor</a> to trigger the video recording.</p>\n<p>What's the problem with that approach? </p>\n<h3 id=\"tocexternal-hardware\">1. External hardware</h3>\n<p>First of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice <a href=\"https://www.banggood.com/M5CameraF-ESP32-Fish-eye-Camera-Development-Board-Module-OV2640-Mini-Fisheye-Camera-Unit-Demoboard-p-1496820.html?rmmds=search&amp;cur_warehouse=CN\">M5stick camera</a> (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.</p>\n<h3 id=\"tocfield-of-view\">2. Field of View</h3>\n<p>PIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. </p>\n<p>My camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.</p>\n<h3 id=\"toccold-objects\">3. Cold objects</h3>\n<p>PIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).</p>\n<p>But motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.</p>\n<p>A PIR sensor can't do this: video motion detection can.</p>\n<hr /><p><em>ESP32 cam pure video motion detection can detect motion due to cold objects</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fmotion-detection-with-esp32-cam-only-arduino-version%2F&#038;text=ESP32%20cam%20pure%20video%20motion%20detection%20can%20detect%20motion%20due%20to%20cold%20objects&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<div class=\"infobox\"> Do you like the motion effect at the beginning of the post? <a href=\"https://gist.github.com/eloquentarduino/6bb0b26a3900d7fac68b2f3cc7b2c688\">Check it out on Github</a></div>\n<h2 id=\"tocwhat-do-you-need\">What do you need?</h2>\n<p>All you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:</p>\n<ul>\n<li>ESP32 cam</li>\n<li>ESP32 eye</li>\n<li>TTGO camera</li>\n<li>... any other flavor of ESP32 camera</li>\n</ul>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-models.jpg\" alt=\"ESP32 camera models\" /></p>\n<h2 id=\"tochow-does-it-work\">How does it work?</h2>\n<p>Ok, let's go to the &quot;technical&quot; stuff.</p>\n<p>Simply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.</p>\n<p>Well, it's <em>almost</em> like this.</p>\n<p>Of course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.</p>\n<h3 id=\"tocdownsampling\">Downsampling</h3>\n<p>One super-simple and super-effective way of doing this is to <strong>work with blocks</strong>, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.</p>\n<p>This greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>It's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  </p>\n<p>This has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.</p>\n<p>How should you choose the scale factor?</p>\n<p>Well, it depends.</p>\n<p>It depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. </p>\n<p>If you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.</p>\n<p>Experiment with your own use case a tweak with trial-and-error.</p>\n<h3 id=\"tocblocks-difference-threshold\">Blocks difference threshold</h3>\n<p>Once we've defined the block size, we need to detect if a block changed from one frame to the next.</p>\n<p>Of course, just testing for difference (<code>current != prev</code>) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.</p>\n<p>So we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.</p>\n<p>The higher the threshold, the less sensitive the algorithm will be.</p>\n<p>In code it is calculated as</p>\n<pre><code class=\"language-cpp\">float delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;</code></pre>\n<p>which indicates the relative increment/decrement from the previous value.</p>\n<h3 id=\"tocimage-difference-threshold\">Image difference threshold</h3>\n<p>Now that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.</p>\n<p>You could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.</p>\n<p>Let's return to the 320x240 image example. With a 10x10 block, you'll be working with <code>32x24 = 768</code> blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?</p>\n<p>I don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.</p>\n<p>If you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.</p>\n<p>In code it is calculated as:</p>\n<pre><code class=\"language-cpp\">float changedBlocksPercent = changedBlocks / totalBlocks</code></pre>\n<h3 id=\"toccombining-all-together\">Combining all together</h3>\n<p>Recapping: when running the motion detection algorithm you have 3 parameters to set:</p>\n<ol>\n<li>the block size</li>\n<li>the block difference threshold</li>\n<li>the image differerence threshold</li>\n</ol>\n<p>Let's pick 3 sensible defaults: <code>block size = 10</code>, <code>block threshold = 15%</code>, <code>image threshold = 20%</code>.</p>\n<p>What does these parameters translate to in the practice?</p>\n<p>They mean that motion will be detected if <code>20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next</code>.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-example.jpg\" alt=\"ESP32 camera motion example\" /></p>\n<p>As you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.</p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>Now the fun part. I'll show you how it performs on a real-world scenario.</p>\n<p>To keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. </p>\n<p>This means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.</p>\n<div style=\"width: 652px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-779-4\" width=\"652\" height=\"604\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4?_=4\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4</a></video></div>\n<p>This is me passing my arm in front of the camera a few times.</p>\n<p>The grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.</p>\n<p>The orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.</p>\n<p>When I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. </p>\n<p>While moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.</p>\n<p>This is why I suggest you keep the <code>image difference threshold</code> to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.</p>\n<p>Do you like the grid effect of the sample video? Let me know in the comment if you want me to share it.</p>\n<p>Or even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ESP32CameraNaiveMotionDetection/ESP32CameraNaiveMotionDetection.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>Check out also the <a href=\"https://gist.github.com/eloquentarduino/6bb0b26a3900d7fac68b2f3cc7b2c688\">gist for the visualization tool</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">Motion detection with ESP32 cam only (Arduino version)</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? Do you want to do motion detection WITHOUT ANY external hardware?\nHere's a tutorial made just for you: 30 lines of code and you will know when something changes in your video stream  \ud83c\udfa5\n\n\nTable of contentsWhat is (naive) motion detection?Can't I use an external PIR?External hardwareField of ViewCold objectsWhat do you need?How does it work?DownsamplingBlocks difference thresholdImage difference thresholdCombining all togetherReal world example\nWhat is (naive) motion detection?\nQuoting from Wikipedia\n\nMotion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object\n\nIn this project, we're implementing what I call naive motion detection: that is, we're not focusing on a particular object and following its motion.\nWe'll only detect if any considerable portion of the image changed from one frame to the next.\nWe won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.\nCan't I use an external PIR?\nSeveral projects on the internet about motion detection with an ESP32 cam use an external PIR sensor to trigger the video recording.\nWhat's the problem with that approach? \n1. External hardware\nFirst of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice M5stick camera (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.\n2. Field of View\nPIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. \nMy camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.\n3. Cold objects\nPIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).\nBut motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.\nA PIR sensor can't do this: video motion detection can.\nESP32 cam pure video motion detection can detect motion due to cold objectsClick To Tweet\n Do you like the motion effect at the beginning of the post? Check it out on Github\nWhat do you need?\nAll you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:\n\nESP32 cam\nESP32 eye\nTTGO camera\n... any other flavor of ESP32 camera\n\n\nHow does it work?\nOk, let's go to the &quot;technical&quot; stuff.\nSimply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.\nWell, it's almost like this.\nOf course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.\nDownsampling\nOne super-simple and super-effective way of doing this is to work with blocks, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.\nThis greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.\n\nIt's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  \nThis has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.\nHow should you choose the scale factor?\nWell, it depends.\nIt depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. \nIf you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.\nExperiment with your own use case a tweak with trial-and-error.\nBlocks difference threshold\nOnce we've defined the block size, we need to detect if a block changed from one frame to the next.\nOf course, just testing for difference (current != prev) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.\nSo we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.\nThe higher the threshold, the less sensitive the algorithm will be.\nIn code it is calculated as\nfloat delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;\nwhich indicates the relative increment/decrement from the previous value.\nImage difference threshold\nNow that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.\nYou could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.\nLet's return to the 320x240 image example. With a 10x10 block, you'll be working with 32x24 = 768 blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?\nI don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.\nIf you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.\nIn code it is calculated as:\nfloat changedBlocksPercent = changedBlocks / totalBlocks\nCombining all together\nRecapping: when running the motion detection algorithm you have 3 parameters to set:\n\nthe block size\nthe block difference threshold\nthe image differerence threshold\n\nLet's pick 3 sensible defaults: block size = 10, block threshold = 15%, image threshold = 20%.\nWhat does these parameters translate to in the practice?\nThey mean that motion will be detected if 20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next.\n\nAs you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.\nReal world example\nNow the fun part. I'll show you how it performs on a real-world scenario.\nTo keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. \nThis means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\nThis is me passing my arm in front of the camera a few times.\nThe grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.\nThe orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.\nWhen I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. \nWhile moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.\nThis is why I suggest you keep the image difference threshold to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.\nDo you like the grid effect of the sample video? Let me know in the comment if you want me to share it.\nOr even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n\r\nCheck the full project code on Github\nCheck out also the gist for the visualization tool\nL'articolo Motion detection with ESP32 cam only (Arduino version) proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-05T12:08:08+01:00",
            "date_modified": "2020-03-08T11:43:11+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 1673368
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/",
            "url": "https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/",
            "title": "Embedded Machine learning on Attiny85",
            "content_html": "<p>You won't believe it, but <strong>you can run Machine learning on embedded systems like an Attiny85</strong> (and many others Attiny)!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/attiny85_ml.png\" alt=\"ri-elaborated from https://cyaninfinite.com/miniaturize-projects-with-attiny85/\" /></p>\n<p><span id=\"more-688\"></span></p>\n<p>When I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of RAM and you can buy a chinese clone for around 2.50 $.</p>\n<p>It already opened the path to a embedded machine learning at a new scale, given the huge amount of microcontrollers ready to become &quot;intelligent&quot;. </p>\n<p>But it was not enough for me: after all, the <a href=\"/2019/11/how-to-create-a-classifier-for-arduino-machine-learning-projects/\">MicroML generator</a> exports plain C that should run on any embedded system, not only on Arduino boards.</p>\n<p>So I setup to test if I could go even smaller and run it on the #1 of tiny chips: the Attiny85.</p>\n<hr /><p><em>MicroML exports plain C that could run anywhere, not only on Arduino boards.</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=MicroML%20exports%20plain%20C%20that%20could%20run%20anywhere%2C%20not%20only%20on%20Arduino%20boards.&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>No, I couldn't.</p>\n<p>The generated code makes use of a variadic function, which seems to not be supported by the Attiny compiler in the Arduino IDE.</p>\n<p>So I had to come up with an alternative implementation to make it work.</p>\n<p>Fortunately I already experimented with a non-variadic version when first writing the porter, so it was a matter of refreshing that algorithm and try it out.</p>\n<p>Guess what? It compiled!</p>\n<p>So I tried porting one my earliear tutorial (the <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\">color identification</a> one) to the Attiny and...</p>\n<hr /><p><em>Boom! Machine learning on an Attiny85!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=Boom%21%20Machine%20learning%20on%20an%20Attiny85%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Here's a step-by-step tutorial on how you can do it too.</p>\n<div class=\"watchout\">\nI strongly suggest you read <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\" target=\"_blank\" rel=\"noopener noreferrer\">the original tutorial</a> before following this one, because I won't go into too much details on the common steps here.\n</div>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><li><a href=\"#tocrun-the-inference\">Run the inference</a><li><a href=\"#tocproject-figures\">Project figures</a></ol></div></p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>We're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be 3-dimensional, which leads to a really simple model with very high accuracy.</p>\n<div class=\"watchout\">\nThe Attiny85 has 8 Kb of flash and 512 bytes of RAM, so you won't be able to load any model that uses more than a few features (probably less than 10).\n</div>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<p>You must do this step on a board with a Serial interface, like an Arduino Uno / Nano / Pro Mini. See <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\" target=\"_blank\" rel=\"noopener noreferrer\">the original tutorial</a> for the code of this step.</p>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\n<p>This part is exactly the same as the original, except for a single parameter: you will pass <code>platform=attiny</code> to the <code>port</code> function.</p>\n<pre><code class=\"language-python\">from sklearn.svm import SVC\nfrom micromlgen import port\n\n# put your samples in the dataset folder\n# one class per file\n# one feature vector per line, in CSV format\nfeatures, classmap = load_features(&#039;dataset/&#039;)\nX, y = features[:, :-1], features[:, -1]\nclassifier = SVC(kernel=&#039;linear&#039;).fit(X, y)\nc_code = port(classifier, classmap=classmap, platform=&#039;attiny&#039;)\nprint(c_code)</code></pre>\n<div class=\"watchout\">\nThe Attiny mode has been implemented in version 0.8 of micromlgen: if you installed an earlier version, first update\n</div>\n<p>At this point you have to copy the printed code and import it in your project, in a file called <code>model.h</code>.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<p>Since we don't have a Serial, we will blink a LED a number of times dependant on the prediction result.</p>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\n#define LED 0\n\nvoid loop() {\n  readRGB();\n  classify();\n  delay(1000);\n}\n\nvoid classify() {\n    for (uint8_t times = predict(features) + 1; times &gt; 0; times--) {\n        digitalWrite(LED, HIGH);\n        delay(10);\n        digitalWrite(LED, LOW);\n        delay(10);\n    }\n}</code></pre>\n<p>Here we are: put some colored object in front of the sensor and see the LED blink.</p>\n<h2 id=\"tocproject-figures\">Project figures</h2>\n<p>On my machine, the sketch requires 3434 bytes (41%) of program space and 21 bytes (4%) of RAM. This means you could actually run machine learning in even less space than what the Attiny85 provides. </p>\n<p>This model in particular it's so tiny you <strong>can run in even on an Attiny45, which has only 4 Kb of flash and 256 bytes of RAM</strong>.</p>\n<p>I'd like you to look at the RAM figure for a moment: <strong>21 bytes</strong>. 21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller. This is the result of the implementation I chose: the least RAM overhead possible. I challenge you to go any lower than this.</p>\n<hr /><p><em>21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=21%20bytes%20is%20all%20the%20memory%20you%20need%20to%20run%20a%20Machine%20learning%20algorithm%20on%20a%20microcontroller&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlColorIdentificationAttinyExample/MicromlColorIdentificationAttinyExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/\">Embedded Machine learning on Attiny85</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "You won't believe it, but you can run Machine learning on embedded systems like an Attiny85 (and many others Attiny)!\n\n\nWhen I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of RAM and you can buy a chinese clone for around 2.50 $.\nIt already opened the path to a embedded machine learning at a new scale, given the huge amount of microcontrollers ready to become &quot;intelligent&quot;. \nBut it was not enough for me: after all, the MicroML generator exports plain C that should run on any embedded system, not only on Arduino boards.\nSo I setup to test if I could go even smaller and run it on the #1 of tiny chips: the Attiny85.\nMicroML exports plain C that could run anywhere, not only on Arduino boards.Click To Tweet\nNo, I couldn't.\nThe generated code makes use of a variadic function, which seems to not be supported by the Attiny compiler in the Arduino IDE.\nSo I had to come up with an alternative implementation to make it work.\nFortunately I already experimented with a non-variadic version when first writing the porter, so it was a matter of refreshing that algorithm and try it out.\nGuess what? It compiled!\nSo I tried porting one my earliear tutorial (the color identification one) to the Attiny and...\nBoom! Machine learning on an Attiny85!Click To Tweet\nHere's a step-by-step tutorial on how you can do it too.\n\nI strongly suggest you read the original tutorial before following this one, because I won't go into too much details on the common steps here.\n\nTable of contentsFeatures definitionRecord sample dataTrain and export the SVM classifierRun the inferenceProject figures\n1. Features definition\nWe're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be 3-dimensional, which leads to a really simple model with very high accuracy.\n\nThe Attiny85 has 8 Kb of flash and 512 bytes of RAM, so you won't be able to load any model that uses more than a few features (probably less than 10).\n\n2. Record sample data\nYou must do this step on a board with a Serial interface, like an Arduino Uno / Nano / Pro Mini. See the original tutorial for the code of this step.\n3. Train and export the SVM classifier\nThis part is exactly the same as the original, except for a single parameter: you will pass platform=attiny to the port function.\nfrom sklearn.svm import SVC\nfrom micromlgen import port\n\n# put your samples in the dataset folder\n# one class per file\n# one feature vector per line, in CSV format\nfeatures, classmap = load_features(&#039;dataset/&#039;)\nX, y = features[:, :-1], features[:, -1]\nclassifier = SVC(kernel=&#039;linear&#039;).fit(X, y)\nc_code = port(classifier, classmap=classmap, platform=&#039;attiny&#039;)\nprint(c_code)\n\nThe Attiny mode has been implemented in version 0.8 of micromlgen: if you installed an earlier version, first update\n\nAt this point you have to copy the printed code and import it in your project, in a file called model.h.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n4. Run the inference\nSince we don't have a Serial, we will blink a LED a number of times dependant on the prediction result.\n#include &quot;model.h&quot;\n\n#define LED 0\n\nvoid loop() {\n  readRGB();\n  classify();\n  delay(1000);\n}\n\nvoid classify() {\n    for (uint8_t times = predict(features) + 1; times &gt; 0; times--) {\n        digitalWrite(LED, HIGH);\n        delay(10);\n        digitalWrite(LED, LOW);\n        delay(10);\n    }\n}\nHere we are: put some colored object in front of the sensor and see the LED blink.\nProject figures\nOn my machine, the sketch requires 3434 bytes (41%) of program space and 21 bytes (4%) of RAM. This means you could actually run machine learning in even less space than what the Attiny85 provides. \nThis model in particular it's so tiny you can run in even on an Attiny45, which has only 4 Kb of flash and 256 bytes of RAM.\nI'd like you to look at the RAM figure for a moment: 21 bytes. 21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller. This is the result of the implementation I chose: the least RAM overhead possible. I challenge you to go any lower than this.\n21 bytes is all the memory you need to run a Machine learning algorithm on a microcontrollerClick To Tweet\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo Embedded Machine learning on Attiny85 proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-23T17:57:15+01:00",
            "date_modified": "2020-01-25T17:17:04+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning"
            ]
        }
    ]
}