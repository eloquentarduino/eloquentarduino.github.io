<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Eloquent Arduino Blog</title>
	<atom:link href="https://eloquentarduino.github.io/feed/" rel="self" type="application/rss+xml" />
	<link>http://eloquentarduino.github.io/</link>
	<description>Machine learning on Arduino, programming &#38; electronics</description>
	<lastBuildDate>Sat, 17 Oct 2020 15:50:13 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.4</generator>
	<item>
		<title>&#8220;Principal&#8221; FFT components as efficient features extrator</title>
		<link>https://eloquentarduino.github.io/2020/09/principal-fft-components-as-efficient-features-extrator/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sat, 05 Sep 2020 08:52:02 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1297</guid>

					<description><![CDATA[<p>Fourier Transform is probably the most well known algorithm for feature extraction from time-dependent data (in particular speech data), where frequency holds a great deal of information. Sadly, computing the transform over the whole spectrum of the signal still requires O(NlogN) with the best implementation (FFT - Fast Fourier Transform); we would like to achieve [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/09/principal-fft-components-as-efficient-features-extrator/">&#8220;Principal&#8221; FFT components as efficient features extrator</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier Transform</a> is probably the most well known algorithm for feature extraction from time-dependent data (in particular speech data), where frequency holds a great deal of information. Sadly, computing the transform over the whole spectrum of the signal still requires O(NlogN) with the best implementation (<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT - Fast Fourier Transform</a>); we would like to achieve faster computation on our microcontrollers.</p>
<p>In this post I propose a partial, naive <strong>linear-time</strong> implementation of the Fourier Transform you can use to extract features from your data for Machine Learning models.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/09/FFT-spectrum.png" alt="FFT spectrum example" /></p>
<p><span id="more-1297"></span></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#toctraining-aware-fft">Training-aware FFT</a><li><a href="#tocaccuracy-comparison">Accuracy comparison</a><li><a href="#tochow-to-use-principal-fft-in-python">How to use Principal FFT in Python</a><li><a href="#tochow-to-use-principal-fft-in-c">How to use Principal FFT in C</a><ol><li><a href="#tocbenchmarking">Benchmarking</a></ol></div></p>
<p><strong>DISCLAIMER</strong></p>
<p><em>The contents of this post represent my own knowledge and are not supported by any academic work (as far as I know). It may really be the case that the findings of my work don't apply to your own projects; yet, I think this idea can turn useful in solving certain kind of problems.</em></p>
<h2 id="toctraining-aware-fft">Training-aware FFT</h2>
<p>Fourier transform is used to describe a signal over its entire frequency range. This is useful in a number of applications, but here we're focused on the FT for the sole purpose of extracting features to be used with Machine learning models.</p>
<p>For this reason, we don't actually need a full description of the input signal: we're only interested in extracting some kind of signature that a ML model can use to distinguish among the different classes. Noticing that in a signal spectrum most frequencies have a low magnitude (as you can see in the picture above), the idea to only keep the <em>most important</em> frequencies came to my mind as a mean to speed up the computation on resource constrained microcontrollers.</p>
<p>I was thinking to a kind of PCA (Principal Component Analysis), but using FFT spectrum as features.</p>
<p>Since we will have a training set with the raw signals, we would like to select the most prominent frequencies among all the samples and apply the computation only on those: even using the naive implementation of FFT, this will yield a linear-time implementation.</p>
<h2 id="tocaccuracy-comparison">Accuracy comparison</h2>
<p>How does this <em>Principal FFT</em> compare to, let's say, PCA as a dimensionality reduction algorithm w.r.t model accuracy? Let's see the numbers!</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/09/FFT-vs-PCA.png" alt="FFT vs PCA accuracy comparison on various datasets" /></p>
<p><a href="/wp-content/uploads/2020/09/Principal-FFT-benchmark.ods" title="Principal FFT benchmark spreadsheet">Download the Principal FFT benchmark spreadsheet</a></p>
<p>I couldn't find many examples of the kind of datasets I wished to test, but in the image you can see different types of data:</p>
<ul>
<li>human activity classification from smartphone data</li>
<li>gesture classification by IMU data</li>
<li>MNIST handwritten digits image data</li>
<li>free speech audio data</li>
</ul>
<p>We can note a couple findings:</p>
<ol>
<li>Principal FFT is almost on par with PCA after a certain number of components</li>
<li>PrincipalFFT definitely leaves PCA behind on audio data</li>
</ol>
<p>From even this simple analysis you should be convinced that Principal FFT can be (under certain cases) a fast, performant features extractor for your projects that involve time-dependant data.</p>
<h2 id="tochow-to-use-principal-fft-in-python">How to use Principal FFT in Python</h2>
<p>I created a Python package to use Principal FFT, called <code>principal-fft</code>.</p>
<pre><code class="language-bash">pip install principal-fft</code></pre>
<p>The class follows the <code>Transformer</code> API from <code>scikit-learn</code>, so it has <code>fit</code> and <code>transform</code> methods.</p>
<pre><code class="language-python">from principalfft import PrincipalFFT
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

mnist = load_digits()
X, y = mnist.data, mnist.target
Xfft = PrincipalFFT(n_components=10).fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
Xfft_train, Xfft_test, y_train, y_test = train_test_split(Xfft, y, test_size=0.3)

clf = RandomForestClassifier(50, min_samples_leaf=5).fit(X_train, y_train)
print(&quot;Raw score&quot;, clf.score(X_test, y_test))

clf = RandomForestClassifier(50, min_samples_leaf=5).fit(Xfft_train, y_train)
print(&quot;FFT score&quot;, clf.score(Xfft_test, y_test))</code></pre>
<p>My results are <code>0.09</code> for raw data and <code>0.78</code> for FFT transformed: quite a big difference!</p>
<p>As with any dimensionality reduction, <code>n_components</code> is an hyperparameter you have to tune for your specific project: from my experiments, you shouldn't go lower than <code>8</code> to achieve a reasonable accuracy.</p>
<h2 id="tochow-to-use-principal-fft-in-c">How to use Principal FFT in C</h2>
<p>So, now that we tested our Principal FFT transformer in Python and achieved good results, how do we use it on our microcontroller? Of course with the <code>micromlgen</code> porter: it is now (<code>version 1.1.9</code>) able to port PrincipalFFT objects to plain C.</p>
<pre><code class="language-bash">pip install micromlgen==1.1.9</code></pre>
<p>What does the C code look like?</p>
<pre><code class="language-cpp">void principalFFT(float *features, float *fft) {
    // apply principal FFT (naive implementation for the top 10 frequencies only)
    const int topFrequencies[] = { 0, 8, 17, 16, 1, 9, 2, 7, 15, 6 };

    for (int i = 0; i &lt; 10; i++) {
        const int k = topFrequencies[i];
        const float harmonic = 0.09817477042468103 * k;
        float re = 0;
        float im = 0;

        // optimized case
        if (k == 0) {
            for (int n = 0; n &lt; 64; n++) {
                re += features[n];
            }
        }

        else {
            for (int n = 0; n &lt; 64; n++) {
                const float harmonic_n = harmonic * n;
                const float cos_n = cos(harmonic_n);
                const float sin_n = sin(harmonic_n);
                re += features[n] * cos_n;
                im -= features[n] * sin_n;
            }
        }

        fft[i] = sqrt(re * re + im * im);
    }
}</code></pre>
<p>This is the most direct porting available.</p>
<p>In the <em>Benchmarks</em> section, we'll see how this implementation can be speed-up with alternative implementations.</p>
<h3 id="tocbenchmarking">Benchmarking</h3>
<p>The following table reports the benchmark on the MNIST dataset (64 features) with 10 principal FFT components vs various tecniques to decrease the computation time at the expense of memory usage.</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th style="text-align: center;">Flash (Kb)</th>
<th style="text-align: center;">Execution time (micros)</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td style="text-align: center;">137420</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td>arduinoFFT library</td>
<td style="text-align: center;">147812</td>
<td style="text-align: center;">3200</td>
</tr>
<tr>
<td>principalFFT</td>
<td style="text-align: center;">151404</td>
<td style="text-align: center;">4400</td>
</tr>
<tr>
<td>principalFFT w/ cos+sin LUT</td>
<td style="text-align: center;">152124</td>
<td style="text-align: center;">900</td>
</tr>
<tr>
<td>principalFFT w/ cos LUT + sin sign LUT</td>
<td style="text-align: center;">150220</td>
<td style="text-align: center;">1250</td>
</tr>
</tbody>
</table>
<p>*<em>all the benchmarks were run on the Arduino 33 Nano BLE Sense</em></p>
<p>Some thoughts:</p>
<ol start="2">
<li><code>principalFFT w/ cos+sin LUT</code> means I pre-compute the values of <code>sin</code> and <code>cos</code> at compile time, so there's no computation on the board; of course these lookup tables will eat some memory</li>
<li><code>principalFFT w/ cos LUT + sin sign LUT</code> means I pre-compute the <code>cos</code> values only and compute <code>sin</code> using <code>sqrt(1 - cos(x)^2)</code>; it adds some microseconds to the computation, but requires less memory</li>
<li><code>arduinoFFT library</code> is faster than <code>principalFFT</code> in the execution time and requires less memory, even if <code>principalFFT</code> is only computing 10 frequencies: I need to investigate how it can achieve such performances</li>
</ol>
<p>You can activate the LUT functionality with:</p>
<pre><code class="language-python">from micromlgen import port
from principalfft import PrincipalFFT

fft = PrincipalFFT(n_components=10).fit(X)

# cos lookup, sin computed
port(fft, lookup_cos=True)

# cos + sin lookup
port(fft, lookup_cos=True, lookup_sin=True)</code></pre>
<p>Here's how the C code looks like with LUT.</p>
<pre><code class="language-cpp">void principalFFT(float *features, float *fft) {
    // apply principal FFT (naive implementation for the top N frequencies only)
    const int topFrequencies[] = { 0, 8, 17, 16, 1, 9, 2, 7, 15, 6 };
    const float cosLUT[10][64] = {
        {  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0,  1.0},
        {  1.0,  0.7071,  6.1232e-17,  -0.7071,  -1.0,  -0.7071,  -1.8369e-16,  0.7071,  1.0,  0.7071,  3.0616e-16,  -0.7071,  -1.0,  -0.7071,  -4.2862e-16,  0.7071,  1.0,  0.7071,  5.5109e-16,  -0.7071,  -1.0,  -0.7071,  -2.4499e-15,  0.7071,  1.0,  0.7071,  -9.8033e-16,  -0.7071,  -1.0,  -0.7071,  -2.6948e-15,  0.7071,  1.0,  0.7071,  -7.3540e-16,  -0.7071,  -1.0,  -0.7071,  -2.9397e-15,  0.7071,  1.0,  0.7071,  -4.9047e-16,  -0.7071,  -1.0,  -0.7071,  -3.1847e-15,  0.7071,  1.0,  0.7071,  -2.4554e-16,  -0.7071,  -1.0,  -0.7071,  -3.4296e-15,  0.7071,  1.0,  0.7071,  -6.1898e-19,  -0.7071,  -1.0,  -0.7071,  -3.6745e-15,  0.7071},   ... };
    const bool sinLUT[10][64] = {
        {  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false,  false},
        {  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false,  false,  true,  true,  true,  false,  false,  false,  false,  false,  true,  true,  true,  true,  false,  false,  false},  ...};

    for (int i = 0; i &lt; 10; i++) {
        const int k = topFrequencies[i];
        const float harmonic = 0.09817477042468103 * k;
        float re = 0;
        float im = 0;
        // optimized case
        if (k == 0) {
            for (int n = 0; n &lt; 64; n++) {
                re += features[n];
            }
        }

        else {
            for (int n = 0; n &lt; 64; n++) {
                const float cos_n = cosLUT[i][n];
                const float sin_n = sinLUT[i][n] ? sqrt(1 - cos_n * cos_n) : -sqrt(1 - cos_n * cos_n);
                re += features[n] * cos_n;
                im -= features[n] * sin_n;
            }
        }

        fft[i] = sqrt(re * re + im * im);
    }
}</code></pre>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<hr />
<p>This post required much work to be produced, so I hope I didn't forgot anything  and you found these information useful.<br />
As always, there's a <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/PrincipalFFTExample/PrincipalFFTExample.ino">Github repo</a> with all the code of this post.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/09/principal-fft-components-as-efficient-features-extrator/">&#8220;Principal&#8221; FFT components as efficient features extrator</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Better word classification with Arduino Nano 33 BLE Sense and Machine Learning</title>
		<link>https://eloquentarduino.github.io/2020/08/better-word-classification-with-arduino-33-ble-sense-and-machine-learning/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Mon, 24 Aug 2020 17:04:57 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[ml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1282</guid>

					<description><![CDATA[<p>Let's revamp the post I wrote about word classification using Machine Learning on Arduino, this time using a proper microphone (the MP34DT05 mounted on the Arduino Nano 33 BLE Sense) instead of a chinese, analog one: will the results improve? Updated on 16 October 2020: step by step explanation of the process with ready-made sketch [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/better-word-classification-with-arduino-33-ble-sense-and-machine-learning/">Better word classification with Arduino Nano 33 BLE Sense and Machine Learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Let's revamp the post I wrote about <a href="/2019/12/word-classification-using-arduino/">word classification using Machine Learning on Arduino</a>, this time using a proper microphone (the MP34DT05 mounted on the Arduino Nano 33 BLE Sense) instead of a chinese, analog one: will the results improve?</p>
<div id="attachment_653" style="width: 760px" class="wp-caption alignnone"><img aria-describedby="caption-attachment-653" src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg" width="750" height="422" class="size-full wp-image-653" srcset="https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg 750w, https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord-300x169.jpg 300w" sizes="(max-width: 750px) 100vw, 750px" /><p id="caption-attachment-653" class="wp-caption-text">from https://www.udemy.com/course/learn-audio-processing-complete-engineers-course/</p></div>
<p><span id="more-1282"></span></p>
<p><strong>Updated on 16 October 2020: step by step explanation of the process with ready-made sketch code</strong></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocwhat-youll-learn">What you'll learn</a><li><a href="#tocwhat-youll-need">What you'll need</a><li><a href="#tocstep-1-capture-audio-samples">Step 1. Capture audio samples</a><ol><li><a href="#toctheory-pulse-density-modulation-a-k-a-pdm">Theory: Pulse-density modulation (a.k.a. PDM)</a><li><a href="#tocpractice-the-code-to-capture-the-samples">Practice: the code to capture the samples</a><li><a href="#tocaction-capture-the-words-examples">Action: capture the words examples</a></li></ol><li><a href="#tocstep-2-train-the-machine-learning-model">Step 2. Train the machine learning model</a><li><a href="#tocstep-3-deploy-to-your-microcontroller">Step 3. Deploy to your microcontroller</a></ol></div></p>
<h2 id="tocwhat-youll-learn">What you'll learn</h2>
<p>This tutorial will teach you how to capture audio from the Arduino Nano 33 BLE Sense microphone and classify it: at the end of this post, you will have a trained model able to detect in real-time the word you tell, among the ones that you trained it to recognize. The classification will occur directly on your Arduino board.</p>
<p><em>This is not a general-purpose speech recognizer able to convert speech-to-text: it works only on the words <strong>you</strong> train it on</em>.</p>
<h2 id="tocwhat-youll-need">What you'll need</h2>
<ul>
<li>
<p><strong>Hardware</strong></p>
<ol>
<li><a href="https://store.arduino.cc/arduino-nano-33-ble-sense">Arduino Nano 33 BLE Sense</a></li>
</ol>
</li>
<li>
<p><strong>Software</strong></p>
<ol>
<li>Python</li>
<li>Python's module <a href="https://scikit-learn.org/stable/">scikit-learn</a></li>
<li>Python's module <a href="https://pypi.org/project/micromlgen/">micromlgen</a></li>
</ol>
</li>
</ul>
<p>To install the software, open your terminal and install the libraries.</p>
<pre><code class="language-bash">pip install -U scikit-learn
pip install -U micromlgen</code></pre>
<h2 id="tocstep-1-capture-audio-samples">Step 1. Capture audio samples</h2>
<p>First of all, we need to capture a bunch of examples of the words we want to recognize.</p>
<p>In the <a href="/2019/12/word-classification-using-arduino/">original post</a>, we used an analog microphone to record the audio. It is for sure the easiest way to interact with audio on a microcontroller since you only need to <code>analogRead()</code> the selected pin to get a value from the sensor.</p>
<p>This semplicity, however, comes at the cost of a nearly inexistent signal pre-processing from the sensor itself: most of the time, you will get junk - I don't want to be rude, but that's it.</p>
<h3 id="toctheory-pulse-density-modulation-a-k-a-pdm">Theory: Pulse-density modulation (a.k.a. PDM)</h3>
<p>The microphone mounted on the Arduino Nano 33 BLE Sense (the <a href="https://content.arduino.cc/assets/Nano_BLE_Sense_mp34dt05-a.pdf">MP34DT05</a>) is fortunately much better than this: it gives you access to a modulated signal much more suitable for our processing needs.</p>
<p>The modulation used is pulse-density: I won't try to explain you how this works since I'm not an expert in DSP and neither it is the main scope of this article (refer to <a href="https://en.wikipedia.org/wiki/Pulse-density_modulation">Wikipedia</a> for some more information).</p>
<p>What matters to us is that we can grab an array of bytes from the microphone and extract its <a href="https://en.wikipedia.org/wiki/Root_mean_square">Root Mean Square</a> (a.k.a. RMS) to be used as a feature for our Machine Learning model.</p>
<p>I had some difficulty finding examples on how to access the microphone on the Arduino Nano 33 BLE Sense board: fortunately, there's a <a href="https://github.com/DaleGia/nano-33-sense-serial-example">Github repo</a> from <em>DelaGia</em> that shows how to access all the sensors of the board.</p>
<p>I extracted the microphone part and incapsulated it in an easy to use class, so you don't really need to dig into the implementation details if you're not interested.</p>
<h3 id="tocpractice-the-code-to-capture-the-samples">Practice: the code to capture the samples</h3>
<p>When loaded on your Arduino Nano 33 BLE Sense, the following sketch will await for you to speak in front of the microphone: once it detects a sound, it will record  64 audio values and print them to the serial monitor.</p>
<p>From my experience, 64 samples are sufficient to cover short words such as <em>yes</em>, <em>no</em>, <em>play</em>, <em>stop</em>: if you plan to classify longer words, you may need to increase this number.</p>
<div class="watchout">I suggest you keep the words short: longer words will probably decrease the accuracy of the model. If you want nonetheless a longer duration, at least keep the number of words as low as possible</div>
<p>Download the <a href="https://eloquentarduino.github.io/wp-content/uploads/2020/08/arduino-33-ble-sense-capture-audio-samples.zip" title="Arduino Nano 33 BLE Sense - Capture audio samples">Arduino Nano 33 BLE Sense - Capture audio samples sketch</a>, open it the Arduino IDE and flash it to your board.</p>
<p>Here's the main code.</p>
<pre><code class="language-cpp">#include &quot;Mic.h&quot;

// tune as per your needs
#define SAMPLES 64
#define GAIN (1.0f/50)
#define SOUND_THRESHOLD 2000

float features[SAMPLES];
Mic mic;

void setup() {
    Serial.begin(115200);
    PDM.onReceive(onAudio);
    mic.begin();
    delay(3000);
}

void loop() {
    // await for a word to be pronounced
    if (recordAudioSample()) {
        // print features to serial monitor
        for (int i = 0; i &lt; SAMPLES; i++) {
            Serial.print(features[i], 6);
            Serial.print(i == SAMPLES - 1 ? &#039;\n&#039; : &#039;,&#039;);
        }

        delay(1000);
    }

    delay(20);
}

/**
 * PDM callback to update mic object
 */
void onAudio() {
    mic.update();
}

/**
 * Read given number of samples from mic
 */
bool recordAudioSample() {
    if (mic.hasData() &amp;&amp; mic.data() &gt; SOUND_THRESHOLD) {

        for (int i = 0; i &lt; SAMPLES; i++) {
            while (!mic.hasData())
                delay(1);

            features[i] = mic.pop() * GAIN;
        }

        return true;
    }

    return false;
}</code></pre>
<p>Now that we have the acquisition logic in place, it's time for you to record some samples of the words you want to classify. </p>
<h3 id="tocaction-capture-the-words-examples">Action: capture the words examples</h3>
<p>Now you have to capture as many samples of the words you want to classify as possible.</p>
<p>Open the serial monitor and pronounce a word near the microphone: a line of numbers will be printed on the monitor.</p>
<p>This is the <em>description</em> of your word.</p>
<p>You need many lines like this for an accurate prediction, so keep repeating the same word 15-30 times.</p>
<div class="my-advice">**My advice**: while recording the samples, vary both the distance of your mounth from the mic and the intensity of your voice: this will produce a more robust classification model later on.</div>
<p>After you repeated the same words many times, copy the content of the serial monitor and save it in a CSV file named after the word, for example <code>yes.csv</code>.</p>
<p>Then clear the serial monitor and repeat the process for each word.</p>
<p>Keep all these files in a folder because we need them to train our classifier.</p>
<h2 id="tocstep-2-train-the-machine-learning-model">Step 2. Train the machine learning model</h2>
<p>Now that we have the samples, it's time to train the classifier.</p>
<p>Create a Python project in your favourite IDE or use your favourite text editor, if you don't have one.</p>
<p>As described in <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/">my post about how to train a classifier</a>, we create a Python script that reads all the files inside a folder and concatenates them in a single array you feed to the classifier model.</p>
<p>Be sure your folder structure is like the following:</p>
<pre><code>ArduinoWordClassification
  |-- train_classifier.py
  |-- data/
  |---- yes.csv
  |---- no.csv
  |---- play.csv
  |---- any other .csv file you recorded</code></pre>
<pre><code class="language-python"># file: train_classifier.py

import numpy as np
from os.path import basename
from glob import glob
from sklearn.svm import SVC
from micromlgen import port
from sklearn.model_selection import train_test_split

def load_features(folder):
    dataset = None
    classmap = {}
    for class_idx, filename in enumerate(glob(&#039;%s/*.csv&#039; % folder)):
        class_name = basename(filename)[:-4]
        classmap[class_idx] = class_name
        samples = np.loadtxt(filename, dtype=float, delimiter=&#039;,&#039;)
        labels = np.ones((len(samples), 1)) * class_idx
        samples = np.hstack((samples, labels))
        dataset = samples if dataset is None else np.vstack((dataset, samples))
    return dataset, classmap

np.random.seed(0)
dataset, classmap = load_features(&#039;data&#039;)
X, y = dataset[:, :-1], dataset[:, -1]
# this line is for testing your accuracy only: once you&#039;re satisfied with the results, set test_size to 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = SVC(kernel=&#039;poly&#039;, degree=2, gamma=0.1, C=100)
clf.fit(X_train, y_train)

print(&#039;Accuracy&#039;, clf.score(X_test, y_test))
print(&#039;Exported classifier to plain C&#039;)
print(port(clf, classmap=classmap))</code></pre>
<p>Among the classifiers I tried, SVM produced the best accuracy at 96% with 32 support vectors: it's not a super-tiny model, but it's quite small nevertheless.</p>
<p>If you're not satisifed with SVM, you can use Decision Tree, Random Forest, Gaussian Naive Bayes, Relevant Vector Machines. See my other posts for a detailed description of each.</p>
<p>In your console, after the accuracy score, you will have the plain C implementation of the classifier you trained. The following reports my SVM model.</p>
<pre><code class="language-cpp">// File: Classifier.h

#pragma once
namespace Eloquent {
    namespace ML {
        namespace Port {
            class SVM {
            public:
                /**
                * Predict class for features vector
                */
                int predict(float *x) {
                    float kernels[35] = { 0 };
                    float decisions[6] = { 0 };
                    int votes[4] = { 0 };
                    kernels[0] = compute_kernel(x,   33.0  , 41.0  , 47.0  , 54.0  , 59.0  , 61.0  , 56.0  , 51.0  , 50.0  , 51.0  , 44.0  , 32.0  , 23.0  , 15.0  , 12.0  , 8.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );
                    kernels[1] = compute_kernel(x,   40.0  , 50.0  , 51.0  , 60.0  , 56.0  , 57.0  , 58.0  , 53.0  , 50.0  , 45.0  , 42.0  , 34.0  , 23.0  , 16.0  , 10.0  , 7.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 14.0  , 3.0  , 8.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 3.0 );
                    kernels[2] = compute_kernel(x,   56.0  , 68.0  , 78.0  , 91.0  , 84.0  , 84.0  , 84.0  , 74.0  , 69.0  , 64.0  , 57.0  , 44.0  , 33.0  , 18.0  , 12.0  , 8.0  , 5.0  , 9.0  , 15.0  , 12.0  , 12.0  , 9.0  , 12.0  , 7.0  , 3.0  , 10.0  , 12.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 6.0  , 3.0  , 6.0  , 10.0  , 10.0  , 8.0  , 3.0  , 9.0  , 9.0  , 9.0  , 8.0  , 9.0  , 9.0  , 11.0  , 3.0  , 8.0  , 9.0  , 8.0  , 8.0  , 8.0  , 6.0  , 7.0  , 3.0  , 3.0  , 8.0  , 5.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0 );

                    // ...many other kernels computations...

                    decisions[0] = 0.722587775297
                                   + kernels[1] * 3.35855e-07
                                   + kernels[2] * 1.64612e-07
                                   + kernels[4] * 6.00056e-07
                                   + kernels[5] * 3.5195e-08
                                   + kernels[7] * -4.2079e-08
                                   + kernels[8] * -4.2843e-08
                                   + kernels[9] * -9.994e-09
                                   + kernels[10] * -5.11065e-07
                                   + kernels[11] * -5.979e-09
                                   + kernels[12] * -4.4672e-08
                                   + kernels[13] * -1.5606e-08
                                   + kernels[14] * -1.2941e-08
                                   + kernels[15] * -2.18903e-07
                                   + kernels[17] * -2.31635e-07
                            ;
                    decisions[1] = -1.658344586719
                                   + kernels[0] * 2.45018e-07
                                   + kernels[1] * 4.30223e-07
                                   + kernels[3] * 1.00277e-07
                                   + kernels[4] * 2.16524e-07
                                   + kernels[18] * -4.81187e-07
                                   + kernels[20] * -5.10856e-07
                            ;
                    decisions[2] = -1.968607562265
                                   + kernels[0] * 3.001833e-06
                                   + kernels[3] * 4.5201e-08
                                   + kernels[4] * 1.54493e-06
                                   + kernels[5] * 2.81834e-07
                                   + kernels[25] * -5.93581e-07
                                   + kernels[26] * -2.89779e-07
                                   + kernels[27] * -1.73958e-06
                                   + kernels[28] * -1.09552e-07
                                   + kernels[30] * -3.09126e-07
                                   + kernels[31] * -1.294219e-06
                                   + kernels[32] * -5.37961e-07
                            ;
                    decisions[3] = -0.720663029823
                                   + kernels[6] * 1.4362e-08
                                   + kernels[7] * 6.177e-09
                                   + kernels[9] * 1.25e-08
                                   + kernels[10] * 2.05478e-07
                                   + kernels[12] * 2.501e-08
                                   + kernels[15] * 4.363e-07
                                   + kernels[16] * 9.147e-09
                                   + kernels[18] * -1.82182e-07
                                   + kernels[20] * -4.93707e-07
                                   + kernels[21] * -3.3084e-08
                            ;
                    decisions[4] = -1.605747746589
                                   + kernels[6] * 6.182e-09
                                   + kernels[7] * 1.3853e-08
                                   + kernels[8] * 2.12e-10
                                   + kernels[9] * 1.1243e-08
                                   + kernels[10] * 7.80681e-07
                                   + kernels[15] * 8.347e-07
                                   + kernels[17] * 1.64985e-07
                                   + kernels[23] * -4.25014e-07
                                   + kernels[25] * -1.134803e-06
                                   + kernels[34] * -2.52038e-07
                            ;
                    decisions[5] = -0.934328303475
                                   + kernels[19] * 3.3529e-07
                                   + kernels[20] * 1.121946e-06
                                   + kernels[21] * 3.44683e-07
                                   + kernels[22] * -6.23056e-07
                                   + kernels[24] * -1.4612e-07
                                   + kernels[28] * -1.24025e-07
                                   + kernels[29] * -4.31701e-07
                                   + kernels[31] * -9.2146e-08
                                   + kernels[33] * -3.8487e-07
                            ;
                    votes[decisions[0] &gt; 0 ? 0 : 1] += 1;
                    votes[decisions[1] &gt; 0 ? 0 : 2] += 1;
                    votes[decisions[2] &gt; 0 ? 0 : 3] += 1;
                    votes[decisions[3] &gt; 0 ? 1 : 2] += 1;
                    votes[decisions[4] &gt; 0 ? 1 : 3] += 1;
                    votes[decisions[5] &gt; 0 ? 2 : 3] += 1;
                    int val = votes[0];
                    int idx = 0;

                    for (int i = 1; i &lt; 4; i++) {
                        if (votes[i] &gt; val) {
                            val = votes[i];
                            idx = i;
                        }
                    }

                    return idx;
                }

                /**
                * Convert class idx to readable name
                */
                const char* predictLabel(float *x) {
                    switch (predict(x)) {
                        case 0:
                            return &quot;no&quot;;
                        case 1:
                            return &quot;stop&quot;;
                        case 2:
                            return &quot;play&quot;;
                        case 3:
                            return &quot;yes&quot;;
                        default:
                            return &quot;Houston we have a problem&quot;;
                    }
                }

            protected:
                /**
                * Compute kernel between feature vector and support vector.
                * Kernel type: poly
                */
                float compute_kernel(float *x, ...) {
                    va_list w;
                    va_start(w, 64);
                    float kernel = 0.0;

                    for (uint16_t i = 0; i &lt; 64; i++) {
                        kernel += x[i] * va_arg(w, double);
                    }

                    return pow((0.1 * kernel) + 0.0, 2);
                }
            };
        }
    }
}</code></pre>
<h2 id="tocstep-3-deploy-to-your-microcontroller">Step 3. Deploy to your microcontroller</h2>
<p>Now we have all the pieces we need to perform word classification on our Arduino board.</p>
<p>Download <a href="https://eloquentarduino.github.io/wp-content/uploads/2020/08/arduino-33-ble-sense-classify-audio.zip">the Arduino Nano 33 BLE Sense - Audio classification sketch</a>, open it in the Arduino IDE and paste the plain C code you got in the console inside the <code>Classifier.h</code> file (delete all its contents before!).</p>
<p>Fine: it's time to deploy!</p>
<p>Hit the upload button: if everything went fine, open the serial monitor and pronounce one of the words you recorded during <code>Step 1</code>.</p>
<p>Hopefully, you will read the word on the serial monitor.</p>
<p>Here's a quick demo (please forgive me for the bad video quality).</p>
<div style="width: 576px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-1282-1" width="576" height="482" preload="metadata" controls="controls"><source type="video/mp4" src="https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4?_=1" /><a href="https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4">https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4</a></video></div>
<hr />
<p>If you liked this tutorial and it helped you successfully implement word classification on your Arduino Nano 33 BLE Sense, please share it on your social media so others can benefit too.</p>
<p>If you have troubles or questions, don't hesitate to leave a comment: I will be happy to help you.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/better-word-classification-with-arduino-33-ble-sense-and-machine-learning/">Better word classification with Arduino Nano 33 BLE Sense and Machine Learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4" length="5594095" type="video/mp4" />

			</item>
		<item>
		<title>The Ultimate Guide to Wifi Indoor Positioning using Arduino and Machine Learning</title>
		<link>https://eloquentarduino.github.io/2020/08/the-ultimate-guide-to-wifi-indoor-positioning-using-arduino-and-machine-learning/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sat, 08 Aug 2020 13:21:25 +0000</pubDate>
				<category><![CDATA[Senza categoria]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1237</guid>

					<description><![CDATA[<p>This will be the most detailed, easy to follow tutorial over the Web on how to implement Wifi indoor positioning using an Arduino microcontroller and Machine Learning. It contains all the steps, tools and code from the start to the end of the project. ri-elaborated from https://www.accuware.com/blog/ambient-signals-plus-video-images/ My original post abot Wifi indoor positioning is [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/the-ultimate-guide-to-wifi-indoor-positioning-using-arduino-and-machine-learning/">The Ultimate Guide to Wifi Indoor Positioning using Arduino and Machine Learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>This will be the most detailed, easy to follow tutorial over the Web on how to implement Wifi indoor positioning using an Arduino microcontroller and Machine Learning. It contains all the steps, tools and code from the start to the end of the project.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/illustrations_ambient-wifi-site-survey2.jpg" alt="" /><br />
<em>ri-elaborated from <a href="https://www.accuware.com/blog/ambient-signals-plus-video-images/">https://www.accuware.com/blog/ambient-signals-plus-video-images/</a></em></p>
<p><span id="more-1237"></span></p>
<p><a href="/2019/12/wifi-indoor-positioning-on-arduino/">My original post</a> abot Wifi indoor positioning is one of my top-performing post of all time (after <a href="/2020/01/motion-detection-with-esp32-cam-only-arduino-version/">motion detection using ESP32 camera</a> and <a href="/2019/11/you-can-run-machine-learning-on-arduino/">the introductory post on Machine Learning for Arduino</a>). This is why I settled to revamp it and add some more details, tools and scripts to create <strong>the most complete free guide</strong> on how to implement such a system, from the beginning to the end.</p>
<p>This post will cover all the necessary steps and provide all the code you need: for an introduction to the topic, I point you to <a href="/2019/12/wifi-indoor-positioning-on-arduino/">the original post</a>.</p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocdata-gathering">Data gathering</a><li><a href="#tocgenerating-the-features-converter">Generating the features converter</a><li><a href="#tocgenerating-the-classifier">Generating the classifier</a><li><a href="#tocwrapping-it-all-together">Wrapping it all together</a><ol><li><a href="#tocdisclaimer">Disclaimer</a></ol></div></p>
<h2 id="tocfeatures-definition">Features definition</h2>
<p>This part stays the same as the original post: we will use the RSSIs (signal strength) of the nearby Wifi hotspots to classifiy which location we're in.</p>
<p>Each location will &quot;see&quot; a certain number of networks, each with a RSSI that will stay <em>mostly</em> the same: the unique combination of these RSSIs will become a fingerprint to distinguish the locations from one another.</p>
<p>Since not all networks will be visible all the time, the shape of our data will be more likely a sparse matrix.<br />
A <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse matrix</a> is a matrix where most of the elements will be zero, meaning the absence of the given feature. Only the relevant elements will be non-zero and will represent the RSSI of the nth network.</p>
<p>The following example table should give you an idea of what our data will look like.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Location</th>
<th style="text-align: right;">Net #1</th>
<th style="text-align: right;">Net #2</th>
<th style="text-align: right;">Net #3</th>
<th style="text-align: right;">Net #4</th>
<th style="text-align: right;">Net #5</th>
<th style="text-align: right;">Net #6</th>
<th style="text-align: right;">Net #7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Kitchen/1</td>
<td style="text-align: right;"><strong>50</strong></td>
<td style="text-align: right;"><strong>30</strong></td>
<td style="text-align: right;"><strong>60</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Kitchen/2</td>
<td style="text-align: right;"><strong>55</strong></td>
<td style="text-align: right;"><strong>30</strong></td>
<td style="text-align: right;"><strong>55</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>5</strong></td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Kitchen/3</td>
<td style="text-align: right;"><strong>50</strong></td>
<td style="text-align: right;"><strong>35</strong></td>
<td style="text-align: right;"><strong>65</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>5</strong></td>
</tr>
<tr>
<td style="text-align: left;">Bedroom/1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>80</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>80</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>40</strong></td>
<td style="text-align: right;"><strong>40</strong></td>
</tr>
<tr>
<td style="text-align: left;">Bedroom/2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>80</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>85</strong></td>
<td style="text-align: right;"><strong>10</strong></td>
<td style="text-align: right;"><strong>20</strong></td>
<td style="text-align: right;"><strong>20</strong></td>
</tr>
<tr>
<td style="text-align: left;">Bedroom/3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>70</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>85</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>30</strong></td>
<td style="text-align: right;"><strong>40</strong></td>
</tr>
<tr>
<td style="text-align: left;">Bathroom/1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>30</strong></td>
<td style="text-align: right;"><strong>80</strong></td>
<td style="text-align: right;"><strong>80</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Bathroom/2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>10</strong></td>
<td style="text-align: right;"><strong>90</strong></td>
<td style="text-align: right;"><strong>85</strong></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Bathroom/3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><strong>30</strong></td>
<td style="text-align: right;"><strong>90</strong></td>
<td style="text-align: right;"><strong>90</strong></td>
<td style="text-align: right;"><strong>5</strong></td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Even though the numbers in this table are fake, you should recognize a pattern:</p>
<ul>
<li>each location is characterized by a certain combination of always-visible networks</li>
<li>some sample could be &quot;noised&quot; by weak networks (the <strong>5</strong> in the table)</li>
</ul>
<p>Our machine learning algorithm should be able to extract each location's fingerprint without being fooled by this inconsistent features.</p>
<h2 id="tocdata-gathering">Data gathering</h2>
<p>Now that we know what our data should look like, we need to first get it.</p>
<p>In the original post, this point was the one I'm unhappy with since it's not as straight-forward as I would have liked. The method I present you in this post, instead, is by far way simpler to follow.</p>
<p>First of all, you will need a Wifi equipped board. I will use an <a href="https://store.arduino.cc/arduino-mkr-wifi-1010">Arduino MKR WiFi 1010</a>, but any ESP8266 / ESP32 or the like will work.</p>
<p>The following sketch will do the job: it scans the visible networks at a regular interval and prints their RSSIs encoded in JSON format.</p>
<pre><code class="language-cpp">// file DataGathering.h

#include &quot;WiFi.h&quot;

#define print(string) Serial.print(string);
#define quote(string) print(&#039;&quot;&#039;); print(string); print(&#039;&quot;&#039;);

String location = &quot;&quot;;

/**
 * 
 */
void setup() {
  Serial.begin(115200);
  delay(3000);
  WiFi.disconnect();
}

/**
 * 
 */
void loop() {  
  // if location is set, scan networks
  if (location != &quot;&quot;) {
    int numNetworks = WiFi.scanNetworks();

    // print location
    print(&#039;{&#039;);
    quote(&quot;__location&quot;);
    print(&quot;: &quot;);
    quote(location);
    print(&quot;, &quot;);

    // print each network SSID and RSSI
    for (int i = 0; i &lt; numNetworks; i++) {
      quote(WiFi.SSID(i));
      print(&quot;: &quot;);
      print(WiFi.RSSI(i));
      print(i == numNetworks - 1 ? &quot;}\n&quot; : &quot;, &quot;);
    }

    delay(1000);
  }
  // else wait for user to enter the location
  else {
    String input;

    Serial.println(&quot;Enter &#039;scan {location}&#039; to start the scanning&quot;);

    while (!Serial.available())
      delay(200);

    input = Serial.readStringUntil(&#039;\n&#039;);

    if (input.indexOf(&quot;scan &quot;) == 0) {
      input.replace(&quot;scan &quot;, &quot;&quot;);
      location = input;
    }
    else {
      location = &quot;&quot;;
    }
  }
}</code></pre>
<p>Upload the sketch to your board and start mapping your house / office: go to the target location and type <code>scan {location}</code> in the serial monitor, where <code>{location}</code>is the name you want to give to the current location (so, for example, if you're mapping the kitchen, type <code>scan kitchen</code>).</p>
<p>Move around the room a bit so you capture a few variations of the visible hotspots: this will lead to a more robust classification later on.</p>
<p>To stop the recording just type <code>stop</code> in the serial monitor.</p>
<p>Now repeat this process for each location you want to classify. At this point you should have ended with something similar to the following:</p>
<pre><code class="language-python">{&quot;__location&quot;: &quot;Kitchen&quot;, &quot;N1&quot;: 100, &quot;N2&quot;: 50}
{&quot;__location&quot;: &quot;Bedroom&quot;, &quot;N3&quot;: 100, &quot;N2&quot;: 50}
{&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N1&quot;: 100, &quot;N4&quot;: 50}
{&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N5&quot;: 100, &quot;N4&quot;: 50}</code></pre>
<p>In your case, &quot;N1&quot;, &quot;N2&quot;... will contain the name of the visible networks.</p>
<p>When you're happy with your training data, it's time to convert it to something useful.</p>
<h2 id="tocgenerating-the-features-converter">Generating the features converter</h2>
<p>Given the data we have, we want to generate C code that can convert a Wifi scan result into a feature vector we can use for classification.</p>
<p>Since I'm a fan of code-generators, I wrote one specifically for this very project. And since I already have a code-generator library I use for Machine Learning code written in Python, I updated it with this new functionality.</p>
<div class="watchout">You must have Python installed on your system</div>
<p>Start by installing the library.</p>
<pre><code class="language-bash"># be sure it installs version &gt;= 1.1.8
pip install --upgrade micromlgen</code></pre>
<p>Now create a script with the following code:</p>
<pre><code class="language-python">from micromlgen import port_wifi_indoor_positioning

if __name__ == &#039;__main__&#039;:
    samples = &#039;&#039;&#039;
    {&quot;__location&quot;: &quot;Kitchen&quot;, &quot;N1&quot;: 100, &quot;N2&quot;: 50}
    {&quot;__location&quot;: &quot;Bedroom&quot;, &quot;N3&quot;: 100, &quot;N2&quot;: 50}
    {&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N1&quot;: 100, &quot;N4&quot;: 50}
    {&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N5&quot;: 100, &quot;N4&quot;: 50}
    &#039;&#039;&#039;
    X, y, classmap, converter_code = port_wifi_indoor_positioning(samples)
    print(converter_code)</code></pre>
<p>Of course you have to replace the <code>samples</code> content with the output you got in the previous step. </p>
<p>In the console you should see a C++ class we will use later in the Arduino sketch. The class should be similar to the following example code.</p>
<pre><code class="language-cpp">// Save this code in your sketch as Converter.h

#pragma once
namespace Eloquent {
    namespace Projects {
        class WifiIndoorPositioning {
            public:
                /**
                * Get feature vector
                */
                float* getFeatures() {
                    static float features[5] = {0};
                    uint8_t numNetworks = WiFi.scanNetworks();

                    for (uint8_t i = 0; i &lt; 5; i++) {
                        features[i] = 0;
                    }

                    for (uint8_t i = 0; i &lt; numNetworks; i++) {
                        int featureIdx = ssidToFeatureIdx(WiFi.SSID(i));

                        if (featureIdx &gt;= 0) {
                            features[featureIdx] = WiFi.RSSI(i);
                        }
                    }

                    return features;
                }

            protected:
                /**
                * Convert SSID to featureIdx
                */
                int ssidToFeatureIdx(String ssid) {
                    if (ssid.equals(&quot;N1&quot;))
                    return 0;

                    if (ssid.equals(&quot;N2&quot;))
                    return 1;

                    if (ssid.equals(&quot;N3&quot;))
                    return 2;

                    if (ssid.equals(&quot;N4&quot;))
                    return 3;

                    if (ssid.equals(&quot;N5&quot;))
                    return 4;

                    return -1;
                }
            };
        }
    }</code></pre>
<p>I will briefly explain what it does: when you call <code>getFeatures()</code>, it runs a Wifi scan and for each network it finds, it fills the corresponding element in the feature vector (if the network is a known one).</p>
<p>At the end of the procedure, your feature vector will look something like <code>[0, 10, 0, 0, 50, 0, 0]</code>, each element representing the RSSI of a given network.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2 id="tocgenerating-the-classifier">Generating the classifier</h2>
<p>To close the loop of the project, we need to be able to classify the features vector into one of the recorded location. Since we already have <code>micromlgen</code> installed, it will be very easy to do so.</p>
<p>Let's update the Python code we already have: this time, instead of printing the converter code, we will print the classifier code.</p>
<pre><code class="language-bash"># install ml package first
pip install scikit-learn</code></pre>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from micromlgen import port_wifi_indoor_positioning, port

if __name__ == &#039;__main__&#039;:
    samples = &#039;&#039;&#039;
    {&quot;__location&quot;: &quot;Kitchen&quot;, &quot;N1&quot;: 100, &quot;N2&quot;: 50}
    {&quot;__location&quot;: &quot;Bedroom&quot;, &quot;N3&quot;: 100, &quot;N2&quot;: 50}
    {&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N1&quot;: 100, &quot;N4&quot;: 50}
    {&quot;__location&quot;: &quot;Bathroom&quot;, &quot;N5&quot;: 100, &quot;N4&quot;: 50}
    &#039;&#039;&#039;
    X, y, classmap, converter_code = port_wifi_indoor_positioning(samples)
    clf = DecisionTreeClassifier()
    clf.fit(X, y)
    print(port(clf, classmap=classmap))</code></pre>
<p>Here I chose <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">Decision tree</a> because it is a very lightweight algorithm and should work fine for the kind of features we're working with.<br />
If you're not satisfied with the results, you can try to use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC">SVM</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB">Gaussian Naive Bayes</a>, which are both supported by <code>micromlgen</code>.</p>
<p>In the console you will see the generated code for the classifier you trained. In the case of <code>DecisionTree</code> the code will look like the following.</p>
<pre><code class="language-cpp">// Save this code in your sketch as Classifier.h

#pragma once
namespace Eloquent {
    namespace ML {
        namespace Port {
            class DecisionTree {
                public:
                    /**
                    * Predict class for features vector
                    */
                    int predict(float *x) {
                        if (x[2] &lt;= 25.0) {
                            if (x[4] &lt;= 50.0) {
                                return 1;
                            }

                            else {
                                return 2;
                            }
                        }

                        else {
                            return 0;
                        }
                    }

                    /**
                    * Convert class idx to readable name
                    */
                    const char* predictLabel(float *x) {
                        switch (predict(x)) {
                            case 0:
                            return &quot;Bathroom&quot;;
                            case 1:
                            return &quot;Bedroom&quot;;
                            case 2:
                            return &quot;Kitchen&quot;;
                            default:
                            return &quot;Houston we have a problem&quot;;
                        }
                    }

                protected:
                };
            }
        }
    }</code></pre>
<h2 id="tocwrapping-it-all-together">Wrapping it all together</h2>
<p>Now that we have all the pieces together, we only need to merge them to get a complete working example.</p>
<pre><code class="language-cpp">// file WifiIndoorPositioning.h

#include &quot;WiFi.h&quot;
#include &quot;Converter.h&quot;
#include &quot;Classifier.h&quot;

Eloquent::Projects::WifiIndoorPositioning positioning;
Eloquent::ML::Port::DecisionTree classifier;

void setup() {
  Serial.begin(115200);
}

void loop() {
  Serial.print(&quot;You&#039;re in &quot;);
  Serial.println(classifier.predictLabel(positioning.getFeatures()));
  delay(3000);
}</code></pre>
<p>To the bare minimum, the above code runs the scan and tells you which location you're in. That's it.</p>
<h3 id="tocdisclaimer">Disclaimer</h3>
<p>This system should be pretty accurate and robust if you properly gather the data, though I can quantify how much accurate.</p>
<p>This is not an <em>indoor navigation system</em>: it can't tell you &quot;the coordinates&quot; of where you are, it can only detect in which room you're in.</p>
<p>If your location lack of nearby Wifi hotspots, an easy and cheap solution would be to spawn a bunch of ESP8266 / ESP32 boards around your house each acting as Access Point: with this simple trick you should be able to be as accurate as needed by just adding more boards.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<hr />
<p>With this in-depth tutorial I hope I helped you going from start to end of setting up a Wifi indoor positioning system using cheap hardware as ESP8266 / ESP32 boards and the Arduino IDE. </p>
<p>As you can see, Machine learning has not to be intimidating even for beginners: you just need the right tools to get the job done.</p>
<p>If this guide excited you about Machine learning on microcontrollers, I invite you to read the many other posts I wrote on the topic and share them on the socials.</p>
<p>You can find the whole project on <a href="https://github.com/eloquentarduino/EloquentMicroML/tree/master/examples/TheUltimateGuideToWifiIndoorPositioning">Github</a>. Don't forget to star the repo if you like it.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/the-ultimate-guide-to-wifi-indoor-positioning-using-arduino-and-machine-learning/">The Ultimate Guide to Wifi Indoor Positioning using Arduino and Machine Learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino</title>
		<link>https://eloquentarduino.github.io/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 02 Aug 2020 08:44:36 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[ml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1225</guid>

					<description><![CDATA[<p>Are you looking for a top-performer classifiers with a minimal amount of parameters to tune? Look no further: Gaussian Naive Bayes is what you're looking for. And thanks to EloquentML you can now port it to your microcontroller. (Gaussian) Naive Bayes Naive Bayes classifiers are simple models based on the probability theory that can be [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/">EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Are you looking for a top-performer classifiers with a minimal amount of parameters to tune? Look no further: Gaussian Naive Bayes is what you're looking for. And thanks to EloquentML you can now port it to your microcontroller.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/08/GaussianNB.png" alt="GaussianNB" /></p>
<p><span id="more-1225"></span></p>
<h2>(Gaussian) Naive Bayes</h2>
<p><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> classifiers are simple models based on the probability theory that can be used for classification.</p>
<p>They originate from the assumption of independence among the input variables. Even though this assumption doesn't hold true in the vast majority of the cases, they often perform very good at many classification tasks, so they're quite popular.</p>
<p>Gaussian Naive Bayes stack another (mostly wrong) assumption: that the variables exhibit a Gaussian probability distribution.</p>
<p>I (and many others like me) will never understand how it is possible that so many wrong assumptions lead to such good performances!</p>
<p>Nevertheless, what is important to us is that <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">sklearn implements GaussianNB</a>, so we easily train such a classifier.<br />
The most interesting part is that <code>GaussianNB</code> can be tuned with just a single parameter: <code>var_smoothing</code>.</p>
<p>Don't ask me what it does in theory: in practice you change it and your accuracy can boost. This leads to an easy tuning process that doesn't involves expensive <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">grid search</a>.</p>
<pre><code class="language-python">import sklearn.datasets as d
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from sklearn.naive_bayes import GaussianNB

def pick_best(X_train, X_test, y_train, y_test):
    best = (None, 0)
    for var_smoothing in range(-7, 1):
        clf = GaussianNB(var_smoothing=pow(10, var_smoothing))
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = (y_pred == y_test).sum()
        if accuracy &gt; best[1]:
            best = (clf, accuracy)
    print(&#039;best accuracy&#039;, best[1] / len(y_test))
    return best[0]

iris = d.load_iris()
X = normalize(iris.data)
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
clf = pick_best(X_train, X_test, y_train, y_test)</code></pre>
<p>This simple procedure will train a bunch of classifiers with a different <code>var_smoothing</code> factor and pick the best performing one.</p>
<h2>EloquentML integration</h2>
<p>Once you have your trained classifier, porting it to C is as easy as always:</p>
<pre><code class="language-python">from micromlgen import port

clf = pick_best()
print(port(clf))</code></pre>
<p class="watchout">Always remember to run </p>
<pre><code>pip install --upgrade micromlgen</code></pre>
</p>
<p><code>port</code> is a magic method able to port many classifiers: it will automatically detect the proper converter for you.</p>
<p>What does the exported code looks like?</p>
<pre><code class="language-cpp">#pragma once
namespace Eloquent {
    namespace ML {
        namespace Port {
            class GaussianNB {
                public:
                    /**
                    * Predict class for features vector
                    */
                    int predict(float *x) {
                        float votes[3] = { 0.0f };
                        float theta[4] = { 0 };
                        float sigma[4] = { 0 };
                        theta[0] = 0.801139789889; theta[1] = 0.54726920354; theta[2] = 0.234408773313; theta[3] = 0.039178084094;
                        sigma[0] = 0.000366881742; sigma[1] = 0.000907992556; sigma[2] = 0.000740960787; sigma[3] = 0.000274925514;
                        votes[0] = 0.333333333333 - gauss(x, theta, sigma);
                        theta[0] = 0.748563871324; theta[1] = 0.349390892644; theta[2] = 0.536186138345; theta[3] = 0.166747384117;
                        sigma[0] = 0.000529727082; sigma[1] = 0.000847956504; sigma[2] = 0.000690057342; sigma[3] = 0.000311828658;
                        votes[1] = 0.333333333333 - gauss(x, theta, sigma);
                        theta[0] = 0.704497203305; theta[1] = 0.318862439835; theta[2] = 0.593755956917; theta[3] = 0.217288784452;
                        sigma[0] = 0.000363782089; sigma[1] = 0.000813846722; sigma[2] = 0.000415475678; sigma[3] = 0.000758478249;
                        votes[2] = 0.333333333333 - gauss(x, theta, sigma);
                        // return argmax of votes
                        uint8_t classIdx = 0;
                        float maxVotes = votes[0];

                        for (uint8_t i = 1; i &lt; 3; i++) {
                            if (votes[i] &gt; maxVotes) {
                                classIdx = i;
                                maxVotes = votes[i];
                            }
                        }

                        return classIdx;
                    }

                protected:
                    /**
                    * Compute gaussian value
                    */
                    float gauss(float *x, float *theta, float *sigma) {
                        float gauss = 0.0f;

                        for (uint16_t i = 0; i &lt; 4; i++) {
                            gauss += log(sigma[i]);
                            gauss += pow(x[i] - theta[i], 2) / sigma[i];
                        }

                        return gauss;
                    }
                };
            }
        }
    }</code></pre>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<p>As you can see, we need a couple of &quot;weight vectors&quot;:</p>
<ul>
<li><code>theta</code> is the mean of each feature</li>
<li><code>sigma</code> is the standard deviation</li>
</ul>
<p>The computation is quite thin: just a couple of operations; the class with the highest score is then selected.</p>
<h2>Benchmarks</h2>
<p>Following there's a recap of a couple benchmarks I run on an Arduino Nano 33 Ble Sense.</p>
<table>
<thead>
<tr>
<th>Classifier</th>
<th>Dataset</th>
<th style="text-align: center;">Flash</th>
<th style="text-align: center;">RAM</th>
<th style="text-align: center;">Execution time</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>GaussianNB</td>
<td>Iris (150x4)</td>
<td style="text-align: center;">82 kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">65 ms</td>
<td style="text-align: center;">97%</td>
</tr>
<tr>
<td>LinearSVC</td>
<td>Iris (150x4)</td>
<td style="text-align: center;">83 Kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">76 ms</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td>GaussianNB</td>
<td>Breast cancer (80x40)</td>
<td style="text-align: center;">90 Kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">160 ms</td>
<td style="text-align: center;">77%</td>
</tr>
<tr>
<td>LinearSVC</td>
<td>Breast cancer (80x40)</td>
<td style="text-align: center;">112 Kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">378 ms</td>
<td style="text-align: center;">73%</td>
</tr>
<tr>
<td>GaussianNB</td>
<td>Wine (100x13)</td>
<td style="text-align: center;">85 Kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">130 ms</td>
<td style="text-align: center;">97%</td>
</tr>
<tr>
<td>LinearSVC</td>
<td>Wine (100x13)</td>
<td style="text-align: center;">89 Kb</td>
<td style="text-align: center;">42 Kb</td>
<td style="text-align: center;">125 ms</td>
<td style="text-align: center;">99%</td>
</tr>
</tbody>
</table>
<p>We can see that the accuracy is on par with a linear SVM, reaching up to 97% on some datasets. Its semplicity shines with high-dimensional datasets (breast cancer) where execution time is half of the LinearSVC: I can see this pattern repeating with other real-world, medium-sized datasets.</p>
<hr />
<p>This is it, you can find the example project on <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/GaussianNBClassificationExample/GaussianNBClassificationExample.ino">Github</a>.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/">EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices</title>
		<link>https://eloquentarduino.github.io/2020/07/sefr-a-fast-linear-time-classifier-for-ultra-low-power-devices/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Fri, 10 Jul 2020 15:09:58 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1214</guid>

					<description><![CDATA[<p>A brand new binary classifier that's tiny and accurate, perfect for embedded scenarios: easily achieve 90+ % accuracy with a minimal memory footprint! A few weeks ago I was wandering over arxiv.org looking for insipiration relative to Machine learning on microcontrollers when I found exactly what I was looking for. SEFR: A Fast Linear-Time Classifier [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/07/sefr-a-fast-linear-time-classifier-for-ultra-low-power-devices/">SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>A brand new binary classifier that's tiny and accurate, perfect for embedded scenarios: easily achieve 90+ % accuracy with a minimal memory footprint!</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/07/Binary-classification.png" alt="Binary classification - from https://towardsdatascience.com" /></p>
<p><span id="more-1214"></span></p>
<p>A few weeks ago I was wandering over <a href="https://arxiv.org/search/cs?query=microcontroller&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50">arxiv.org</a> looking for insipiration relative to Machine learning on microcontrollers when I found exactly what I was looking for.</p>
<p><a href="https://arxiv.org/abs/2006.04620">SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices</a> is a paper from Hamidreza Keshavarz, Mohammad Saniee Abadeh, Reza Rawassizadeh where the authors develop a binary classifier that is:</p>
<ul>
<li>fast during training</li>
<li>fast during prediction</li>
<li>requires minimal memory</li>
</ul>
<p>It has been specifically designed for embedded machine learning, so no optimization is required to run in on microcontrollers: it is tiny by design. In short, it uses a combination of the averages of the features as weights plus a bias to distinguish between positive and negative class. If you read the paper you will sure understand it: it's very straightforward.</p>
<h2>How to use</h2>
<p>The authors both provided a <a href="https://github.com/sefr-classifier/sefr">C and Python implementation</a> on Github you can read.  I ported the C version &quot;manually&quot; to my <a href="https://github.com/eloquentarduino/EloquentMicroML">Eloquent ML library</a> and created a <a href="https://github.com/eloquentarduino/sefr">Python package called sefr</a> copy-pasting from the original repo.</p>
<p>Here's a Python example.</p>
<pre><code class="language-python">from sefr import SEFR
from sklearn.datasets import load_iris
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split

if __name__ == &#039;__main__&#039;:
    iris = load_iris()
    X = normalize(iris.data)
    y = iris.target
    X = X[y &lt; 2]
    y = y[y &lt; 2]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    clf = SEFR()
    clf.fit(X_train, y_train)
    print(&#039;accuracy&#039;, (clf.predict(X_test) == y_test).sum() / len(y_test))</code></pre>
<p>How good is it?</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th style="text-align: center;">No. of features</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Iris</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">100%</td>
</tr>
<tr>
<td>Breast cancer</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">89%</td>
</tr>
<tr>
<td>Wine</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">84%</td>
</tr>
<tr>
<td>Digits</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">99%</td>
</tr>
</tbody>
</table>
<p>Considering that the model only needs 1 weight per feature, I think this results are impressive!</p>
<h2>Micromlgen integration</h2>
<p>The Python porting was done so I could integrate it easily in my <a href="https://github.com/eloquentarduino/micromlgen">micromlgen</a> package.</p>
<p>How to use it?</p>
<pre><code class="language-python">from sefr import SEFR
from sklearn.datasets import load_iris
from micromlgen import port

if __name__ == &#039;__main__&#039;:
    iris = load_iris()
    X = iris.data
    y = iris.target
    X = X[y &lt; 2]
    y = y[y &lt; 2]
    clf = SEFR()
    clf.fit(X_train, y_train)
    print(port(clf))</code></pre>
<p>The produced code is so compact I will report it here.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<pre><code class="language-cpp">#pragma once
namespace Eloquent {
    namespace ML {
        namespace Port {
            class SEFR {
                public:
                    /**
                    * Predict class for features vector
                    */
                    int predict(float *x) {
                        return dot(x,   0.084993602632  , -0.106163278477  , 0.488989863684  , 0.687022900763 ) &lt;= 2.075 ? 0 : 1;
                    }

                protected:
                    /**
                    * Compute dot product between features vector and classifier weights
                    */
                    float dot(float *x, ...) {
                        va_list w;
                        va_start(w, 4);
                        float kernel = 0.0;

                        for (uint16_t i = 0; i &lt; 4; i++) {
                            kernel += x[i] * va_arg(w, double);
                        }

                        return kernel;
                    }
                };
            }
        }
    }</code></pre>
<p>In your sketch:</p>
<pre><code class="language-cpp">#include &quot;IrisSEFR.h&quot;
#include &quot;IrisTest.h&quot;

void setup() {
    Serial.begin(115200);
}

void loop() {
    Eloquent::ML::Port::SEFR clf;
    Eloquent::ML::Test::IrisTestSet testSet;

    testSet.test(clf);
    Serial.println(testSet.dump());
    delay(5000);
}</code></pre>
<p>You have to clone the <a href="https://github.com/eloquentarduino/EloquentMicroML/tree/master/examples/OffboardSEFRExample">Github example</a> to compile the code.</p>
<hr />
<p>That's all for today, I hope you will try this classifier and find a project it fits in: I'm very impressed by the easiness of implementation yet the accuracy it can achieve on benchmark datasets.</p>
<p>In the next weeks I'm thinking in implementing a multi-class version of this and see how it performs, so stay tuned!</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/07/sefr-a-fast-linear-time-classifier-for-ultra-low-power-devices/">SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Easy ESP32 camera HTTP video streaming server</title>
		<link>https://eloquentarduino.github.io/2020/06/easy-esp32-camera-http-video-streaming-server/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Wed, 24 Jun 2020 17:27:33 +0000</pubDate>
				<category><![CDATA[Eloquent library]]></category>
		<category><![CDATA[camera]]></category>
		<category><![CDATA[esp32]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1203</guid>

					<description><![CDATA[<p>This will be a short post where I introduce a new addition to the Arduino Eloquent library aimed to make video streaming from an ESP32 camera over HTTP super easy. It will be the first component of a larger project I'm going to implement. If you Google &#34;esp32 video streaming&#34; you will get a bunch [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/06/easy-esp32-camera-http-video-streaming-server/">Easy ESP32 camera HTTP video streaming server</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>This will be a short post where I introduce a new addition to the Arduino Eloquent library aimed to make video streaming from an ESP32 camera over HTTP super easy. It will be the first component of a larger project I'm going to implement.</p>
<p><span id="more-1203"></span></p>
<p>If you Google &quot;esp32 video streaming&quot; you will get a bunch of results that are essentialy copy-pasted from the official Espressif repo: many of them neither copy-pasted the code, just tell you to load the example sketch.</p>
<p>And if you try to read it and try to modify just a bit for your own use-case, you won't understand much.</p>
<p>This is the exact environment for an Eloquent component to live: make it painfully easy what's messy.</p>
<p>I still have to find a good naming scheme for my libraries since Arduino IDE doesn't allow nested imports, so forgive me if &quot;ESP32CameraHTTPVideoStreamingServer.h&quot; was the best that came to mind.</p>
<p>How easy is it to use?</p>
<p>1 line of code if used in conjuction with my other library <a href="https://github.com/eloquentarduino/EloquentVision">EloquentVision</a>.</p>
<pre><code class="language-cpp">#define CAMERA_MODEL_M5STACK_WIDE
#include &quot;WiFi.h&quot;
#include &quot;EloquentVision.h&quot;
#include &quot;ESP32CameraHTTPVideoStreamingServer.h&quot;

using namespace Eloquent::Vision;
using namespace Eloquent::Vision::Camera;

ESP32Camera camera;
HTTPVideoStreamingServer server(81);

/**
 *
 */
void setup() {
    Serial.begin(115200);
    WiFi.softAP(&quot;ESP32&quot;, &quot;12345678&quot;);

    camera.begin(FRAMESIZE_QVGA, PIXFORMAT_JPEG);
    server.start();

    Serial.print(&quot;Camera Ready! Use &#039;http://&quot;);
    Serial.print(WiFi.softAPIP());
    Serial.println(&quot;:81&#039; to stream&quot;);
}

void loop() {
}</code></pre>
<p><code>HTTPVideoStreamingServer</code> assumes you already initialized your camera. You can achieve this task in the way you prefer: <code>ESP32Camera</code> class makes this a breeze.</p>
<p><code>81</code> in the server constructor is the port you want the server to be listening to.</p>
<p>Once connected to WiFi or started in AP mode, all you have to do is call <code>start()</code>: that's it!</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<p>What else is it good for?</p>
<p>The main reason I wrote this piece of library is because one of you reader commented on the <a href="/2020/05/easier-faster-pure-video-esp32-cam-motion-detection/">motion detection post</a> asking if it would be possible to start the video streaming once motion is detected.</p>
<p>Of course it is.</p>
<p>It's just a matter of composing the Eloquent pieces.</p>
<pre><code class="language-cpp">#define CAMERA_MODEL_M5STACK_WIDE
#include &quot;WiFi.h&quot;
#include &quot;EloquentVision.h&quot;
#include &quot;ESP32CameraHTTPVideoStreamingServer.h&quot;

#define SOURCE_WIDTH 320
#define SOURCE_HEIGHT 240
#define BLOCK_SIZE 10
#define BLOCK_DIFF_THRESHOLD 0.2
#define IMAGE_DIFF_THRESHOLD 0.1

using namespace Eloquent::Vision;
using namespace Eloquent::Vision::Camera;
using namespace Eloquent::Vision::ImageProcessing;
using namespace Eloquent::Vision::ImageProcessing::Downscale;

ESP32Camera camera;
HTTPVideoStreamingServer server(81);
MotionDetection&lt; SOURCE_WIDTH, SOURCE_HEIGHT, BLOCK_SIZE&gt; motion(nearest);

/**
 *
 */
void setup() {
    Serial.begin(115200);
    WiFi.softAP(&quot;ESP32&quot;, &quot;12345678&quot;);

    camera.begin(FRAMESIZE_QVGA, PIXFORMAT_JPEG);

    Serial.print(&quot;Camera Ready! Use &#039;http://&quot;);
    Serial.print(WiFi.softAPIP());
    Serial.println(&quot;:81&#039; to stream&quot;);
}

void loop() {
    motion.update(camera.capture()-&gt;buf);

    if (motion.detectRatio() &gt; IMAGE_DIFF_THRESHOLD) {
        Serial.print(&quot;Motion detected&quot;);
        // start the streaming server when motion is detected
        // shutdown after 20 seconds if no one connects
        server.start();
        delay(20000);
        server.stop();
    }

    // probably we don&#039;t need 30 fps, save some power
    delay(300);
}</code></pre>
<p>Does it look good?</p>
<p>Now the rationale behind Eloquent components should be starting to be clear to you: easy to use objects you can compose the way it fits to achieve the result you want.</p>
<p>Would you suggest me more piece of functionality you would like to see wrapped in an Eloquent component?</p>
<hr />
<p>You can find the <a href="https://github.com/eloquentarduino/EloquentVision/blob/master/src/ESP32CameraHTTPVideoStreamingServer.h">class code</a> and the <a href="https://github.com/eloquentarduino/EloquentVision/blob/master/examples/ESP32CameraHTTPVideoStreamingServerExample/ESP32CameraHTTPVideoStreamingServerExample.ino">example sketch</a> on the <a href="https://github.com/eloquentarduino/EloquentVision">Github repo</a>.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/06/easy-esp32-camera-http-video-streaming-server/">Easy ESP32 camera HTTP video streaming server</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Arduino dimensionality reduction (PCA) for Machine Learning projects</title>
		<link>https://eloquentarduino.github.io/2020/06/arduino-dimensionality-reduction-pca-for-machine-learning-projects/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 07 Jun 2020 07:24:20 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[pca]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1174</guid>

					<description><![CDATA[<p>When working with Machine Learning projects on microcontrollers and embedded devices the dimension of features can become a limiting factor due to the lack of RAM: dimensionality reduction (eg. PCA) will help you shrink your models and even achieve higher prediction accuracy. Why dimensionality reduction on Arduino microcontrollers? Dimensionality reduction is a tecnique you see [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/06/arduino-dimensionality-reduction-pca-for-machine-learning-projects/">Arduino dimensionality reduction (PCA) for Machine Learning projects</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>When working with <strong>Machine Learning projects</strong> on microcontrollers and embedded devices the dimension of features can become a limiting factor due to the lack of RAM: <strong>dimensionality reduction</strong> (eg. PCA) will help you shrink your models and even achieve higher prediction accuracy.</p>
<p><a href="https://setosa.io/ev/principal-component-analysis"><img src="https://setosa.io/ev/principal-component-analysis/fb-thumb.png" alt="PCA application example" /></a></p>
<p><span id="more-1174"></span></p>
<h2>Why dimensionality reduction on Arduino microcontrollers?</h2>
<p><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction">Dimensionality reduction</a> is a tecnique you see often in Machine Learning projects. By stripping away &quot;unimportant&quot; or redundant information, it generally helps speeding up the training process and achieving higher classification performances.</p>
<p>Since we now know we can run Machine Learning on Arduino boards and embedded microcontrollers, it can become a key tool at our disposal to squeeze out the most out of our boards.</p>
<p>In the specific case of resource-constrained devices as old Arduino boards (the UNO for example, with only 2 kb of RAM), it can become a decisive turn in unlocking even more application scenarios where the high dimensionality of the input features would not allow any model to fit.</p>
<p>Let's take the <a href="/2019/12/how-to-do-gesture-identification-on-arduino/">Gesture classification project</a> as an example: among the different classifiers we trained, only one fitted on the Arduino UNO, since most of them required too much flash memory due to the high dimension of features (90) and support vectors (25 to 61).</p>
<p>In this post I will resume that example and see if dimensionality reduction can help reduce this gap.</p>
<p>If you are working on a project with many features, let me know in the comments so I can create a detailed list of real world examples.</p>
<h2>How to export PCA (Principal Component Analysis) to plain C</h2>
<p>Among the many algorithms available for dimensionality reduction, I decided to start with <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA (Principal Component Analysis)</a> because it's one of the most widespread. In the next weeks I will probably work on porting other alternatives.</p>
<p>If you never used my Python package <a href="https://github.com/eloquentarduino/micromlgen">micromlgen</a> I first invite you to read <a href="/2019/11/you-can-run-machine-learning-on-arduino/">the introduction post</a> to get familiar with it.</p>
<p>Always remember to install the latest version, since I publish frequent updates.</p>
<pre><code class="language-bash">pip install --upgrade micromlgen</code></pre>
<p>Now it is pretty straight-forward to convert a sklearn PCA transformer to plain C: you use the magic method <code>port</code>. In addition to converting SVM/RVM classifiers, it is now able to export PCA too.</p>
<pre><code class="language-python">from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from micromlgen import port

if __name__ == &#039;__main__&#039;:
    X = load_iris().data
    pca = PCA(n_components=2, whiten=False).fit(X)

    print(port(pca))</code></pre>
<h2>How to deploy PCA to Arduino</h2>
<p>To use the exported code, we first have to include it in our sketch. Save the contents to a file (I named it <code>pca.h</code>) in the same folder of your <code>.ino</code> project and include it.</p>
<pre><code class="language-cpp">#include &quot;pca.h&quot;

// this was trained on the IRIS dataset, with 2 principal components
Eloquent::ML::Port::PCA pca;</code></pre>
<p>The <code>pca</code> object is now able to take an array of size N as input and return an array of size K as output, with K &lt; N usually.</p>
<pre><code class="language-cpp">void setup() {
    float x_input[4] = {5.1, 3.5, 1.4, 0.2};
    float x_output[2];

    pca.transform(x_input, x_output);
}</code></pre>
<p>That's it: now you can run your classifier on <code>x_output</code>.</p>
<pre><code class="language-cpp">#include &quot;pca.h&quot;
#include &quot;svm.h&quot;

Eloquent::ML::Port::PCA pca;
Eloquent::ML::Port::SVM clf;

void setup() {
    float x_input[4] = {5.1, 3.5, 1.4, 0.2};
    float x_output[2];
    int y_pred;

    pca.transform(x_input, x_output);

    y_pred = clf.predict(x_output);
}</code></pre>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2>A real world example</h2>
<p>As I anticipated, let's take a look at how PCA dimensionality reduction can help in fitting classifiers that would otherwise be too large to fit on our microcontrollers.</p>
<p>This is the exact table from the <a href="/2019/12/how-to-do-gesture-identification-on-arduino/">Gesture classification project</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Kernel</th>
<th style="text-align: center;">C</th>
<th style="text-align: center;">Gamma</th>
<th style="text-align: center;">Degree</th>
<th style="text-align: center;">Vectors</th>
<th style="text-align: center;">Flash size</th>
<th style="text-align: center;">RAM (b)</th>
<th style="text-align: center;">Avg accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RBF</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">53 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Poly</strong></td>
<td style="text-align: center;"><strong>100</strong></td>
<td style="text-align: center;"><strong>0.001</strong></td>
<td style="text-align: center;"><strong>2</strong></td>
<td style="text-align: center;"><strong>12</strong></td>
<td style="text-align: center;"><strong>25 Kb</strong></td>
<td style="text-align: center;"><strong>1228</strong></td>
<td style="text-align: center;"><strong>99%</strong></td>
</tr>
<tr>
<td style="text-align: left;">Poly</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">40 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">97%</td>
</tr>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">55 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">95%</td>
</tr>
<tr>
<td style="text-align: left;">RBF</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">80 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">95%</td>
</tr>
</tbody>
</table>
<p>The dataset has 90 features (30 samples x 3 axes) and achieves 99% accuracy. </p>
<p>Let's pick the <code>poly</code> kernel with degree <code>2</code> and see how much we can decrease the number of components while still achieving a good accuracy.</p>
<table>
<thead>
<tr>
<th>PCA components</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Support vectors</th>
</tr>
</thead>
<tbody>
<tr>
<td>90</td>
<td style="text-align: center;">99%</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td>50</td>
<td style="text-align: center;">99%</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td>40</td>
<td style="text-align: center;">99%</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td>30</td>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td>20</td>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td>15</td>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td><strong>10</strong></td>
<td style="text-align: center;"><strong>99%</strong></td>
<td style="text-align: center;"><strong>18</strong></td>
</tr>
<tr>
<td>5</td>
<td style="text-align: center;">76%</td>
<td style="text-align: center;">28</td>
</tr>
</tbody>
</table>
<p>We clearly see a couple of things:</p>
<ol>
<li>we still achieve 99% accuracy even with only 40 out of 90 principal components</li>
<li>we get a satisfactory 90% accuracy even <strong>with only 15 components</strong></li>
<li>(this is a bit unexpected) it looks like there's a sweet spot at 10 components where the accuracy skyrockets to 99% again. <em>This could be just a contingency of this particular dataset, don't expect to replicate this results on your own dataset</em></li>
</ol>
<p>What do these numbers mean to you? It means your board has to do many less computations to give you a prediction and will probably be able to host a more complex model.</p>
<p>Let's check out the figures with <code>n_components = 10</code> compared with the ones without PCA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Kernel</th>
<th style="text-align: center;">PCA support vectors</th>
<th style="text-align: center;">PCA flash size</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RBF C=10</td>
<td style="text-align: center;">46 (+24%)</td>
<td style="text-align: center;">32 Kb (-40%)</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;">RBF C=100</td>
<td style="text-align: center;">28 (-54%)</td>
<td style="text-align: center;">32 Kb (-60%)</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Poly 2</strong></td>
<td style="text-align: center;">13 (-48%)</td>
<td style="text-align: center;">28 Kb (+12%)</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Poly 3</strong></td>
<td style="text-align: center;">24 (-4%)</td>
<td style="text-align: center;">32 Kb (-20%)</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear</strong></td>
<td style="text-align: center;">18 (-64%)</td>
<td style="text-align: center;">29 Kb (-47%)</td>
<td style="text-align: center;">99%</td>
</tr>
</tbody>
</table>
<p>A couple notes:</p>
<ol>
<li>accuracy increased (on stayed the same) for all kernels</li>
<li>with one exception, flash size decreased in the range 20 - 50%</li>
<li>now we can fit 3 classifiers on our Arduino UNO instead of only one</li>
</ol>
<p>I will probably spend some more time investingating the usefulness of PCA for Arduino Machine Learning projects, but for now that's it: it's a good starting point in my opinion.</p>
<hr />
<p>There's a little example sketch on <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/PCAExample/PCAExample.ino">Github</a> that applies PCA to the IRIS dataset.</p>
<p>Tell me what you think may be a clever application of dimensionality reduction in the world of microcontrollers and see if we can build something great together.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/06/arduino-dimensionality-reduction-pca-for-machine-learning-projects/">Arduino dimensionality reduction (PCA) for Machine Learning projects</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Anomaly detection on your Arduino microcontroller via One Class SVM</title>
		<link>https://eloquentarduino.github.io/2020/05/anomaly-detection-on-your-arduino-microcontroller-via-one-class-svm/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 31 May 2020 16:44:36 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1156</guid>

					<description><![CDATA[<p>Support Vector Machines are very often used for classification tasks: but you may not know that they're so flexible they can be used for anomaly detection and novelty detection. Thanks to the micromlgen package, you can run One Class SVM on your Arduino microcontorller. What is anomaly / novelty detection useful for? Detect noise or [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/05/anomaly-detection-on-your-arduino-microcontroller-via-one-class-svm/">Anomaly detection on your Arduino microcontroller via One Class SVM</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="/tag/svm/">Support Vector Machines</a> are very often used for classification tasks: but you may not know that they're so flexible they can be used for <a href="https://scikit-learn.org/stable/modules/outlier_detection.html" target="_blank" rel="noopener noreferrer"><strong>anomaly detection and novelty detection</strong></a>. Thanks to the <a href="https://github.com/eloquentarduino/micromlgen">micromlgen</a> package, you can run One Class SVM on your Arduino microcontorller.</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_oneclass_001.png" alt="Novelty detection from sklearn documentation" title="Novelty detection from sklearn documentation" /><br />
<span id="more-1156"></span></p>
<h2>What is anomaly / novelty detection useful for?</h2>
<h3>Detect noise or anomalies</h3>
<p>As the name implies, anomaly detection can be used to monitor a stream of data and alert you when something unexpected happens.<br />
Think of an Industrial IoT setup where you have a bunch of sensors monitoring the working state of a production plant: you want to know as soon as possible if something bad is gonna happen.</p>
<p>In this case, anomaly detection tells you if your machinery is acting in a different way from the normal state so you can take action.</p>
<h3>Ignore irrelevant data</h3>
<p><em>(This application was suggested from two of my readers)</em><br />
Say you're developing a super simple word classification project: you want to distinguish door bell from fire alarm (as per one of the two readers).<br />
So you train your SVM classifier and use <a href="https://github.com/eloquentarduino/micromlgen">micromlgen</a> to run it on your Arduino microcontroller.</p>
<p>It works well, but we have a problem: you live in a noisy environment with many sounds, so not all of them will either be door bells or fire alarms. Since your classifier is binary, it <em>has to classify all of the sounds</em> as either A or B.</p>
<p>The solution will be <strong>novelty detection</strong>: before running the binary SVM, you run the <strong>OneClassSVM</strong> to filter known sounds (bell and alarm) from unknown ones (eg. dog barking).<br />
If OneClassSVM predicts the sound as a novelty, you discard it since it's of no interest for you. If it predicts the sound as known, you run the binary SVM.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2>How to run anomaly / novelty detection on Arduino microcontroller via OneClassSVM</h2>
<p>Porting a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html">OneClassSVM</a> from Python to plain C++ is as easy as a single command in the <strong>micromlgen</strong> package:</p>
<pre><code class="language-python">from sklearn.svm import OneClassSVM
from micromlgen import port

clf = OneClassSVM(kernel=&quot;rbf&quot;, nu=0.5, gamma=0.1)
clf.fit(X, y)
print(port(clf))</code></pre>
<div class="watchout">
You will need micromlgen version 1.0.2 to port OneClassSVM. If you have an outdated version, please run <code>pip install --upgrade micromlgen</code>
</div>
<p>If you read <a href="/tag/svm/">my previous posts</a> about <strong>micromlgen</strong> and SVM the above snippet should be familiar: with the latest release, <code>port</code> is able to export either SVC, LinearSVC, OneClassSVC and <a href="/2020/02/even-smaller-machine-learning-models-for-your-mcu/">RVC (Relevant Vector Machines)</a> to object oriented C++.</p>
<p>Now you can embed the generated code in your Arduino sketch.</p>
<pre><code class="language-cpp">#include &quot;OneClassSVM.h&quot;

Eloquent::ML::Port::OneClassSVM clf;

void setup() {
  Serial.begin(115200);
  delay(2000);

  for (int i = 0; i &lt; DATASET_SIZE; i++)
    clf.predict(X[i]);
}

void loop() {}</code></pre>
<hr />
<p>I created an example sketch from a synthetic dataset for anomaly detection (copied from a <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py">scikit-learn example</a>) you can run to get a feel of how it performs.</p>
<p>Go checkout the <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/OneClassSVMExample/OneClassSVMExample.ino">Github repo</a></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/05/anomaly-detection-on-your-arduino-microcontroller-via-one-class-svm/">Anomaly detection on your Arduino microcontroller via One Class SVM</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Easier, faster pure video ESP32 cam motion detection</title>
		<link>https://eloquentarduino.github.io/2020/05/easier-faster-pure-video-esp32-cam-motion-detection/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 10 May 2020 19:26:08 +0000</pubDate>
				<category><![CDATA[Computer vision]]></category>
		<category><![CDATA[camera]]></category>
		<category><![CDATA[esp32]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1110</guid>

					<description><![CDATA[<p>If you liked my post about ESP32 cam motion detection, you'll love this updated version: it's easier to use and blazing fast! The post about pure video ESP32 cam motion detection without an external PIR is my most successful post at the moment. Many of you are interested about this topic. One of my readers, [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/05/easier-faster-pure-video-esp32-cam-motion-detection/">Easier, faster pure video ESP32 cam motion detection</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>If you liked my post about <a href="/2020/01/motion-detection-with-esp32-cam-only-arduino-version/">ESP32 cam motion detection</a>, you'll love this updated version: it's easier to use and blazing fast!</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Faster-motion-detection.gif" alt="Faster motion detection" /></p>
<p><span id="more-1110"></span></p>
<p>The post about <strong>pure video ESP32 cam motion detection</strong> without an external PIR is my most successful post at the moment. Many of you are interested about this topic.</p>
<p>One of my readers, though, pointed out my implementation was quite slow and he only achieved bare 5 fps in his project. So he asked for a better alternative.</p>
<p>Since the post was of great interest for many people, I took the time to revisit the code and make improvements.</p>
<p>I came up with a 100% re-writing that is both easier to use and faster. Actually, it is <strong>blazing fast!</strong>.</p>
<p>Let's see how it works.</p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocdownsampling">Downsampling</a><ol><li><a href="#tocnearest-neighbor">Nearest neighbor</a><li><a href="#tocfull-block-average">Full block average</a><li><a href="#toccore-block-average">Core block average</a><li><a href="#toccross-block-average">Cross block average</a><li><a href="#tocdiagonal-block-average">Diagonal block average</a><li><a href="#tocimplement-your-own">Implement your own</a></li></ol><li><a href="#tocbenchmarks">Benchmarks</a><li><a href="#tocmotion-detection">Motion detection</a><li><a href="#tocfull-code">Full code</a></ol></div></p>
<h2 id="tocdownsampling">Downsampling</h2>
<p>In the original post I introduced the idea of downsampling the image from the camera for a faster and more robust motion detection. I wrote the code in the main sketch to keep it self-contained.</p>
<p>Looking back now it was a poor choice, since it cluttered the project and distracted from the main purpose, which is motion detection.</p>
<p>Moreover, I thought that scanning the image buffer in sequential order would be the fastest approach.</p>
<p>It turns out I was wrong.</p>
<p>This time I scan the image buffer following the blocks that will compose the resulting image and the results are... much faster.</p>
<p>Also, I decided to inject some more efficiency that will further speedup the computation: using different strategies for downsampling.</p>
<p>The idea of downsampling is that you have to &quot;collapse&quot; a block of NxN from the original image to just one pixel of the resulting image.</p>
<p>Now, there are a variety of ways you can accomplish this. The first two I present here are the most obvious, the other two are of my &quot;invention&quot;: nothing fancy nor new, but they're fast and serve the purpose well.</p>
<h3 id="tocnearest-neighbor">Nearest neighbor</h3>
<p>You can just pick the center of the NxN block and use its value for the output.<br />
Of course it is fast (possibly the fastest approach), but wouldn't be very accurate. One pixel out of NxN wouldn't be representative of the overall region and will heavily suffer from noise.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Nearest-diagram.png" alt="Nearest diagram" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/nn.jpg" alt="Nearest neighbor block averaging" /></p>
<h3 id="tocfull-block-average">Full block average</h3>
<p>This is the most intuitive alternative: use the average of all the pixels in the block as the ouput value. This is arguabily the &quot;proper&quot; way to do it, since you're using all the pixels in the source image to compute the new one.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Full-diagram.png" alt="Full diagram" /><br />
<img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/full.jpg" alt="Full block averaging" /></p>
<h3 id="toccore-block-average">Core block average</h3>
<p>As a faster alternative, I thought that averaging only the &quot;core&quot; (the most internal part) of the block would have been a good-enough solution. It has no theoretical proof that this yields true, but our task here is to create a smaller <em>representation</em> of the original image, not producing an accurate smaller version.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Core-diagram.png" alt="Core diagram" /><br />
<img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/core.jpg" alt="Core block averaging" /></p>
<p><em>I'll stress this point: the only reason we do downsampling is to compare two sequential frame and detect if they differ above a certain threshold. This downsampling doesn't have to mimic the actual image: it can transform the source in any fancy way, as long as it stays consistent and captures the variations over time.</em></p>
<h3 id="toccross-block-average">Cross block average</h3>
<p>This time we consider all the pixels along the vertical and horizontal central axes. The idea is that you will capture a good portion of the variation along both the axis, given quite accurate results.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Cross-diagram.png" alt="Cross diagram" /><br />
<img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/cross.jpg" alt="Cross block averaging" /></p>
<h3 id="tocdiagonal-block-average">Diagonal block average</h3>
<p>This alternative too came to my mind from nowhere, really. I just think it is a good alternative to capture all the block's variation, probably even better than vertical and horizontal directions.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/Diagonal-diagram.png" alt="Diagonal diagram" /><br />
<img src="https://eloquentarduino.github.io/wp-content/uploads/2020/05/diagonal.jpg" alt="Diagonal block averaging" /></p>
<h3 id="tocimplement-your-own">Implement your own</h3>
<p>Not satisfied from the methods above? No problem, you can still implement your own.</p>
<p>The ones presented above are just some algorithms that came to my mind: I'm not telling you they're the best.</p>
<p>They worked for me, that's it.</p>
<p>If you think you found a better solution, I encourage you implement it and even share it with me and the other readers, so we can all make progress on this together.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2 id="tocbenchmarks">Benchmarks</h2>
<p>So, at the very beginning I said this new implementation is blazingly fast. </p>
<p>How much fast?</p>
<p>As fast as it can be, arguably.</p>
<p>I mean, so fast it won't alter your fps.</p>
<p>Look at the results I got on my M5Stack camera.</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th style="text-align: center;">Time to execute (micros)</th>
<th style="text-align: right;">FPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td>Nearest neighbor</td>
<td style="text-align: center;">160</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td>Cross block</td>
<td style="text-align: center;">700</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td>Core block</td>
<td style="text-align: center;">800</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td>Diagonal block</td>
<td style="text-align: center;">950</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td>Full block</td>
<td style="text-align: center;">4900</td>
<td style="text-align: right;">12</td>
</tr>
</tbody>
</table>
<p>As you can see, only the full block creates a delay in the process (quite a bit of delay even): the other  methods  won't slow down your program in any noticeable way.</p>
<p>If you test Nearest neighbor and it works for you, then you'll be extremely light on computation resources with only <strong>160 microseconds</strong> of delay.</p>
<p>This is what I mean by <em>blazing fast</em>.</p>
<h2 id="tocmotion-detection">Motion detection</h2>
<p>The motion detection part hasn't changed, so I point you to <a href="/2020/01/motion-detection-with-esp32-cam-only-arduino-version#tocblocks-difference-threshold">the original post</a> to read more about the Block difference threshold and the Image difference threshold.</p>
<h2 id="tocfull-code">Full code</h2>
<pre><code class="language-cpp">#define CAMERA_MODEL_M5STACK_WIDE
#include &quot;EloquentVision.h&quot;

#define FRAME_SIZE FRAMESIZE_QVGA
#define SOURCE_WIDTH 320
#define SOURCE_HEIGHT 240
#define BLOCK_SIZE 10
#define DEST_WIDTH (SOURCE_WIDTH / BLOCK_SIZE)
#define DEST_HEIGHT (SOURCE_HEIGHT / BLOCK_SIZE)
#define BLOCK_DIFF_THRESHOLD 0.2
#define IMAGE_DIFF_THRESHOLD 0.1
#define DEBUG 0

using namespace Eloquent::Vision;

ESP32Camera camera;
uint8_t prevFrame[DEST_WIDTH * DEST_HEIGHT] = { 0 };
uint8_t currentFrame[DEST_WIDTH * DEST_HEIGHT] = { 0 };

// function prototypes
bool motionDetect();
void updateFrame();

/**
 *
 */
void setup() {
    Serial.begin(115200);
    camera.begin(FRAME_SIZE, PIXFORMAT_GRAYSCALE);
}

/**
 *
 */
void loop() {
    /**
     * Algorithm:
     *  1. grab frame
     *  2. compare with previous to detect motion
     *  3. update previous frame
     */

    time_t start = millis();
    camera_fb_t *frame = camera.capture();

    downscaleImage(frame-&gt;buf, currentFrame, nearest, SOURCE_WIDTH, SOURCE_HEIGHT, BLOCK_SIZE);

    if (motionDetect()) {
        Serial.print(&quot;Motion detected @ &quot;);
        Serial.print(floor(1000.0f / (millis() - start)));
        Serial.println(&quot; FPS&quot;);
    }

    updateFrame();
}

/**
 * Compute the number of different blocks
 * If there are enough, then motion happened
 */
bool motionDetect() {
    uint16_t changes = 0;
    const uint16_t blocks = DEST_WIDTH * DEST_HEIGHT;

    for (int y = 0; y &lt; DEST_HEIGHT; y++) {
        for (int x = 0; x &lt; DEST_WIDTH; x++) {
            float current = currentFrame[y * DEST_WIDTH + x];
            float prev = prevFrame[y * DEST_WIDTH + x];
            float delta = abs(current - prev) / prev;

            if (delta &gt;= BLOCK_DIFF_THRESHOLD)
                changes += 1;
        }
    }

    return (1.0 * changes / blocks) &gt; IMAGE_DIFF_THRESHOLD;
}

/**
 * Copy current frame to previous
 */
void updateFrame() {
    memcpy(prevFrame, currentFrame, DEST_WIDTH * DEST_HEIGHT);
}</code></pre>
<hr />
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentVision/blob/master/examples/FasterMotionDetection/FasterMotionDetection.ino">Github</a> and remember to star!</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/05/easier-faster-pure-video-esp32-cam-motion-detection/">Easier, faster pure video ESP32 cam motion detection</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Incremental multiclass classification on microcontrollers: One vs One</title>
		<link>https://eloquentarduino.github.io/2020/04/incremental-multiclass-classification-on-microcontrollers-one-vs-one/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 26 Apr 2020 08:01:14 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[incremental-learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[ml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1079</guid>

					<description><![CDATA[<p>In earlier posts I showed you can run incremental binary classification on your microcontroller with Stochastic Gradient Descent or Passive-Aggressive classifier. Now it is time to upgrade your toolbelt with a new item: One-vs-One multiclass classifier. One vs One Many classifiers are, by nature, binary: they can only distinguish the positive class from the negative [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/04/incremental-multiclass-classification-on-microcontrollers-one-vs-one/">Incremental multiclass classification on microcontrollers: One vs One</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In earlier posts I showed you can run incremental binary classification on your microcontroller with Stochastic Gradient Descent or Passive-Aggressive classifier. Now it is time to upgrade your toolbelt with a new item: One-vs-One multiclass classifier.</p>
<p><span id="more-1079"></span></p>
<h2>One vs One</h2>
<p>Many classifiers are, by nature, binary: they can only distinguish the positive class from the negative one. Many of real-world problems, however, are multiclass: you have 3 or more possible outcomes to distinguish from.</p>
<p>There are a couple of ways to achieve this:</p>
<ol>
<li><strong>One vs All</strong>: if your classifier is able to output a confidence score of its prediction, for N classes you train N classifiers, each able to recognize a single class. During inference, you pick the &quot;most confident&quot; one.</li>
<li><strong>One vs One</strong>: for N classes, you train N * (N-1) / 2 classifiers, one for each couple of classes. During inference, each classifier makes a prediction and you pick the class with the highest number of votes.</li>
</ol>
<p>Since SGD and Passive-Aggressive don't output a confidence score, I implemented the One vs One algorithm to tackle the multiclass classification problem on microcontrollers.</p>
<p>Actually, One vs One is not a new type of classifier: it is really a &quot;coordinator&quot; class that sorts which samples go to which classifier. You can still choose your own classifier type to use.</p>
<p>As SGD and Passive-Aggressive, OneVsOne implements the classifier interface, so you will use the well known <code>fitOne</code> and <code>predict</code> methods.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2>Example code</h2>
<pre><code class="language-cpp">// Esp32 has some problems with min/max
#define min(a, b) (a) &lt; (b) ? (a) : (b)
#define max(a, b) (a) &gt; (b) ? (a) : (b)
// you will actually need only one of SGD or PassiveAggressive
#include &quot;EloquentSGD.h&quot;
#include &quot;EloquentPassiveAggressive.h&quot;
#include &quot;EloquentOneVsOne.h&quot;
#include &quot;EloquentAccuracyScorer.h&quot;
// this file defines NUM_FEATURES, NUM_CLASSES, TRAIN_SAMPLES and TEST_SAMPLES
#include &quot;dataset.h&quot;

using namespace Eloquent::ML;

void setup() {
  Serial.begin(115200);
  delay(3000);
}

void loop() {
  AccuracyScorer scorer;
  // OneVsOne needs the actual classifier class, the number of features and the number of classes
  OneVsOne&lt;SGD&lt;FEATURES_DIM&gt;, FEATURES_DIM, NUM_CLASSES&gt; clf;

  // clf.set() propagates the configuration to the actual classifiers
  // if a parameter does not exists on the classifier, it does nothing
  // in this example, alpha and momentum refer to SGD, C to Passive-Aggressive
  clf.set(&quot;alpha&quot;, 1);
  clf.set(&quot;momentum&quot;, 0.7);
  clf.set(&quot;C&quot;, 0.1);

  // fit
  // I noticed that repeating the training a few times over the same dataset increases performance  to a certain extent: if you re-train it too much, performance will decay
  for (unsigned int i = 0; i &lt; TRAIN_SAMPLES * 5; i++) {
      clf.fitOne(X_train[i % TRAIN_SAMPLES], y_train[i % TRAIN_SAMPLES]);
  }

  // predict
  for (int i = 0; i &lt; TEST_SAMPLES; i++) {
      int y_true = y_test[i];
      int y_pred = clf.predict(X_test[i]);

      Serial.print(&quot;Predicted &quot;);
      Serial.print(y_pred);
      Serial.print(&quot; vs &quot;);
      Serial.println(y_true);
      scorer.scoreOne(y_true, y_pred);
  }

  Serial.print(&quot;Accuracy = &quot;);
  Serial.print(scorer.accuracy() * 100);
  Serial.print(&quot; out of &quot;);
  Serial.print(scorer.support());
  Serial.println(&quot; samples&quot;);
  delay(30000);
}</code></pre>
<p>If you refer to the previous posts on <a href="/2020/04/stochastic-gradient-descent-on-your-microcontroller/">SGD</a> and <a href="/2020/04/passive-aggressive-classifier-for-embedded-devices/">Passive-Aggressive</a>, you'll notice that you would be able to replace one with the other and your code will change by <strong>1 single line only</strong>. This let's you experiment to find the best configuration for your project without hassle.</p>
<h2>Accuracy</h2>
<p>Well, accuracy vary.</p>
<p>In my tests, I couldn't get predictable accuracy on all datasets. I couldn't even get acceptable accuracy on the Iris dataset (60% max). But I got 90% accuracy on the Digits dataset from scikit-learn with 6 classes.</p>
<p>You have to experiment. Try Passive-Aggressive with many <code>C</code> values. If it doesn't work, try SGD with varying <code>momentum</code> and <code>alpha</code>. Try to repeat the training over the dataset 5, 10 times.</p>
<p>In a next post I'll report my benchmarks so you can see what works for you and what not.<br />
This is an emerging field for me, so I will need time to master it.</p>
<hr />
<p>As always, you can find the examle on <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/OvOExample/OvOExample.ino">Github</a> with a the dataset to experiment with.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/04/incremental-multiclass-classification-on-microcontrollers-one-vs-one/">Incremental multiclass classification on microcontrollers: One vs One</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
