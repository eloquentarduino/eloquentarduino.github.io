{
    "version": "https://jsonfeed.org/version/1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://eloquentarduino.github.io/feed/json/ -- and add it your reader.",
    "home_page_url": "https://eloquentarduino.github.io/",
    "feed_url": "https://eloquentarduino.github.io/feed/json/",
    "title": "Eloquent Arduino Blog",
    "description": "Machine learning on Arduino, programming &amp; electronics",
    "items": [
        {
            "id": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "url": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "title": "Apple or Orange? Image recognition with ESP32 and Arduino",
            "content_html": "<p>Do you have an ESP32 camera? </p>\n<p>Want to do image recognition directly on your ESP32, without a PC?</p>\n<p>In this post we'll look into a very basic image recognition task: <strong>distinguish apples from oranges with machine learning</strong>.</p>\n<p><img src=\"/wp-content/uploads/2020/01/Apple-vs-Orange.gif\" alt=\"Apple vs Orange\" /></p>\n<p><span id=\"more-820\"></span></p>\n<p>Image recognition is a very hot topic these days in the AI/ML landscape. <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">Convolutional Neural Networks</a> really shines in this task and can achieve almost perfect accuracy on many scenarios.</p>\n<p>Sadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.</p>\n<p>Since in this series about <a href=\"/category/programming/arduino-machine-learning/\">Machine Learning on Microcontrollers</a> we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.</p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-were-going-to-do\">What we're going to do</a><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocextracting-rgb-components\">Extracting RGB components</a><li><a href=\"#tocrecord-samples-image\">Record samples image</a><li><a href=\"#toctraining-the-classifier\">Training the classifier</a><li><a href=\"#tocreal-world-example\">Real world example</a><ol><li><a href=\"#tocdisclaimer\">Disclaimer</a></ol></div></p>\n<h2 id=\"tocwhat-were-going-to-do\">What we're going to do</h2>\n<p>In a previous post about <a href=\"/2019/12/color-identification-on-arduino/\">color identification with Machine learning</a>, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.</p>\n<p>Of course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.</p>\n<p>Object inference, in that case, works only if you have exactly one object for a given color.</p>\n<p>The objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.</p>\n<p>This is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.</p>\n<div class=\"watchout\">\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. <b>This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame</b>. It will classify the current image based on the samples recorded during training.\n</div>\n<p>As any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.</p>\n<h2 id=\"tocfeatures-definition\">Features definition</h2>\n<p>I have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.</p>\n<p>Those few times I used CNN, I always used the whole image as input, <em>as-is</em>. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.</p>\n<p>I don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.</p>\n<p>I said we're using the RGB components of the image. But not all of them.</p>\n<p>Even at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.</p>\n<p>We need to reduce this number  to the bare minimum.</p>\n<p>How much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?</p>\n<p>You would be surprised to know that I got 90% accuracy with an RGB image of <strong>8x6</strong>!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Orange-and-Apple-pixelated.jpg\" alt=\"You actually need very few pixels to do image classification\" /></p>\n<p>Yes, that's all we really need to do a <em>good enough</em> classification.</p>\n<hr /><p><em>You can distinguish apples from oranges on ESP32 with 8x6 pixels only!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fimage-recognition-with-esp32-and-arduino%2F&#038;text=You%20can%20distinguish%20apples%20from%20oranges%20on%20ESP32%20with%208x6%20pixels%20only%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Of course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.</p>\n<p>You have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.</p>\n<p>So we'll stick to 8x6 images.</p>\n<p>Now, how do you compact a 160x120 image to 8x6? With <em>downsampling</em>.</p>\n<p>This is the same tecnique we've used in the post about <a href=\"/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">motion detection on ESP32</a>: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).</p>\n<p><img src=\"/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>This time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.</p>\n<p>This is the code excerpt that does the downsampling.</p>\n<pre><code class=\"language-cpp\">uint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}</code></pre>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">STAY UP TO DATE</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2 id=\"tocextracting-rgb-components\">Extracting RGB components</h2>\n<p>The ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):</p>\n<ol>\n<li><strong>grayscale</strong>: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH</li>\n<li><strong>RGB565</strong>: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2</li>\n<li><strong>JPEG</strong>: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results</li>\n</ol>\n<p>For our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.</p>\n<p><img src=\"https://s2.www.theimagingsource.com/application-1.1.29/documentation/ic_imaging_control_class/en_US/images/rgb565.gif\" alt=\"taken from https://www.theimagingsource.com/support/documentation/ic-imaging-control-cpp/PixelformatRGB565.htm\" /></p>\n<pre><code class=\"language-cpp\">config.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}</code></pre>\n<h2 id=\"tocrecord-samples-image\">Record samples image</h2>\n<p>Now that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.</p>\n<p>Before doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.</p>\n<pre><code class=\"language-cpp\">#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}</code></pre>\n<p>Now you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.</p>\n<div class=\"watchout\">\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. <br />You need to be very accurate during this phase if you want to achieve good results.<br /> I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n</div>\n<h2 id=\"toctraining-the-classifier\">Training the classifier</h2>\n<p>To train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on <a href=\"/2019/11/how-to-create-a-classifier-for-arduino-machine-learning-projects/\">how to train a ML classifier for Arduino</a> to get the exported model.</p>\n<p>You can experiment with different classifier configurations. </p>\n<p>My features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).</p>\n<p>One odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.</p>\n<p>The model produced 13 support vectors.</p>\n<p>I did no features scaling: you could try it if classifying more than 2 classes and having poor results.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange-decision-boundaries.png\" alt=\"Apple vs Orange decision boundaries\" /></p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>If you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.</p>\n<div style=\"width: 788px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-820-1\" width=\"788\" height=\"443\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4?_=1\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4</a></video></div>\n<p></p>\n<p>The little white object you see at the bottom of the image is the camera, taped to the desk.</p>\n<p>Did you think it was possible to do simple image classification on your ESP32?</p>\n<h3 id=\"tocdisclaimer\">Disclaimer</h3>\n<p>This is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.</p>\n<p>You have to carefully craft your setup and be as consistent as possible between training and inferencing.</p>\n<p>Still, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.</p>\n<p>In the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.</p>\n<p>Now that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? </p>\n<p>Let me know in the comments, we could even try realize it together if you need some help.</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/Apple_vs_Orange/Apple_vs_Orange.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/\">Apple or Orange? Image recognition with ESP32 and Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? \nWant to do image recognition directly on your ESP32, without a PC?\nIn this post we'll look into a very basic image recognition task: distinguish apples from oranges with machine learning.\n\n\nImage recognition is a very hot topic these days in the AI/ML landscape. Convolutional Neural Networks really shines in this task and can achieve almost perfect accuracy on many scenarios.\nSadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.\nSince in this series about Machine Learning on Microcontrollers we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.\nTable of contentsWhat we're going to doFeatures definitionExtracting RGB componentsRecord samples imageTraining the classifierReal world exampleDisclaimer\nWhat we're going to do\nIn a previous post about color identification with Machine learning, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.\nOf course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.\nObject inference, in that case, works only if you have exactly one object for a given color.\nThe objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.\nThis is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.\n\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame. It will classify the current image based on the samples recorded during training.\n\nAs any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.\nFeatures definition\nI have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.\nThose few times I used CNN, I always used the whole image as input, as-is. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.\nI don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.\nI said we're using the RGB components of the image. But not all of them.\nEven at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.\nWe need to reduce this number  to the bare minimum.\nHow much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?\nYou would be surprised to know that I got 90% accuracy with an RGB image of 8x6!\n\nYes, that's all we really need to do a good enough classification.\nYou can distinguish apples from oranges on ESP32 with 8x6 pixels only!Click To Tweet\nOf course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.\nYou have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.\nSo we'll stick to 8x6 images.\nNow, how do you compact a 160x120 image to 8x6? With downsampling.\nThis is the same tecnique we've used in the post about motion detection on ESP32: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).\n\nThis time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.\nThis is the code excerpt that does the downsampling.\nuint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}\n\r\n\r\n\r\n    \r\n\tSTAY UP TO DATE\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nExtracting RGB components\nThe ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):\n\ngrayscale: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH\nRGB565: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2\nJPEG: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results\n\nFor our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.\n\nconfig.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}\nRecord samples image\nNow that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.\nBefore doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.\n#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}\nNow you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.\n\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. You need to be very accurate during this phase if you want to achieve good results. I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n\nTraining the classifier\nTo train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on how to train a ML classifier for Arduino to get the exported model.\nYou can experiment with different classifier configurations. \nMy features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).\nOne odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.\nThe model produced 13 support vectors.\nI did no features scaling: you could try it if classifying more than 2 classes and having poor results.\n\nReal world example\nIf you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.\n\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\n\nThe little white object you see at the bottom of the image is the camera, taped to the desk.\nDid you think it was possible to do simple image classification on your ESP32?\nDisclaimer\nThis is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.\nYou have to carefully craft your setup and be as consistent as possible between training and inferencing.\nStill, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.\nIn the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.\nNow that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? \nLet me know in the comments, we could even try realize it together if you need some help.\n\r\nCheck the full project code on Github\nL'articolo Apple or Orange? Image recognition with ESP32 and Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-12T11:32:08+01:00",
            "date_modified": "2020-01-19T11:34:31+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Arduino Machine learning"
            ],
            "attachments": [
                [
                    {
                        "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4",
                        "mime_type": "video/mp4",
                        "size_in_bytes": 1642079
                    }
                ]
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "url": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "title": "Motion detection with ESP32 cam only (Arduino version)",
            "content_html": "<p>Do you have an <strong>ESP32 camera</strong>? Do you want to do motion detection <em>WITHOUT ANY</em> external hardware?</p>\n<p>Here's a tutorial made just for you: <strong>30 lines of code</strong> and you will know when something changes in your video stream  \ud83c\udfa5</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.gif\" alt=\"ESP32 camera motion detection example\" /></p>\n<p><span id=\"more-779\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</a><li><a href=\"#toccant-i-use-an-external-pir\">Can't I use an external PIR?</a><ol><li><a href=\"#tocexternal-hardware\">External hardware</a><li><a href=\"#tocfield-of-view\">Field of View</a><li><a href=\"#toccold-objects\">Cold objects</a></li></ol><li><a href=\"#tocwhat-do-you-need\">What do you need?</a><li><a href=\"#tochow-does-it-work\">How does it work?</a><ol><li><a href=\"#tocdownsampling\">Downsampling</a><li><a href=\"#tocblocks-difference-threshold\">Blocks difference threshold</a><li><a href=\"#tocimage-difference-threshold\">Image difference threshold</a><li><a href=\"#toccombining-all-together\">Combining all together</a></li></ol><li><a href=\"#tocreal-world-example\">Real world example</a></ol></div></p>\n<h2 id=\"tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</h2>\n<p>Quoting from Wikipedia</p>\n<blockquote>\n<p>Motion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object</p>\n</blockquote>\n<p>In this project, we're implementing what I call <em>naive</em> motion detection: that is, we're not focusing on a particular object and following its motion.</p>\n<p>We'll only detect if any considerable portion of the image changed from one frame to the next.</p>\n<p>We won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.</p>\n<h2 id=\"toccant-i-use-an-external-pir\">Can't I use an external PIR?</h2>\n<p>Several projects on the internet about motion detection with an ESP32 cam use an external <a href=\"https://en.wikipedia.org/wiki/Passive_infrared_sensor\">PIR sensor</a> to trigger the video recording.</p>\n<p>What's the problem with that approach? </p>\n<h3 id=\"tocexternal-hardware\">1. External hardware</h3>\n<p>First of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice <a href=\"https://www.banggood.com/M5CameraF-ESP32-Fish-eye-Camera-Development-Board-Module-OV2640-Mini-Fisheye-Camera-Unit-Demoboard-p-1496820.html?rmmds=search&amp;cur_warehouse=CN\">M5stick camera</a> (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.</p>\n<h3 id=\"tocfield-of-view\">2. Field of View</h3>\n<p>PIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. </p>\n<p>My camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.</p>\n<h3 id=\"toccold-objects\">3. Cold objects</h3>\n<p>PIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).</p>\n<p>But motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.</p>\n<p>A PIR sensor can't do this: video motion detection can.</p>\n<hr /><p><em>ESP32 cam pure video motion detection can detect motion due to cold objects</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fmotion-detection-with-esp32-cam-only-arduino-version%2F&#038;text=ESP32%20cam%20pure%20video%20motion%20detection%20can%20detect%20motion%20due%20to%20cold%20objects&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<h2 id=\"tocwhat-do-you-need\">What do you need?</h2>\n<p>All you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:</p>\n<ul>\n<li>ESP32 cam</li>\n<li>ESP32 eye</li>\n<li>TTGO camera</li>\n<li>... any other flavor of ESP32 camera</li>\n</ul>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-models.jpg\" alt=\"ESP32 camera models\" /></p>\n<h2 id=\"tochow-does-it-work\">How does it work?</h2>\n<p>Ok, let's go to the &quot;technical&quot; stuff.</p>\n<p>Simply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.</p>\n<p>Well, it's <em>almost</em> like this.</p>\n<p>Of course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.</p>\n<h3 id=\"tocdownsampling\">Downsampling</h3>\n<p>One super-simple and super-effective way of doing this is to <strong>work with blocks</strong>, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.</p>\n<p>This greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>It's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  </p>\n<p>This has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.</p>\n<p>How should you choose the scale factor?</p>\n<p>Well, it depends.</p>\n<p>It depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. </p>\n<p>If you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.</p>\n<p>Experiment with your own use case a tweak with trial-and-error.</p>\n<h3 id=\"tocblocks-difference-threshold\">Blocks difference threshold</h3>\n<p>Once we've defined the block size, we need to detect if a block changed from one frame to the next.</p>\n<p>Of course, just testing for difference (<code>current != prev</code>) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.</p>\n<p>So we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.</p>\n<p>The higher the threshold, the less sensitive the algorithm will be.</p>\n<p>In code it is calculated as</p>\n<pre><code class=\"language-cpp\">float delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;</code></pre>\n<p>which indicates the relative increment/decrement from the previous value.</p>\n<h3 id=\"tocimage-difference-threshold\">Image difference threshold</h3>\n<p>Now that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.</p>\n<p>You could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.</p>\n<p>Let's return to the 320x240 image example. With a 10x10 block, you'll be working with <code>32x24 = 768</code> blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?</p>\n<p>I don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.</p>\n<p>If you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.</p>\n<p>In code it is calculated as:</p>\n<pre><code class=\"language-cpp\">float changedBlocksPercent = changedBlocks / totalBlocks</code></pre>\n<h3 id=\"toccombining-all-together\">Combining all together</h3>\n<p>Recapping: when running the motion detection algorithm you have 3 parameters to set:</p>\n<ol>\n<li>the block size</li>\n<li>the block difference threshold</li>\n<li>the image differerence threshold</li>\n</ol>\n<p>Let's pick 3 sensible defaults: <code>block size = 10</code>, <code>block threshold = 15%</code>, <code>image threshold = 20%</code>.</p>\n<p>What does these parameters translate to in the practice?</p>\n<p>They mean that motion will be detected if <code>20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next</code>.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-example.jpg\" alt=\"ESP32 camera motion example\" /></p>\n<p>As you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.</p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>Now the fun part. I'll show you how it performs on a real-world scenario.</p>\n<p>To keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. </p>\n<p>This means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.</p>\n<div style=\"width: 652px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-779-2\" width=\"652\" height=\"604\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4?_=2\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4</a></video></div>\n<p>This is me passing my arm in front of the camera a few times.</p>\n<p>The grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.</p>\n<p>The orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.</p>\n<p>When I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. </p>\n<p>While moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.</p>\n<p>This is why I suggest you keep the <code>image difference threshold</code> to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.</p>\n<p>Do you like the grid effect of the sample video? Let me know in the comment if you want me to share it.</p>\n<p>Or even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">STAY UP TO DATE</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ESP32CameraNaiveMotionDetection/ESP32CameraNaiveMotionDetection.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">Motion detection with ESP32 cam only (Arduino version)</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? Do you want to do motion detection WITHOUT ANY external hardware?\nHere's a tutorial made just for you: 30 lines of code and you will know when something changes in your video stream  \ud83c\udfa5\n\n\nTable of contentsWhat is (naive) motion detection?Can't I use an external PIR?External hardwareField of ViewCold objectsWhat do you need?How does it work?DownsamplingBlocks difference thresholdImage difference thresholdCombining all togetherReal world example\nWhat is (naive) motion detection?\nQuoting from Wikipedia\n\nMotion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object\n\nIn this project, we're implementing what I call naive motion detection: that is, we're not focusing on a particular object and following its motion.\nWe'll only detect if any considerable portion of the image changed from one frame to the next.\nWe won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.\nCan't I use an external PIR?\nSeveral projects on the internet about motion detection with an ESP32 cam use an external PIR sensor to trigger the video recording.\nWhat's the problem with that approach? \n1. External hardware\nFirst of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice M5stick camera (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.\n2. Field of View\nPIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. \nMy camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.\n3. Cold objects\nPIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).\nBut motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.\nA PIR sensor can't do this: video motion detection can.\nESP32 cam pure video motion detection can detect motion due to cold objectsClick To Tweet\nWhat do you need?\nAll you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:\n\nESP32 cam\nESP32 eye\nTTGO camera\n... any other flavor of ESP32 camera\n\n\nHow does it work?\nOk, let's go to the &quot;technical&quot; stuff.\nSimply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.\nWell, it's almost like this.\nOf course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.\nDownsampling\nOne super-simple and super-effective way of doing this is to work with blocks, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.\nThis greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.\n\nIt's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  \nThis has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.\nHow should you choose the scale factor?\nWell, it depends.\nIt depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. \nIf you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.\nExperiment with your own use case a tweak with trial-and-error.\nBlocks difference threshold\nOnce we've defined the block size, we need to detect if a block changed from one frame to the next.\nOf course, just testing for difference (current != prev) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.\nSo we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.\nThe higher the threshold, the less sensitive the algorithm will be.\nIn code it is calculated as\nfloat delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;\nwhich indicates the relative increment/decrement from the previous value.\nImage difference threshold\nNow that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.\nYou could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.\nLet's return to the 320x240 image example. With a 10x10 block, you'll be working with 32x24 = 768 blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?\nI don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.\nIf you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.\nIn code it is calculated as:\nfloat changedBlocksPercent = changedBlocks / totalBlocks\nCombining all together\nRecapping: when running the motion detection algorithm you have 3 parameters to set:\n\nthe block size\nthe block difference threshold\nthe image differerence threshold\n\nLet's pick 3 sensible defaults: block size = 10, block threshold = 15%, image threshold = 20%.\nWhat does these parameters translate to in the practice?\nThey mean that motion will be detected if 20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next.\n\nAs you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.\nReal world example\nNow the fun part. I'll show you how it performs on a real-world scenario.\nTo keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. \nThis means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\nThis is me passing my arm in front of the camera a few times.\nThe grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.\nThe orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.\nWhen I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. \nWhile moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.\nThis is why I suggest you keep the image difference threshold to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.\nDo you like the grid effect of the sample video? Let me know in the comment if you want me to share it.\nOr even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.\n\r\n\r\n\r\n    \r\n\tSTAY UP TO DATE\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n\r\nCheck the full project code on Github\nL'articolo Motion detection with ESP32 cam only (Arduino version) proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-05T12:08:08+01:00",
            "date_modified": "2020-01-05T15:06:34+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Senza categoria"
            ],
            "attachments": [
                [
                    {
                        "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4",
                        "mime_type": "video/mp4",
                        "size_in_bytes": 1673368
                    }
                ]
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/",
            "url": "https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/",
            "title": "Embedded Machine learning on Attiny85",
            "content_html": "<p>You won't believe it, but <strong>you can run Machine learning on embedded systems like an Attiny85</strong> (and many others Attiny)!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/attiny85_ml.png\" alt=\"ri-elaborated from https://cyaninfinite.com/miniaturize-projects-with-attiny85/\" /></p>\n<p><span id=\"more-688\"></span></p>\n<p>When I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of RAM and you can buy a chinese clone for around 2.50 $.</p>\n<p>It already opened the path to a embedded machine learning at a new scale, given the huge amount of microcontrollers ready to become &quot;intelligent&quot;. </p>\n<p>But it was not enough for me: after all, the <a href=\"/2019/11/how-to-create-a-classifier-for-arduino-machine-learning-projects/\">MicroML generator</a> exports plain C that should run on any embedded system, not only on Arduino boards.</p>\n<p>So I setup to test if I could go even smaller and run it on the #1 of tiny chips: the Attiny85.</p>\n<hr /><p><em>MicroML exports plain C that could run anywhere, not only on Arduino boards.</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=MicroML%20exports%20plain%20C%20that%20could%20run%20anywhere%2C%20not%20only%20on%20Arduino%20boards.&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>No, I couldn't.</p>\n<p>The generated code makes use of a variadic function, which seems to not be supported by the Attiny compiler in the Arduino IDE.</p>\n<p>So I had to come up with an alternative implementation to make it work.</p>\n<p>Fortunately I already experimented with a non-variadic version when first writing the porter, so it was a matter of refreshing that algorithm and try it out.</p>\n<p>Guess what? It compiled!</p>\n<p>So I tried porting one my earliear tutorial (the <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\">color identification</a> one) to the Attiny and...</p>\n<hr /><p><em>Boom! Machine learning on an Attiny85!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=Boom%21%20Machine%20learning%20on%20an%20Attiny85%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Here's a step-by-step tutorial on how you can do it too.</p>\n<div class=\"watchout\">\nI strongly suggest you read <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\" target=\"_blank\" rel=\"noopener noreferrer\">the original tutorial</a> before following this one, because I won't go into too much details on the common steps here.\n</div>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><li><a href=\"#tocrun-the-inference\">Run the inference</a><li><a href=\"#tocproject-figures\">Project figures</a></ol></div></p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>We're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be 3-dimensional, which leads to a really simple model with very high accuracy.</p>\n<div class=\"watchout\">\nThe Attiny85 has 8 Kb of flash and 512 bytes of RAM, so you won't be able to load any model that uses more than a few features (probably less than 10).\n</div>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<p>You must do this step on a board with a Serial interface, like an Arduino Uno / Nano / Pro Mini. See <a href=\"/2019/12/how-to-do-color-identification-on-arduino/\" target=\"_blank\" rel=\"noopener noreferrer\">the original tutorial</a> for the code of this step.</p>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\n<p>This part is exactly the same as the original, except for a single parameter: you will pass <code>platform=attiny</code> to the <code>port</code> function.</p>\n<pre><code class=\"language-python\">from sklearn.svm import SVC\nfrom micromlgen import port\n\n# put your samples in the dataset folder\n# one class per file\n# one feature vector per line, in CSV format\nfeatures, classmap = load_features(&#039;dataset/&#039;)\nX, y = features[:, :-1], features[:, -1]\nclassifier = SVC(kernel=&#039;linear&#039;).fit(X, y)\nc_code = port(classifier, classmap=classmap, platform=&#039;attiny&#039;)\nprint(c_code)</code></pre>\n<div class=\"watchout\">\nThe Attiny mode has been implemented in version 0.8 of micromlgen: if you installed an earlier version, first update\n</div>\n<p>At this point you have to copy the printed code and import it in your project, in a file called <code>model.h</code>.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">STAY UP TO DATE</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<p>Since we don't have a Serial, we will blink a LED a number of times dependant on the prediction result.</p>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\n#define LED 0\n\nvoid loop() {\n  readRGB();\n  classify();\n  delay(1000);\n}\n\nvoid classify() {\n    for (uint8_t times = predict(features) + 1; times &gt; 0; times--) {\n        digitalWrite(LED, HIGH);\n        delay(10);\n        digitalWrite(LED, LOW);\n        delay(10);\n    }\n}</code></pre>\n<p>Here we are: put some colored object in front of the sensor and see the LED blink.</p>\n<h2 id=\"tocproject-figures\">Project figures</h2>\n<p>On my machine, the sketch requires 3434 bytes (41%) of program space and 21 bytes (4%) of RAM. This means you could actually run machine learning in even less space than what the Attiny85 provides. </p>\n<p>This model in particular it's so tiny you <strong>can run in even on an Attiny45, which has only 4 Kb of flash and 256 bytes of RAM</strong>.</p>\n<p>I'd like you to look at the RAM figure for a moment: <strong>21 bytes</strong>. 21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller. This is the result of the implementation I chose: the least RAM overhead possible. I challenge you to go any lower than this.</p>\n<hr /><p><em>21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=21%20bytes%20is%20all%20the%20memory%20you%20need%20to%20run%20a%20Machine%20learning%20algorithm%20on%20a%20microcontroller&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlColorIdentificationAttinyExample/MicromlColorIdentificationAttinyExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/\">Embedded Machine learning on Attiny85</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "You won't believe it, but you can run Machine learning on embedded systems like an Attiny85 (and many others Attiny)!\n\n\nWhen I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of RAM and you can buy a chinese clone for around 2.50 $.\nIt already opened the path to a embedded machine learning at a new scale, given the huge amount of microcontrollers ready to become &quot;intelligent&quot;. \nBut it was not enough for me: after all, the MicroML generator exports plain C that should run on any embedded system, not only on Arduino boards.\nSo I setup to test if I could go even smaller and run it on the #1 of tiny chips: the Attiny85.\nMicroML exports plain C that could run anywhere, not only on Arduino boards.Click To Tweet\nNo, I couldn't.\nThe generated code makes use of a variadic function, which seems to not be supported by the Attiny compiler in the Arduino IDE.\nSo I had to come up with an alternative implementation to make it work.\nFortunately I already experimented with a non-variadic version when first writing the porter, so it was a matter of refreshing that algorithm and try it out.\nGuess what? It compiled!\nSo I tried porting one my earliear tutorial (the color identification one) to the Attiny and...\nBoom! Machine learning on an Attiny85!Click To Tweet\nHere's a step-by-step tutorial on how you can do it too.\n\nI strongly suggest you read the original tutorial before following this one, because I won't go into too much details on the common steps here.\n\nTable of contentsFeatures definitionRecord sample dataTrain and export the SVM classifierRun the inferenceProject figures\n1. Features definition\nWe're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be 3-dimensional, which leads to a really simple model with very high accuracy.\n\nThe Attiny85 has 8 Kb of flash and 512 bytes of RAM, so you won't be able to load any model that uses more than a few features (probably less than 10).\n\n2. Record sample data\nYou must do this step on a board with a Serial interface, like an Arduino Uno / Nano / Pro Mini. See the original tutorial for the code of this step.\n3. Train and export the SVM classifier\nThis part is exactly the same as the original, except for a single parameter: you will pass platform=attiny to the port function.\nfrom sklearn.svm import SVC\nfrom micromlgen import port\n\n# put your samples in the dataset folder\n# one class per file\n# one feature vector per line, in CSV format\nfeatures, classmap = load_features(&#039;dataset/&#039;)\nX, y = features[:, :-1], features[:, -1]\nclassifier = SVC(kernel=&#039;linear&#039;).fit(X, y)\nc_code = port(classifier, classmap=classmap, platform=&#039;attiny&#039;)\nprint(c_code)\n\nThe Attiny mode has been implemented in version 0.8 of micromlgen: if you installed an earlier version, first update\n\nAt this point you have to copy the printed code and import it in your project, in a file called model.h.\n\r\n\r\n\r\n    \r\n\tSTAY UP TO DATE\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n4. Run the inference\nSince we don't have a Serial, we will blink a LED a number of times dependant on the prediction result.\n#include &quot;model.h&quot;\n\n#define LED 0\n\nvoid loop() {\n  readRGB();\n  classify();\n  delay(1000);\n}\n\nvoid classify() {\n    for (uint8_t times = predict(features) + 1; times &gt; 0; times--) {\n        digitalWrite(LED, HIGH);\n        delay(10);\n        digitalWrite(LED, LOW);\n        delay(10);\n    }\n}\nHere we are: put some colored object in front of the sensor and see the LED blink.\nProject figures\nOn my machine, the sketch requires 3434 bytes (41%) of program space and 21 bytes (4%) of RAM. This means you could actually run machine learning in even less space than what the Attiny85 provides. \nThis model in particular it's so tiny you can run in even on an Attiny45, which has only 4 Kb of flash and 256 bytes of RAM.\nI'd like you to look at the RAM figure for a moment: 21 bytes. 21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller. This is the result of the implementation I chose: the least RAM overhead possible. I challenge you to go any lower than this.\n21 bytes is all the memory you need to run a Machine learning algorithm on a microcontrollerClick To Tweet\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo Embedded Machine learning on Attiny85 proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-23T17:57:15+01:00",
            "date_modified": "2019-12-29T19:33:17+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/",
            "url": "https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/",
            "title": "Word classification using Arduino and MicroML",
            "content_html": "<p>In this Arduno Machine learning tutorial we're going to use a microphone to identify the word you speak.<br />\nThis is going to run on an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg\" alt=\"from https://www.udemy.com/course/learn-audio-processing-complete-engineers-course/\" /></p>\n<p><span id=\"more-180\"></span></p>\n<p>In this project the features are going to be the Fast Fourier Transform of 50 analog readings from a microphone, taken starting from when a loud sound is detected, sampled at intervals of 5 millis.</p>\n<div class=\"watchout\">\nThis tutorial is not about \"Wake word\" detection: it can't distinguish a known word from any other word. It can classify the word you speak among the ones you trained it to recognize!!!\n</div>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><ol><li><a href=\"#toctranslate-the-raw-values\">Translate the raw values</a><li><a href=\"#tocdetect-sound\">Detect sound</a><li><a href=\"#tocrecord-the-words\">Record the words</a><li><a href=\"#tocfast-fourier-transform\">Fast Fourier Transform</a></li></ol><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><ol><li><a href=\"#tocselect-a-suitable-model\">Select a suitable model</a></li></ol><li><a href=\"#tocrun-the-inference\">Run the inference</a></ol></div></p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>The microphone we're going to use is a super simple device: it produces an analog signal (0-1024) based on the sound it detects. </p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord-Sensor.jpg\" alt=\"from http://arduinolearning.com/code/ky038-microphone-module-and-arduino-example.php\" /></p>\n<p>When working with audio you almost always don't want to use raw readings, since they're hardly useful. Instead you often go with <a href=\"https://en.wikipedia.org/wiki/Fourier_transform\">Fourier Transform</a>, which extracts the frequency information from a time signal. That's going to become our features vector: let's see how in the next step.</p>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<p>First of all, we start with raw audio data. The following plot is me saying random words.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Raw-audio.gif\" alt=\"Raw audio stream\" /></p>\n<pre><code class=\"language-cpp\">#define MIC A0\n#define INTERVAL 5\n\nvoid setup() {\n    Serial.begin(115200);\n    pinMode(MIC, INPUT);\n}\n\nvoid loop() {\n    Serial.println(analogRead(MIC));\n    delay(INTERVAL);\n}</code></pre>\n<h3 id=\"toctranslate-the-raw-values\">2.1 Translate the raw values</h3>\n<p>For the Fourier Transform to work, we need to provide as input an array of values both positive and negative. <code>analogRead()</code> is returning only positive values, tough, so we need to translate them.</p>\n<pre><code class=\"language-cpp\">int16_t readMic() {\n    // this translated the analog value to a proper interval\n    return  (analogRead(MIC) - 512) &gt;&gt; 2;\n}</code></pre>\n<h3 id=\"tocdetect-sound\">2.2 Detect sound</h3>\n<p>As in the <a href=\"/2019/12/how-to-do-gesture-identification-on-arduino/\">tutorial about gesture classification</a>, we'll start recording the features when a word is beginning to be pronounced. Also in this project we'll use a threshold to detect the start of a word.</p>\n<p>To do this, we first record a &quot;background&quot; sound level, that is the value produced by the sensor when we're not talking at all.</p>\n<pre><code class=\"language-cpp\">float backgroundSound = 0;\n\nvoid setup() {\n    Serial.begin(115200);\n    pinMode(MIC, INPUT);\n    calibrate();\n}\n\nvoid calibrate() {\n    for (int i = 0; i &lt; 200; i++)\n        backgroundSound += readMic();\n\n    backgroundSound /= 200;\n\n    Serial.print(&quot;Background sound level is &quot;);\n    Serial.println(backgroundSound);\n}</code></pre>\n<p>At this point we can check for the starting of a word when the detected sound level exceeds tha background one by a given threshold.</p>\n<pre><code class=\"language-cpp\">// adjust as per your need\n// it will depend on the sensitivity of you microphone\n#define SOUND_THRESHOLD 3\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n}\n\nbool soundDetected() {\n    return abs(read() - backgroundSound) &gt;= SOUND_THRESHOLD;\n}</code></pre>\n<h3 id=\"tocrecord-the-words\">2.3 Record the words</h3>\n<p>As for the gestures, we'll record a fixed number of readings at a fixed interval.<br />\nHere a tradeoff arises: you want to have a decent number of readings to be able to accurately describe the words you want to classify, but not too much otherwise your model is going to be too large to fit in your board.</p>\n<p>I made some experiments, and I got good results with 32 samples at 5 millis interval, which covers ~150 ms of speech.</p>\n<div class=\"watchout\">\nThe dilemma here is that the Fourier Transform to work needs a number of samples that is a power of 2. So, if you think 32 features are not enough for you, you're forced to go with at least 64: this has a REALLY bad impact on the model size.\n</div>\n<pre><code class=\"language-cpp\">#define NUM_SAMPLES 32\n#define INTERVAL 5\n\ndouble features[NUM_SAMPLES];\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n\n    captureWord();\n    printFeatures();\n    delay(1000);\n}\n\nvoid captureWord() {\n    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {\n        features[i] = readMic();\n        delay(INTERVAL);\n    }\n}</code></pre>\n<pre><code class=\"language-cpp\">\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n</code></pre>\n<h3 id=\"tocfast-fourier-transform\">2.4 Fast Fourier Transform</h3>\n<p>Here we are with the Fourier Transform. When implemented in software, the most widely implementation of the FT is actually called <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\">Fast Fourier Transform (FFT)</a>, which is - as you may guess - a fast implementation of the FT. </p>\n<p>Luckily for us, there exists <a href=\"https://github.com/kosme/arduinoFFT\">a library</a> for Arduino that does FFT.</p>\n<p>And is so easy to use that we only need a line to get usable results!</p>\n<pre><code class=\"language-cpp\">#include &lt;arduinoFFT.h&gt;\n\narduinoFFT fft;\n\nvoid captureWord() {\n    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {\n        features[i] = readMic();\n        delay(INTERVAL);\n    }\n\n    fft.Windowing(features, NUM_SAMPLES, FFT_WIN_TYP_HAMMING, FFT_FORWARD);\n}</code></pre>\n<p>You don't need to know what the <code>Windowing</code> function actually does (I don't either): what matters is that it extracts meaningful informations from our signal. Since it overwrites the features array, after calling that line we have what we need to input to our classifier.</p>\n<p>At this point, record 10-15 samples for each word and save them to a file, one for each word.</p>\n<div class=\"watchout\">\nAfter you have recorded the samples for a word, I suggest you to manually check them. It is sufficient to look at the first 3 values: if one of them seems to be clearly out of range, I suggest you to delete it. You will lose some accuracy, but your model will be smaller.\n</div>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<p>In this project on Machine learning we're not achieving 100% accuracy easily.<br />\nAudio is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-linear-kernel-87-accuracy.png\" alt=\"Decision boundaries of 2 PCA components of Word classification, linear kernel\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-poly-3-kernel-91-accuracy.png\" alt=\"Decision boundaries of 2 PCA components of Word classification, poly-3 kernel\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-rbf-kernel-94-accuracy.png\" alt=\"Decision boundaries of 2 PCA components of Word classification, rbf kernel\" /></p>\n<h3 id=\"tocselect-a-suitable-model\">2.5 Select a suitable model</h3>\n<p>Here's an overview table of the 3 tests I did.</p>\n<table>\n<thead>\n<tr>\n<th>Kernel</th>\n<th style=\"text-align: center;\">No. support vectors</th>\n<th style=\"text-align: center;\">Avg. accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Linear</td>\n<td style=\"text-align: center;\">22</td>\n<td style=\"text-align: center;\">87%</td>\n</tr>\n<tr>\n<td>Poly 3</td>\n<td style=\"text-align: center;\">29</td>\n<td style=\"text-align: center;\">91%</td>\n</tr>\n<tr>\n<td>RBF</td>\n<td style=\"text-align: center;\">36</td>\n<td style=\"text-align: center;\">94%</td>\n</tr>\n</tbody>\n</table>\n<p>Of course the one with the RBF kernel would be the most desiderable since it has a very high accuracy: 36 support vectors, tough, will produce a model too large to fit on an Arduino Nano.</p>\n<p>So you're forced to pick the one with the highest accuracy that fit on your board: in my case it was the Linear kernel one.</p>\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n\n    captureWord();\n    Serial.print(&quot;You said &quot;);\n    Serial.println(classIdxToName(predict(features)));\n\n    delay(1000);\n}</code></pre>\n<p>And that's it: word classification through machine learning on your Arduino board! Say some word and see the classification result on the Serial monitor. </p>\n<p>Here's me testing the system (English is not my language, so forgive my bad pronounce). The video quality is very low, I know, but you get the point.</p>\n<div style=\"width: 640px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-180-3\" width=\"640\" height=\"352\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4?_=3\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4</a></video></div>\n<div class=\"watchout\">\nAs you can hear from the video, you should be quite accurate when pronouncing the words. I have to admit there are cases where the system totally fails to classify correctly the words. Restarting helps most of the time, so I'm suspecting there could be some kind of leak that \"corrupts\" the inference procedure.\n</div>\n<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlWakeWordIdentificationExample/MicromlWakeWordIdentificationExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/\">Word classification using Arduino and MicroML</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this Arduno Machine learning tutorial we're going to use a microphone to identify the word you speak.\nThis is going to run on an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.\n\n\nIn this project the features are going to be the Fast Fourier Transform of 50 analog readings from a microphone, taken starting from when a loud sound is detected, sampled at intervals of 5 millis.\n\nThis tutorial is not about \"Wake word\" detection: it can't distinguish a known word from any other word. It can classify the word you speak among the ones you trained it to recognize!!!\n\nTable of contentsFeatures definitionRecord sample dataTranslate the raw valuesDetect soundRecord the wordsFast Fourier TransformTrain and export the SVM classifierSelect a suitable modelRun the inference\n1. Features definition\nThe microphone we're going to use is a super simple device: it produces an analog signal (0-1024) based on the sound it detects. \n\nWhen working with audio you almost always don't want to use raw readings, since they're hardly useful. Instead you often go with Fourier Transform, which extracts the frequency information from a time signal. That's going to become our features vector: let's see how in the next step.\n2. Record sample data\nFirst of all, we start with raw audio data. The following plot is me saying random words.\n\n#define MIC A0\n#define INTERVAL 5\n\nvoid setup() {\n    Serial.begin(115200);\n    pinMode(MIC, INPUT);\n}\n\nvoid loop() {\n    Serial.println(analogRead(MIC));\n    delay(INTERVAL);\n}\n2.1 Translate the raw values\nFor the Fourier Transform to work, we need to provide as input an array of values both positive and negative. analogRead() is returning only positive values, tough, so we need to translate them.\nint16_t readMic() {\n    // this translated the analog value to a proper interval\n    return  (analogRead(MIC) - 512) &gt;&gt; 2;\n}\n2.2 Detect sound\nAs in the tutorial about gesture classification, we'll start recording the features when a word is beginning to be pronounced. Also in this project we'll use a threshold to detect the start of a word.\nTo do this, we first record a &quot;background&quot; sound level, that is the value produced by the sensor when we're not talking at all.\nfloat backgroundSound = 0;\n\nvoid setup() {\n    Serial.begin(115200);\n    pinMode(MIC, INPUT);\n    calibrate();\n}\n\nvoid calibrate() {\n    for (int i = 0; i &lt; 200; i++)\n        backgroundSound += readMic();\n\n    backgroundSound /= 200;\n\n    Serial.print(&quot;Background sound level is &quot;);\n    Serial.println(backgroundSound);\n}\nAt this point we can check for the starting of a word when the detected sound level exceeds tha background one by a given threshold.\n// adjust as per your need\n// it will depend on the sensitivity of you microphone\n#define SOUND_THRESHOLD 3\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n}\n\nbool soundDetected() {\n    return abs(read() - backgroundSound) &gt;= SOUND_THRESHOLD;\n}\n2.3 Record the words\nAs for the gestures, we'll record a fixed number of readings at a fixed interval.\nHere a tradeoff arises: you want to have a decent number of readings to be able to accurately describe the words you want to classify, but not too much otherwise your model is going to be too large to fit in your board.\nI made some experiments, and I got good results with 32 samples at 5 millis interval, which covers ~150 ms of speech.\n\nThe dilemma here is that the Fourier Transform to work needs a number of samples that is a power of 2. So, if you think 32 features are not enough for you, you're forced to go with at least 64: this has a REALLY bad impact on the model size.\n\n#define NUM_SAMPLES 32\n#define INTERVAL 5\n\ndouble features[NUM_SAMPLES];\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n\n    captureWord();\n    printFeatures();\n    delay(1000);\n}\n\nvoid captureWord() {\n    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {\n        features[i] = readMic();\n        delay(INTERVAL);\n    }\n}\n\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n\n2.4 Fast Fourier Transform\nHere we are with the Fourier Transform. When implemented in software, the most widely implementation of the FT is actually called Fast Fourier Transform (FFT), which is - as you may guess - a fast implementation of the FT. \nLuckily for us, there exists a library for Arduino that does FFT.\nAnd is so easy to use that we only need a line to get usable results!\n#include &lt;arduinoFFT.h&gt;\n\narduinoFFT fft;\n\nvoid captureWord() {\n    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {\n        features[i] = readMic();\n        delay(INTERVAL);\n    }\n\n    fft.Windowing(features, NUM_SAMPLES, FFT_WIN_TYP_HAMMING, FFT_FORWARD);\n}\nYou don't need to know what the Windowing function actually does (I don't either): what matters is that it extracts meaningful informations from our signal. Since it overwrites the features array, after calling that line we have what we need to input to our classifier.\nAt this point, record 10-15 samples for each word and save them to a file, one for each word.\n\nAfter you have recorded the samples for a word, I suggest you to manually check them. It is sufficient to look at the first 3 values: if one of them seems to be clearly out of range, I suggest you to delete it. You will lose some accuracy, but your model will be smaller.\n\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\nIn this project on Machine learning we're not achieving 100% accuracy easily.\nAudio is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:\n\n\n\n2.5 Select a suitable model\nHere's an overview table of the 3 tests I did.\n\n\n\nKernel\nNo. support vectors\nAvg. accuracy\n\n\n\n\nLinear\n22\n87%\n\n\nPoly 3\n29\n91%\n\n\nRBF\n36\n94%\n\n\n\nOf course the one with the RBF kernel would be the most desiderable since it has a very high accuracy: 36 support vectors, tough, will produce a model too large to fit on an Arduino Nano.\nSo you're forced to pick the one with the highest accuracy that fit on your board: in my case it was the Linear kernel one.\n4. Run the inference\n#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!soundDetected()) {\n        delay(10);\n        return;\n    }\n\n    captureWord();\n    Serial.print(&quot;You said &quot;);\n    Serial.println(classIdxToName(predict(features)));\n\n    delay(1000);\n}\nAnd that's it: word classification through machine learning on your Arduino board! Say some word and see the classification result on the Serial monitor. \nHere's me testing the system (English is not my language, so forgive my bad pronounce). The video quality is very low, I know, but you get the point.\nhttps://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4\n\nAs you can hear from the video, you should be quite accurate when pronouncing the words. I have to admit there are cases where the system totally fails to classify correctly the words. Restarting helps most of the time, so I'm suspecting there could be some kind of leak that \"corrupts\" the inference procedure.\n\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo Word classification using Arduino and MicroML proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-22T19:12:59+01:00",
            "date_modified": "2019-12-29T19:10:35+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "Arduino Machine learning"
            ],
            "attachments": [
                [
                    {
                        "url": "https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4",
                        "mime_type": "video/mp4",
                        "size_in_bytes": 2055653
                    }
                ]
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/",
            "url": "https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/",
            "title": "Indoor positioning using Arduino and Machine Learning in 4 steps",
            "content_html": "<p>In this Arduno Machine learning project we're going to use the nearby WiFi access points to locate where we are. For this project to work you will need a Wifi equipped board, such as ESP8266 or ESP32.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/illustrations_ambient-wifi-site-survey2.jpg\" alt=\"Wifi indoor positioning @ ri-elaborated from https://www.accuware.com/blog/ambient-signals-plus-video-images/\" /></p>\n<p><span id=\"more-224\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-is-indoor-positioning\">What is indoor positioning?</a><ol><li><a href=\"#tocwhat-will-we-use-it-for\">What will we use it for?</a><li><a href=\"#tocwhat-you-need\">What you need</a><li><a href=\"#tochow-it-works\">How it works</a></li></ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><ol><li><a href=\"#tocenumerate-the-access-points\">Enumerate the access points</a><li><a href=\"#toccreate-an-access-point-array\">Create an access point array</a><li><a href=\"#tocconvert-to-features-vector\">Convert to features vector</a></li></ol><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><li><a href=\"#tocrun-the-inference\">Run the inference</a></ol></div></p>\n<h2 id=\"tocwhat-is-indoor-positioning\">What is indoor positioning?</h2>\n<p>We all are used to GPS positioning: our device will use satellites to track our position on Earth. GPS works very well and with a very high accuracy (you can expect only a few meters of error).</p>\n<p>But it suffers a problem: it needs <em>Line of Sight</em> (a clear path from your device to the satellites). If you're not in an open place, like inside a building, you're out of luck.</p>\n<p>The task of detecting where you are when GPS localization is not an option is called <a href=\"https://en.wikipedia.org/wiki/Indoor_positioning_system\">indoor positioning</a>: it could be in a building, an airport, a parking garage. </p>\n<p>There are lots of different approaches to this task (Wikipedia lists more than 10 of them), each with a varying level of commitment, difficulty, cost and accuracy.</p>\n<p>For this tutorial I opted for one that is both cost-efficient, easy to implement and readily available in most of the locations: <strong>WiFi indoor positioning</strong>.</p>\n<h3 id=\"tocwhat-will-we-use-it-for\">What will we use it for?</h3>\n<p>In this tutorial about Machine learning on Arduino we're going to use Wifi indoor positioning to detect in which room of our house / office we are. This is the most basic task we can accomplish and will get us a feeling of level of accuracy we can achieve with such a simple setup.</p>\n<p>On this basis, we'll construct more sophisticated projects in future posts.</p>\n<h3 id=\"tocwhat-you-need\">What you need</h3>\n<p>To accomplish this tutorial, you really need 2 things:</p>\n<ol>\n<li>a WiFi equipped board (ESP8266, ESP32, Arduino MKR WiFi 1010...)</li>\n<li>be in a place with a few WiFi networks around</li>\n</ol>\n<p>If you're doing this at home or in your office, there's a good change your neighbours have WiFi networks in their apartments you can leverage. If you live in an isolated contryside, sorry, this will not work for you.</p>\n<h3 id=\"tochow-it-works\">How it works</h3>\n<p>So, how exactly does Wifi indoor positioning works in conjuction with Machine learning? </p>\n<p>Let's pretend there are 5 different WiFi networks around you, like in the picture below.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Wifi1.png\" alt=\"Wifi indoor positioning example\" /></p>\n<p>As you can see there are two markers on the map: each of these markers will &quot;see&quot; different networks, with different <em>signal strengths</em> (a.k.a <a href=\"https://en.wikipedia.org/wiki/Received_signal_strength_indication\">RSSI</a>).</p>\n<p>As you move around, those numbers will change: each room will be identified by the unique combination of the RSSIs.</p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>The features for this project are going to be the RSSIs (Received signal strength indication) of the known WiFi networks. If a network is out of range, it will have an RSSI equal to 0.</p>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<p>Before actually recording the sample data to train our classifier, we need to do some preliminary work. This is because not all networks will be visible all the time:  we have to work, however, with a fixed number of features.</p>\n<h3 id=\"tocenumerate-the-access-points\">2.1 Enumerate the access points</h3>\n<p>First of all we need to enumerate all the networks we will encounter during the inference process. </p>\n<p>To begin, we take a &quot;reconnaissance tour&quot; of the locations we want to predict and log all the networks we detect. Load the following sketch and take note of all the networks that appear on the Serial monitor.</p>\n<pre><code class=\"language-cpp\">#include &lt;WiFi.h&gt;\n\nvoid setup() {\n    Serial.begin(115200);\n    WiFi.mode(WIFI_STA);\n    WiFi.disconnect();\n}\n\nvoid loop() {\n  int numNetworks = WiFi.scanNetworks();\n\n  for (int i = 0; i &lt; numNetworks; i++) {\n      Serial.println(WiFi.SSID(i));\n\n  delay(3000);\n}</code></pre>\n<h3 id=\"toccreate-an-access-point-array\">2.2 Create an access point array</h3>\n<p>Now that we have a bunch of SSIDs, we need to assign each SSID to a fixed index, from 0 to <code>MAX_NETWORKS</code>.</p>\n<p>You can implement this part as you like, but in this demo I'll make use of a class I wrote called <code>Array</code> (you can see the <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/src/data_structures/Array.h\">source code</a> and <a href=\"https://github.com/agrimagsrl/eloquentarduino/blob/master/examples/ArrayExample/ArrayExample.ino\">example</a> on Github), which implements 2 useful functions: </p>\n<ol>\n<li><code>push()</code> to add an element to the array</li>\n<li><code>indexOf()</code> to get the index of an element.</li>\n</ol>\n<p>See <a href=\"/2019/12/how-to-install-the-eloquent-library/\">how to install the Eloquent library</a> if you don't have it already installed.<br />\nAt this point we populate the array with all the networks we saved from the reconnaissance tour.</p>\n<pre><code class=\"language-cpp\">#include &lt;eDataStructures.h&gt;\n\n#define MAX_NETWORKS 10\n\nusing namespace Eloquent::DataStructures;\n\ndouble features[MAX_NETWORKS];\nArray&lt;String, MAX_NETWORKS&gt; knownNetworks(&quot;&quot;);\n\nvoid setup() {\n    Serial.begin(115200);\n    WiFi.mode(WIFI_STA);\n    WiFi.disconnect();\n\n    knownNetworks.push(&quot;SSID #0&quot;);\n    knownNetworks.push(&quot;SSID #1&quot;);\n    knownNetworks.push(&quot;SSID #2&quot;);\n    knownNetworks.push(&quot;SSID #3&quot;);\n    // and so on\n}</code></pre>\n<h3 id=\"tocconvert-to-features-vector\">2.3 Convert to features vector</h3>\n<p>The second step is to convert the scan results into a features vector. Each feature will be the RSSI of the given SSID, in the exact order we populated the <code>knownNetworks</code> array.</p>\n<p>In practice:</p>\n<pre><code class=\"language-cpp\">features[0] == RSSI of SSID #0;\nfeatures[1] == RSSI of SSID #1;\nfeatures[2] == RSSI of SSID #2;\nfeatures[3] == RSSI of SSID #3;\n// and so on</code></pre>\n<p>The code below will do the job.</p>\n<pre><code class=\"language-cpp\">void loop() {\n    scan();\n    printFeatures();\n    delay(3000);\n}\n\nvoid scan() {\n    int numNetworks = WiFi.scanNetworks();\n\n    resetFeatures();\n\n    // assign RSSIs to feature vector\n    for (int i = 0; i &lt; numNetworks; i++) {\n        String ssid = WiFi.SSID(i);\n        uint16_t networkIndex = knownNetworks.indexOf(ssid);\n\n        // only create feature if the current SSID is a known one\n        if (!isnan(networkIndex))\n            features[networkIndex] = WiFi.RSSI(i);\n    }\n}\n\n// reset all features to 0\nvoid resetFeatures() {\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\n\n    for (int i = 0; i &lt; numFeatues; i++)\n        features[i] = 0;\n}</code></pre>\n<pre><code class=\"language-cpp\">\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n</code></pre>\n<p>Grab some recordings just staying in a location for a few seconds and save the serial output to a file; then move to the next location and repeat: 10-15 samples for each location will suffice.</p>\n<p>If you do a good job, you should end with distinguible features, as show in the plot below.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-from-Wifi-indoor-positioning-features.png\" alt=\"Decision boundaries of 2 PCA components from Wifi indoor positioning features\" /></p>\n<div class=\"watchout\">\nRSSIs may be a little noisy, mostly on the boundaries where weak networks may appear and disappear with a very low RSSI: this was not a problem for me, but if you're getting bad results you may filter out those low values.</p>\n<pre><code class=\"language-cpp\">\n// replace\nfeatures[networkIndex] = WiFi.RSSI(i);\n\n// with\n#define MIN_RSSI -90 // adjust to your needs\n\nfeatures[networkIndex] = WiFi.RSSI(i) > MIN_RSSI ? WiFi.RSSI(i) : 0;\n</code></pre>\n</div>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\nvoid loop() {\n    scan();\n    classify();\n    delay(3000);\n}\n\nvoid classify() {\n    Serial.print(&quot;You are in &quot;);\n    Serial.println(classIdxToName(predict(features)));\n}</code></pre>\n<p>Move around your house/office/whatever and see your location printed on the serial monitor!</p>\n<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlWifiIndoorPositioningExample/MicromlWifiIndoorPositioningExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/\">Indoor positioning using Arduino and Machine Learning in 4 steps</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this Arduno Machine learning project we're going to use the nearby WiFi access points to locate where we are. For this project to work you will need a Wifi equipped board, such as ESP8266 or ESP32.\n\n\nTable of contentsWhat is indoor positioning?What will we use it for?What you needHow it worksFeatures definitionRecord sample dataEnumerate the access pointsCreate an access point arrayConvert to features vectorTrain and export the SVM classifierRun the inference\nWhat is indoor positioning?\nWe all are used to GPS positioning: our device will use satellites to track our position on Earth. GPS works very well and with a very high accuracy (you can expect only a few meters of error).\nBut it suffers a problem: it needs Line of Sight (a clear path from your device to the satellites). If you're not in an open place, like inside a building, you're out of luck.\nThe task of detecting where you are when GPS localization is not an option is called indoor positioning: it could be in a building, an airport, a parking garage. \nThere are lots of different approaches to this task (Wikipedia lists more than 10 of them), each with a varying level of commitment, difficulty, cost and accuracy.\nFor this tutorial I opted for one that is both cost-efficient, easy to implement and readily available in most of the locations: WiFi indoor positioning.\nWhat will we use it for?\nIn this tutorial about Machine learning on Arduino we're going to use Wifi indoor positioning to detect in which room of our house / office we are. This is the most basic task we can accomplish and will get us a feeling of level of accuracy we can achieve with such a simple setup.\nOn this basis, we'll construct more sophisticated projects in future posts.\nWhat you need\nTo accomplish this tutorial, you really need 2 things:\n\na WiFi equipped board (ESP8266, ESP32, Arduino MKR WiFi 1010...)\nbe in a place with a few WiFi networks around\n\nIf you're doing this at home or in your office, there's a good change your neighbours have WiFi networks in their apartments you can leverage. If you live in an isolated contryside, sorry, this will not work for you.\nHow it works\nSo, how exactly does Wifi indoor positioning works in conjuction with Machine learning? \nLet's pretend there are 5 different WiFi networks around you, like in the picture below.\n\nAs you can see there are two markers on the map: each of these markers will &quot;see&quot; different networks, with different signal strengths (a.k.a RSSI).\nAs you move around, those numbers will change: each room will be identified by the unique combination of the RSSIs.\n1. Features definition\nThe features for this project are going to be the RSSIs (Received signal strength indication) of the known WiFi networks. If a network is out of range, it will have an RSSI equal to 0.\n2. Record sample data\nBefore actually recording the sample data to train our classifier, we need to do some preliminary work. This is because not all networks will be visible all the time:  we have to work, however, with a fixed number of features.\n2.1 Enumerate the access points\nFirst of all we need to enumerate all the networks we will encounter during the inference process. \nTo begin, we take a &quot;reconnaissance tour&quot; of the locations we want to predict and log all the networks we detect. Load the following sketch and take note of all the networks that appear on the Serial monitor.\n#include &lt;WiFi.h&gt;\n\nvoid setup() {\n    Serial.begin(115200);\n    WiFi.mode(WIFI_STA);\n    WiFi.disconnect();\n}\n\nvoid loop() {\n  int numNetworks = WiFi.scanNetworks();\n\n  for (int i = 0; i &lt; numNetworks; i++) {\n      Serial.println(WiFi.SSID(i));\n\n  delay(3000);\n}\n2.2 Create an access point array\nNow that we have a bunch of SSIDs, we need to assign each SSID to a fixed index, from 0 to MAX_NETWORKS.\nYou can implement this part as you like, but in this demo I'll make use of a class I wrote called Array (you can see the source code and example on Github), which implements 2 useful functions: \n\npush() to add an element to the array\nindexOf() to get the index of an element.\n\nSee how to install the Eloquent library if you don't have it already installed.\nAt this point we populate the array with all the networks we saved from the reconnaissance tour.\n#include &lt;eDataStructures.h&gt;\n\n#define MAX_NETWORKS 10\n\nusing namespace Eloquent::DataStructures;\n\ndouble features[MAX_NETWORKS];\nArray&lt;String, MAX_NETWORKS&gt; knownNetworks(&quot;&quot;);\n\nvoid setup() {\n    Serial.begin(115200);\n    WiFi.mode(WIFI_STA);\n    WiFi.disconnect();\n\n    knownNetworks.push(&quot;SSID #0&quot;);\n    knownNetworks.push(&quot;SSID #1&quot;);\n    knownNetworks.push(&quot;SSID #2&quot;);\n    knownNetworks.push(&quot;SSID #3&quot;);\n    // and so on\n}\n2.3 Convert to features vector\nThe second step is to convert the scan results into a features vector. Each feature will be the RSSI of the given SSID, in the exact order we populated the knownNetworks array.\nIn practice:\nfeatures[0] == RSSI of SSID #0;\nfeatures[1] == RSSI of SSID #1;\nfeatures[2] == RSSI of SSID #2;\nfeatures[3] == RSSI of SSID #3;\n// and so on\nThe code below will do the job.\nvoid loop() {\n    scan();\n    printFeatures();\n    delay(3000);\n}\n\nvoid scan() {\n    int numNetworks = WiFi.scanNetworks();\n\n    resetFeatures();\n\n    // assign RSSIs to feature vector\n    for (int i = 0; i &lt; numNetworks; i++) {\n        String ssid = WiFi.SSID(i);\n        uint16_t networkIndex = knownNetworks.indexOf(ssid);\n\n        // only create feature if the current SSID is a known one\n        if (!isnan(networkIndex))\n            features[networkIndex] = WiFi.RSSI(i);\n    }\n}\n\n// reset all features to 0\nvoid resetFeatures() {\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\n\n    for (int i = 0; i &lt; numFeatues; i++)\n        features[i] = 0;\n}\n\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n\nGrab some recordings just staying in a location for a few seconds and save the serial output to a file; then move to the next location and repeat: 10-15 samples for each location will suffice.\nIf you do a good job, you should end with distinguible features, as show in the plot below.\n\n\nRSSIs may be a little noisy, mostly on the boundaries where weak networks may appear and disappear with a very low RSSI: this was not a problem for me, but if you're getting bad results you may filter out those low values.\n\n// replace\nfeatures[networkIndex] = WiFi.RSSI(i);\n\n// with\n#define MIN_RSSI -90 // adjust to your needs\n\nfeatures[networkIndex] = WiFi.RSSI(i) > MIN_RSSI ? WiFi.RSSI(i) : 0;\n\n\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\n4. Run the inference\n#include &quot;model.h&quot;\n\nvoid loop() {\n    scan();\n    classify();\n    delay(3000);\n}\n\nvoid classify() {\n    Serial.print(&quot;You are in &quot;);\n    Serial.println(classIdxToName(predict(features)));\n}\nMove around your house/office/whatever and see your location printed on the serial monitor!\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo Indoor positioning using Arduino and Machine Learning in 4 steps proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-20T18:31:16+01:00",
            "date_modified": "2020-01-01T12:44:14+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/",
            "url": "https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/",
            "title": "How to do Gesture identification through machine learning on Arduino",
            "content_html": "<p>In this Arduno Machine learning project we're going to use an accelerometer sensor to identify the gestures you play.<br />\nThis is a remake of the project found on the <a href=\"https://blog.tensorflow.org/2019/11/how-to-get-started-with-machine.html\">Tensorflow blog</a>. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.001-gamma.png\" alt=\"Decision boundaries, 99% accuracy\" /></p>\n<p><span id=\"more-35\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><ol><li><a href=\"#tocread-the-imu-sensor\">Read the IMU sensor</a><li><a href=\"#toccalibration\">Calibration</a><li><a href=\"#tocdetect-first-motion\">Detect first motion</a><li><a href=\"#tocrecord-features\">Record features</a></li></ol><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><ol><li><a href=\"#tocselect-a-suitable-model\">Select a suitable model</a></li></ol><li><a href=\"#tocrun-the-inference\">Run the inference</a></ol></div></p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>We're going to use the accelerations along the 3 axis (X, Y, Z) coming from an <a href=\"https://en.wikipedia.org/wiki/Inertial_measurement_unit\">IMU</a> to infer which gesture we're playing. We'll use a fixed number of recordings (<code>NUM_SAMPLES</code>) starting from the first detection of movement. </p>\n<p>This means our feature vectors are going to be of dimension <code>3 * NUM_SAMPLES</code>, which can become too large to fit in the memory of the Arduino Nano. We'll start with a low value for <code>NUM_SAMPLES</code> to keep it as leaner as possible: if your classifications suffer from poor accuracy, you can increase this number.</p>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<h3 id=\"tocread-the-imu-sensor\">2.1 Read the IMU sensor</h3>\n<p>First of all, we need to read the raw data from the IMU. This piece of code will be different based on the specific chip you use. To keep things consistent, we'll wrap the IMU logic in 2 functions: <code>imu_setup</code> and <code>imu_read</code>. </p>\n<p>I'll report a couple of example implementations for the <code>MPU6050</code> and the <code>MPU9250</code> (these are the chip I have at hand). You should save whichever code you use in a file called <code>imu.h</code>. </p>\n<pre><code class=\"language-cpp\">#include &quot;Wire.h&quot;\n// library from https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050\n#include &quot;MPU6050.h&quot;\n#define OUTPUT_READABLE_ACCELGYRO\n\nMPU6050 imu;\n\nvoid imu_setup() {\n    Wire.begin();\n    imu.initialize();\n}\n\nvoid imu_read(float *ax, float *ay, float *az) {\n    float gx, gy, gz;\n\n    imu.getMotion6(&amp;ax, &amp;ay, &amp;az, &amp;gx, &amp;gy, &amp;gz);\n}</code></pre>\n<pre><code class=\"language-cpp\">#include &quot;Wire.h&quot;\n// library from https://github.com/bolderflight/MPU9250\n#include &quot;MPU9250.h&quot;\n\nMPU9250 imu(Wire, 0x68);\n\nvoid imu_setup() {\n    Wire.begin();\n    imu.begin();\n}\n\nvoid imu_read(float *ax, float *ay, float *az) {\n    imu.readSensor();\n\n    *ax = imu.getAccelX_mss();\n    *ay = imu.getAccelY_mss();\n    *az = imu.getAccelZ_mss();\n}</code></pre>\n<p>In the main .ino file, we dump the values to Serial monitor / plotter.</p>\n<pre><code class=\"language-cpp\">#include &quot;imu.h&quot;\n\n#define NUM_SAMPLES 30\n#define NUM_AXES 3\n// sometimes you may get &quot;spikes&quot; in the readings\n// set a sensible value to truncate too large values\n#define TRUNCATE_AT 20\n\ndouble features[NUM_SAMPLES * NUM_AXES];\n\nvoid setup() {\n    Serial.begin(115200);\n    imu_setup();\n}\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax, -TRUNCATE_AT, TRUNCATE_AT);\n    ay = constrain(ay, -TRUNCATE_AT, TRUNCATE_AT);\n    az = constrain(az, -TRUNCATE_AT, TRUNCATE_AT);\n\n    Serial.print(ax);\n    Serial.print(&#039;\\t&#039;);\n    Serial.print(ay);\n    Serial.print(&#039;\\t&#039;);\n    Serial.println(az);\n}</code></pre>\n<p>Open the Serial plotter and make some movement to have an idea of the range of your readings.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Raw-gestures.gif&quot;\" alt=\"Raw IMU readings for the gestures identification project\" /></p>\n<h3 id=\"toccalibration\">2.2 Calibration</h3>\n<p>Due to gravity, we get a stable value of -9.8 on the Z axis at rest (you can see this in the previous image). Since I'd like to have almost 0 at rest, I created a super simple calibration procedure to remove this fixed offset from the readings.</p>\n<pre><code class=\"language-cpp\">double baseline[NUM_AXES];\ndouble features[NUM_SAMPLES * NUM_AXES];\n\nvoid setup() {\n    Serial.begin(115200);\n    imu_setup();\n    calibrate();\n}\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n}\n\nvoid calibrate() {\n    float ax, ay, az;\n\n    for (int i = 0; i &lt; 10; i++) {\n        imu_read(&amp;ax, &amp;ay, &amp;az);\n        delay(100);\n    }\n\n    baseline[0] = ax;\n    baseline[1] = ay;\n    baseline[2] = az;\n}</code></pre>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Calibrated-gestures.gif\" alt=\"Calibrated IMU readings for the gestures identification project\" /></p>\n<p>Much better.</p>\n<h3 id=\"tocdetect-first-motion\">2.3 Detect first motion</h3>\n<p>Now we need to check if motion is happening. To keep it simple, we'll use a naive approach that will look for an high value in the acceleration: if a threshold is exceeded, a gesture is starting. </p>\n<p>If you did the calibration step, a threshold of 5 should work well. If you didn't calibrate, you have to come up with a value that suits your needs.</p>\n<pre><code class=\"language-cpp\">#include imu.h\n\n#define ACCEL_THRESHOLD 5\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n}\n\nbool motionDetected(float ax, float ay, float az) {\n    return (abs(ax) + abs(ay) + abs(az)) &gt; ACCEL_THRESHOLD;\n}</code></pre>\n<h3 id=\"tocrecord-features\">2.4 Record features</h3>\n<p>If no motion is happening, we don't take any action and keep watching. If motion is happening, we print the next <code>NUM_SAMPLES</code> readings to Serial. </p>\n<pre><code class=\"language-cpp\">void loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n\n    recordIMU();\n    printFeatures();\n    delay(2000);\n}\n\nvoid recordIMU() {\n    float ax, ay, az;\n\n    for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n        imu_read(&amp;ax, &amp;ay, &amp;az);\n\n        ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n        ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n        az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n        features[i * NUM_AXES + 0] = ax;\n        features[i * NUM_AXES + 1] = ay;\n        features[i * NUM_AXES + 2] = az;\n\n        delay(INTERVAL);\n    }\n}</code></pre>\n<pre><code class=\"language-cpp\">\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n</code></pre>\n<p>Record 15-20 samples for each geasture and save them to a file, one for each gesture. Since we're dealing with highly dimensional data, you should collect as much samples as possible, to average out the noise.</p>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<p>In this project on Machine learning, differently from the previous and simpler ones, we're not achieving 100% accuracy easily. Motion is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-Linear-kernel.png\" alt=\"Decision boundaries of 2 PCA components of Gestures features, Linear kernel\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-Polynomial-kernel.png\" alt=\"Decision boundaries of 2 PCA components of Gestures features, Polynomial kernel\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.01-gamma.png\" alt=\"Decision boundaries of 2 PCA components of Gestures features, RBF kernel, 0.01 gamma\" /></p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.001-gamma.png\" alt=\"Decision boundaries of 2 PCA components of Gestures features, RBF kernel, 0.001 gamma\" /></p>\n<h3 id=\"tocselect-a-suitable-model\">3.1 Select a suitable model</h3>\n<p>Now that we selected the best model, we have to export it to C code. Here comes the culprit: not all models will fit on your board.</p>\n<p>The core of SVM (Support Vector Machines) are support vectors: each trained classifier will be characterized by a certain number of them. The problem is: if there're too much, the generated code will be too large to fit in your flash.</p>\n<p>For this reason, instead of selecting <em>the best</em> model on accuracy, you should make a ranking, from the best performing to the worst. For each model, starting from the top, you should import it in your Arduino project and try to compile: if it fits, fine, you're done. Otherwise you should pick the next and try again.</p>\n<p>It may seem a tedious process, but keep in mind that we're trying to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash: I think this is an acceptable tradeoff.</p>\n<hr /><p><em>We&#039;re fitting a model to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fhow-to-do-gesture-identification-on-arduino%2F&#038;text=We%27re%20fitting%20a%20model%20to%20infer%20a%20class%20from%2090%20features%20in%202%20Kb%20of%20RAM%20and%2032%20Kb%20of%20flash%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>I'll report a few figures for different combinations I tested.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Kernel</th>\n<th style=\"text-align: center;\">C</th>\n<th style=\"text-align: center;\">Gamma</th>\n<th style=\"text-align: center;\">Degree</th>\n<th style=\"text-align: center;\">Vectors</th>\n<th style=\"text-align: center;\">Flash size</th>\n<th style=\"text-align: center;\">RAM (b)</th>\n<th style=\"text-align: center;\">Avg accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">RBF</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">0.001</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">37</td>\n<td style=\"text-align: center;\">53 Kb</td>\n<td style=\"text-align: center;\">1228</td>\n<td style=\"text-align: center;\">99%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Poly</strong></td>\n<td style=\"text-align: center;\"><strong>100</strong></td>\n<td style=\"text-align: center;\"><strong>0.001</strong></td>\n<td style=\"text-align: center;\"><strong>2</strong></td>\n<td style=\"text-align: center;\"><strong>12</strong></td>\n<td style=\"text-align: center;\"><strong>25 Kb</strong></td>\n<td style=\"text-align: center;\"><strong>1228</strong></td>\n<td style=\"text-align: center;\"><strong>99%</strong></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Poly</td>\n<td style=\"text-align: center;\">100</td>\n<td style=\"text-align: center;\">0.001</td>\n<td style=\"text-align: center;\">3</td>\n<td style=\"text-align: center;\">25</td>\n<td style=\"text-align: center;\">40 Kb</td>\n<td style=\"text-align: center;\">1228</td>\n<td style=\"text-align: center;\">97%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Linear</td>\n<td style=\"text-align: center;\">50</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: center;\">55 Kb</td>\n<td style=\"text-align: center;\">1228</td>\n<td style=\"text-align: center;\">95%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">RBF</td>\n<td style=\"text-align: center;\">100</td>\n<td style=\"text-align: center;\">0.01</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">61</td>\n<td style=\"text-align: center;\">80 Kb</td>\n<td style=\"text-align: center;\">1228</td>\n<td style=\"text-align: center;\">95%</td>\n</tr>\n</tbody>\n</table>\n<p>As you can see, we achieved a very high accuracy on the test set for all the classifiers: only one, though, fitted on the Arduino Nano. Of course, if you use a larger board, you can deploy the others too.</p>\n<div class=\"infobox\">As a side note, take a look at the <code>RAM</code> column: all the values are equal: this is because in the implementation is independant from the number of support vectors and only depends on the number of features.</div>\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n\n    recordIMU();\n    classify();\n    delay(2000);\n}\n\nvoid classify() {\n    Serial.print(&quot;Detected gesture: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n}</code></pre>\n<p>Here we are: it has been a long post, but now you can classify gestures with an Arduino Nano and 2 Kb of RAM. </p>\n<hr /><p><em>No fancy Neural Networks, no Tensorflow, no 32-bit ARM processors: plain old SVM on plain old 8 bits with 97% accuracy.</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fhow-to-do-gesture-identification-on-arduino%2F&#038;text=No%20fancy%20Neural%20Networks%2C%20no%20Tensorflow%2C%20no%2032-bit%20ARM%20processors%3A%20plain%20old%20SVM%20on%20plain%20old%208%20bits%20with%2097%25%20accuracy.&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Here's a short demo of me playing 3 gestures and getting the results on the serial monitor.</p>\n<div style=\"width: 640px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-35-4\" width=\"640\" height=\"360\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4?_=4\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4</a></video></div>\n<p><h4>Project figures</h4>\r\n<p>On my machine, the sketch targeted at the Arduino Nano (old generation) requires 25310 bytes (82%) of program space and 1228 bytes (59%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question <em>Can I run machine learning on Arduino?</em> is <strong>definetly YES</strong>.<br />\n<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlGestureIdentificationExample/MicromlGestureIdentificationExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/\">How to do Gesture identification through machine learning on Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this Arduno Machine learning project we're going to use an accelerometer sensor to identify the gestures you play.\nThis is a remake of the project found on the Tensorflow blog. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.\n\n\nTable of contentsFeatures definitionRecord sample dataRead the IMU sensorCalibrationDetect first motionRecord featuresTrain and export the SVM classifierSelect a suitable modelRun the inference\n1. Features definition\nWe're going to use the accelerations along the 3 axis (X, Y, Z) coming from an IMU to infer which gesture we're playing. We'll use a fixed number of recordings (NUM_SAMPLES) starting from the first detection of movement. \nThis means our feature vectors are going to be of dimension 3 * NUM_SAMPLES, which can become too large to fit in the memory of the Arduino Nano. We'll start with a low value for NUM_SAMPLES to keep it as leaner as possible: if your classifications suffer from poor accuracy, you can increase this number.\n2. Record sample data\n2.1 Read the IMU sensor\nFirst of all, we need to read the raw data from the IMU. This piece of code will be different based on the specific chip you use. To keep things consistent, we'll wrap the IMU logic in 2 functions: imu_setup and imu_read. \nI'll report a couple of example implementations for the MPU6050 and the MPU9250 (these are the chip I have at hand). You should save whichever code you use in a file called imu.h. \n#include &quot;Wire.h&quot;\n// library from https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050\n#include &quot;MPU6050.h&quot;\n#define OUTPUT_READABLE_ACCELGYRO\n\nMPU6050 imu;\n\nvoid imu_setup() {\n    Wire.begin();\n    imu.initialize();\n}\n\nvoid imu_read(float *ax, float *ay, float *az) {\n    float gx, gy, gz;\n\n    imu.getMotion6(&amp;ax, &amp;ay, &amp;az, &amp;gx, &amp;gy, &amp;gz);\n}\n#include &quot;Wire.h&quot;\n// library from https://github.com/bolderflight/MPU9250\n#include &quot;MPU9250.h&quot;\n\nMPU9250 imu(Wire, 0x68);\n\nvoid imu_setup() {\n    Wire.begin();\n    imu.begin();\n}\n\nvoid imu_read(float *ax, float *ay, float *az) {\n    imu.readSensor();\n\n    *ax = imu.getAccelX_mss();\n    *ay = imu.getAccelY_mss();\n    *az = imu.getAccelZ_mss();\n}\nIn the main .ino file, we dump the values to Serial monitor / plotter.\n#include &quot;imu.h&quot;\n\n#define NUM_SAMPLES 30\n#define NUM_AXES 3\n// sometimes you may get &quot;spikes&quot; in the readings\n// set a sensible value to truncate too large values\n#define TRUNCATE_AT 20\n\ndouble features[NUM_SAMPLES * NUM_AXES];\n\nvoid setup() {\n    Serial.begin(115200);\n    imu_setup();\n}\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax, -TRUNCATE_AT, TRUNCATE_AT);\n    ay = constrain(ay, -TRUNCATE_AT, TRUNCATE_AT);\n    az = constrain(az, -TRUNCATE_AT, TRUNCATE_AT);\n\n    Serial.print(ax);\n    Serial.print(&#039;\\t&#039;);\n    Serial.print(ay);\n    Serial.print(&#039;\\t&#039;);\n    Serial.println(az);\n}\nOpen the Serial plotter and make some movement to have an idea of the range of your readings.\n\n2.2 Calibration\nDue to gravity, we get a stable value of -9.8 on the Z axis at rest (you can see this in the previous image). Since I'd like to have almost 0 at rest, I created a super simple calibration procedure to remove this fixed offset from the readings.\ndouble baseline[NUM_AXES];\ndouble features[NUM_SAMPLES * NUM_AXES];\n\nvoid setup() {\n    Serial.begin(115200);\n    imu_setup();\n    calibrate();\n}\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n}\n\nvoid calibrate() {\n    float ax, ay, az;\n\n    for (int i = 0; i &lt; 10; i++) {\n        imu_read(&amp;ax, &amp;ay, &amp;az);\n        delay(100);\n    }\n\n    baseline[0] = ax;\n    baseline[1] = ay;\n    baseline[2] = az;\n}\n\nMuch better.\n2.3 Detect first motion\nNow we need to check if motion is happening. To keep it simple, we'll use a naive approach that will look for an high value in the acceleration: if a threshold is exceeded, a gesture is starting. \nIf you did the calibration step, a threshold of 5 should work well. If you didn't calibrate, you have to come up with a value that suits your needs.\n#include imu.h\n\n#define ACCEL_THRESHOLD 5\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n}\n\nbool motionDetected(float ax, float ay, float az) {\n    return (abs(ax) + abs(ay) + abs(az)) &gt; ACCEL_THRESHOLD;\n}\n2.4 Record features\nIf no motion is happening, we don't take any action and keep watching. If motion is happening, we print the next NUM_SAMPLES readings to Serial. \nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n\n    recordIMU();\n    printFeatures();\n    delay(2000);\n}\n\nvoid recordIMU() {\n    float ax, ay, az;\n\n    for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n        imu_read(&amp;ax, &amp;ay, &amp;az);\n\n        ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n        ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n        az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n        features[i * NUM_AXES + 0] = ax;\n        features[i * NUM_AXES + 1] = ay;\n        features[i * NUM_AXES + 2] = az;\n\n        delay(INTERVAL);\n    }\n}\n\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n\nRecord 15-20 samples for each geasture and save them to a file, one for each gesture. Since we're dealing with highly dimensional data, you should collect as much samples as possible, to average out the noise.\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\nIn this project on Machine learning, differently from the previous and simpler ones, we're not achieving 100% accuracy easily. Motion is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:\n\n\n\n\n3.1 Select a suitable model\nNow that we selected the best model, we have to export it to C code. Here comes the culprit: not all models will fit on your board.\nThe core of SVM (Support Vector Machines) are support vectors: each trained classifier will be characterized by a certain number of them. The problem is: if there're too much, the generated code will be too large to fit in your flash.\nFor this reason, instead of selecting the best model on accuracy, you should make a ranking, from the best performing to the worst. For each model, starting from the top, you should import it in your Arduino project and try to compile: if it fits, fine, you're done. Otherwise you should pick the next and try again.\nIt may seem a tedious process, but keep in mind that we're trying to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash: I think this is an acceptable tradeoff.\nWe&#039;re fitting a model to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash!Click To Tweet\nI'll report a few figures for different combinations I tested.\n\n\n\nKernel\nC\nGamma\nDegree\nVectors\nFlash size\nRAM (b)\nAvg accuracy\n\n\n\n\nRBF\n10\n0.001\n-\n37\n53 Kb\n1228\n99%\n\n\nPoly\n100\n0.001\n2\n12\n25 Kb\n1228\n99%\n\n\nPoly\n100\n0.001\n3\n25\n40 Kb\n1228\n97%\n\n\nLinear\n50\n-\n1\n40\n55 Kb\n1228\n95%\n\n\nRBF\n100\n0.01\n-\n61\n80 Kb\n1228\n95%\n\n\n\nAs you can see, we achieved a very high accuracy on the test set for all the classifiers: only one, though, fitted on the Arduino Nano. Of course, if you use a larger board, you can deploy the others too.\nAs a side note, take a look at the RAM column: all the values are equal: this is because in the implementation is independant from the number of support vectors and only depends on the number of features.\n4. Run the inference\n#include &quot;model.h&quot;\n\nvoid loop() {\n    float ax, ay, az;\n\n    imu_read(&amp;ax, &amp;ay, &amp;az);\n\n    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);\n    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);\n    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);\n\n    if (!motionDetected(ax, ay, az)) {\n        delay(10);\n        return;\n    }\n\n    recordIMU();\n    classify();\n    delay(2000);\n}\n\nvoid classify() {\n    Serial.print(&quot;Detected gesture: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n}\nHere we are: it has been a long post, but now you can classify gestures with an Arduino Nano and 2 Kb of RAM. \nNo fancy Neural Networks, no Tensorflow, no 32-bit ARM processors: plain old SVM on plain old 8 bits with 97% accuracy.Click To Tweet\nHere's a short demo of me playing 3 gestures and getting the results on the serial monitor.\nhttps://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4\nProject figures\r\nOn my machine, the sketch targeted at the Arduino Nano (old generation) requires 25310 bytes (82%) of program space and 1228 bytes (59%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question Can I run machine learning on Arduino? is definetly YES.\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo How to do Gesture identification through machine learning on Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-19T14:25:46+01:00",
            "date_modified": "2019-12-29T19:13:20+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning"
            ],
            "attachments": [
                [
                    {
                        "url": "https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4",
                        "mime_type": "video/mp4",
                        "size_in_bytes": 1035484
                    }
                ]
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/arduino-pin-class/",
            "url": "https://eloquentarduino.github.io/2019/12/arduino-pin-class/",
            "title": "Eloquent pin management: the Pin class",
            "content_html": "<p><code>Pin</code> is a class for pin manipulation: you can read, write, turnOn, turnOff, toggle and a lot more. Please, stop writing horrible code like <code>digitalWrite(led, HIGH)</code> and start writing <code>led.turnOn()</code> instead. </p>\n<p><span id=\"more-167\"></span></p>\n<p><code>Pin</code> is actually an abstract class, so you won't use it directly, but through its specialized implementations:</p>\n<ol>\n<li>DigitalIn</li>\n<li>DigitalOut</li>\n<li>AnalogIn</li>\n<li>AnalogOut</li>\n</ol>\n<h2>Import the library</h2>\n<div class=\"watchout\">To follow this tutorial along you need to first <a href=\"/2019/12/how-to-install-the-eloquent-library/\" target=\"_blank\" rel=\"noopener noreferrer\">install the Eloquent library</a></div>\n<pre><code class=\"language-cpp\">#import &lt;eIO.h&gt;\n\nusing namespace Eloquent::Pin;</code></pre>\n<p>If the namespace stuff is new to you, here I'll briefly say that it is used to avoid name collisions among different libraries. This seems to be an alien topic in the Arduino world and I can't really explain why.</p>\n<p>99% of the libraries out there deal with this problem in one of two modes:</p>\n<ol>\n<li>ignoring it altogether, so you have an <code>MPU6050.h</code> library, which elects itself as the only one implementation possible to access the MPU6050 accelerometer in the world</li>\n<li>prefixing each library file, so you get the <code>Adafruit_Si7021</code> class</li>\n</ol>\n<hr /><p><em>The case for Adafruit_Si7021 should not exist in my opinion: use the damn namespaces!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Farduino-pin-class%2F&#038;text=The%20case%20for%20Adafruit_Si7021%20should%20not%20exist%20in%20my%20opinion%3A%20use%20the%20damn%20namespaces%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>With namespaces, it would become:</p>\n<pre><code class=\"language-cpp\">using namespace Adafruit;\n\nSi7021 si;</code></pre>\n<h2>How to use</h2>\n<p>First of all, all the 4 implementations accept a single constructor argument: the pin number.</p>\n<pre><code class=\"language-cpp\"> DigitalOut led(BUILTIN_LED);\n DigitalIn pushButton(10);\n AnalogIn potentiometer(A0);\n AnalogOut pwmLed(8);</code></pre>\n<p>Then it is good practice to init your pins in the setup.</p>\n<pre><code class=\"language-cpp\"> void setup() {\n    led.begin();\n    pushButton.begin();\n    potentiometer.begin();\n    pwmLed.begin();\n }</code></pre>\n<p>All the 4 classes let you ask for the pin current value via the <code>value()</code> method:</p>\n<pre><code class=\"language-cpp\"> void test() {\n    // DigitalIn returns the last read value, as 0/1\n    digitalIn.value();\n\n    // AnalogIn returns the last read value, in the range [0, 1024]\n    analogIn.value();\n\n    // DigitalOut returns the last written value, as 0/1\n    digitalOut.value();\n\n    // AnaloglOut returns the last written value, in the range [0, 255]\n    analogOut.value();\n }</code></pre>\n<p>At this point each class will provide its specialized methods.</p>\n<h4>DigitalIn</h4>\n<pre><code class=\"language-cpp\">void test() {\n    // configure pin as INPUT_PULLUP\n    pin.pullup();\n\n    // configure pin as Active Low \n    // that is, pin is ON when digitalRead() is LOW\n    pin.activeLow();\n\n    // read and update pin value\n    pin.read();\n\n    // test if pin is on (doesn&#039;t read the pin)\n    pin.isOn();\n\n    // test if pin is off (doesn&#039;t read the pin)\n    pin.isOff();\n\n    // test if pin value changed from last reading\n    pin.changed();\n}</code></pre>\n<h4>DigitalOut</h4>\n<pre><code class=\"language-cpp\">void test() {\n    // set pin as Active Low\n    // that is, turnOn writes LOW\n    pin.activeLow();\n\n    // turn pin on\n    pin.turnOn();\n\n    // turn pin off\n    pin.turnOff();\n\n    // toggle\n    pin.toggle();\n\n    // blink N times at intervals of X milliseconds\n    pin.blink(N, X);\n}</code></pre>\n<h4>AnalogIn</h4>\n<pre><code class=\"language-cpp\">void test() {\n    // read current pin value\n    pin.read();\n\n    // get pin previous value\n    pin.prev();\n\n    // get difference between current value and previous\n    pin.delta();\n\n    // get absolute value of delta()\n    pin.absDelta();\n}</code></pre>\n<h4>AnalogOut</h4>\n<pre><code class=\"language-cpp\">void test() {\n    // write value X to pin\n    pin.write(X);\n}</code></pre>\n<hr />\n<p>If you don't believe a whole class is worthwhile to work with pins, I'll show a few use cases to illustrate my point.</p>\n<h4>Use case #1: active low LED</h4>\n<p>The ESP8266 has a builtin LED you can control, but it is an <em>active low</em> one: it will turn on when you write <code>LOW</code>. In this case, <code>digitalWrite(BUILTIN_LED, LOW)</code> can be misleading regarding your actual intentions. </p>\n<p>It doesn't look intuitive,  it doesn't look <em>eloquent</em>! <code>builtinLed.turnOn()</code> does, however. All you need to get it working correctly is calling <code>builtinLed.activeLow()</code> once in your setup.</p>\n<pre><code class=\"language-cpp\">// BEFORE\nvoid loop() {\n    // to turn the builtin LED on\n    digitalWrite(led, LOW);\n}</code></pre>\n<pre><code class=\"language-cpp\">// AFTER\nDigitalOut buildtinLed;\n\nvoid setup() {\n    builtinLed.activeLow();\n}\n\nvoid loop() {\n    // to turn the builtin LED on\n    builtinLed.turnOn();\n}</code></pre>\n<h4>Use case #2: toggle</h4>\n<p>If you need to toggle the current state of a digital output, you need an helper variable to keep track of the state and remember to <strong>always</strong> update that variable when you write to the output.<br />\nWith a class, the state is tightly bound to the instance, so you have a <a href=\"https://en.wikipedia.org/wiki/Single_source_of_truth\">single source of truth</a>: <code>turnOn()</code>, <code>turnOff()</code> and <code>toggle()</code> will take care of updating the inner state accordingly.</p>\n<pre><code class=\"language-cpp\">// BEFORE\n#define LED 1\n\nbool ledState = true;\n\nloop() {\n    digitalWrite(LED, ledState);\n    ledState = !ledState\n}</code></pre>\n<pre><code class=\"language-cpp\">// AFTER\nDigitalOut led(1);\n\nvoid loop() {\n    led.toggle();\n}</code></pre>\n<h4>Use case #3: analog delta</h4>\n<p>What if you have an analog input and want to know if its valued changed by at least X from your latest reading? You would need an helper variable again. </p>\n<p>Now imagine if you have 5 analog inputs you want to track: you'll end up with 10 variables and of course you have again to <strong>always</strong> keep both in sync.<br />\n<code>AnalogIn</code> conveniently provides <code>delta()</code> and <code>absDelta()</code> methods that give you the change from the previous reading and will always be in sync. Awesome!</p>\n<pre><code class=\"language-cpp\">// BEFORE\n#define INPUT1 A1\n#define INPUT2 A2\n\nuint16_t current1, prev1;\nuint16_t current2, prev2;\n\nvoid loop() {\n    prev1 = current1;\n    current1 = analogRead(INPUT1);\n    prev2 = current2;\n    current2 = analogRead(INPUT2);\n\n    if (abs(current1 - prev1) &gt; THRESHOLD)\n        ...</code></pre>\n<pre><code class=\"language-cpp\">// AFTER\nAnalogIn input1(A1), input2(A2);\n\nvoid loop() {\n    input1.read();\n    input2.read();\n\n    if (input1.absDelta() &gt; THRESHOLD)\n        ...\n}</code></pre>\n<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/arduino-pin-class/\">Eloquent pin management: the Pin class</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Pin is a class for pin manipulation: you can read, write, turnOn, turnOff, toggle and a lot more. Please, stop writing horrible code like digitalWrite(led, HIGH) and start writing led.turnOn() instead. \n\nPin is actually an abstract class, so you won't use it directly, but through its specialized implementations:\n\nDigitalIn\nDigitalOut\nAnalogIn\nAnalogOut\n\nImport the library\nTo follow this tutorial along you need to first install the Eloquent library\n#import &lt;eIO.h&gt;\n\nusing namespace Eloquent::Pin;\nIf the namespace stuff is new to you, here I'll briefly say that it is used to avoid name collisions among different libraries. This seems to be an alien topic in the Arduino world and I can't really explain why.\n99% of the libraries out there deal with this problem in one of two modes:\n\nignoring it altogether, so you have an MPU6050.h library, which elects itself as the only one implementation possible to access the MPU6050 accelerometer in the world\nprefixing each library file, so you get the Adafruit_Si7021 class\n\nThe case for Adafruit_Si7021 should not exist in my opinion: use the damn namespaces!Click To Tweet\nWith namespaces, it would become:\nusing namespace Adafruit;\n\nSi7021 si;\nHow to use\nFirst of all, all the 4 implementations accept a single constructor argument: the pin number.\n DigitalOut led(BUILTIN_LED);\n DigitalIn pushButton(10);\n AnalogIn potentiometer(A0);\n AnalogOut pwmLed(8);\nThen it is good practice to init your pins in the setup.\n void setup() {\n    led.begin();\n    pushButton.begin();\n    potentiometer.begin();\n    pwmLed.begin();\n }\nAll the 4 classes let you ask for the pin current value via the value() method:\n void test() {\n    // DigitalIn returns the last read value, as 0/1\n    digitalIn.value();\n\n    // AnalogIn returns the last read value, in the range [0, 1024]\n    analogIn.value();\n\n    // DigitalOut returns the last written value, as 0/1\n    digitalOut.value();\n\n    // AnaloglOut returns the last written value, in the range [0, 255]\n    analogOut.value();\n }\nAt this point each class will provide its specialized methods.\nDigitalIn\nvoid test() {\n    // configure pin as INPUT_PULLUP\n    pin.pullup();\n\n    // configure pin as Active Low \n    // that is, pin is ON when digitalRead() is LOW\n    pin.activeLow();\n\n    // read and update pin value\n    pin.read();\n\n    // test if pin is on (doesn&#039;t read the pin)\n    pin.isOn();\n\n    // test if pin is off (doesn&#039;t read the pin)\n    pin.isOff();\n\n    // test if pin value changed from last reading\n    pin.changed();\n}\nDigitalOut\nvoid test() {\n    // set pin as Active Low\n    // that is, turnOn writes LOW\n    pin.activeLow();\n\n    // turn pin on\n    pin.turnOn();\n\n    // turn pin off\n    pin.turnOff();\n\n    // toggle\n    pin.toggle();\n\n    // blink N times at intervals of X milliseconds\n    pin.blink(N, X);\n}\nAnalogIn\nvoid test() {\n    // read current pin value\n    pin.read();\n\n    // get pin previous value\n    pin.prev();\n\n    // get difference between current value and previous\n    pin.delta();\n\n    // get absolute value of delta()\n    pin.absDelta();\n}\nAnalogOut\nvoid test() {\n    // write value X to pin\n    pin.write(X);\n}\n\nIf you don't believe a whole class is worthwhile to work with pins, I'll show a few use cases to illustrate my point.\nUse case #1: active low LED\nThe ESP8266 has a builtin LED you can control, but it is an active low one: it will turn on when you write LOW. In this case, digitalWrite(BUILTIN_LED, LOW) can be misleading regarding your actual intentions. \nIt doesn't look intuitive,  it doesn't look eloquent! builtinLed.turnOn() does, however. All you need to get it working correctly is calling builtinLed.activeLow() once in your setup.\n// BEFORE\nvoid loop() {\n    // to turn the builtin LED on\n    digitalWrite(led, LOW);\n}\n// AFTER\nDigitalOut buildtinLed;\n\nvoid setup() {\n    builtinLed.activeLow();\n}\n\nvoid loop() {\n    // to turn the builtin LED on\n    builtinLed.turnOn();\n}\nUse case #2: toggle\nIf you need to toggle the current state of a digital output, you need an helper variable to keep track of the state and remember to always update that variable when you write to the output.\nWith a class, the state is tightly bound to the instance, so you have a single source of truth: turnOn(), turnOff() and toggle() will take care of updating the inner state accordingly.\n// BEFORE\n#define LED 1\n\nbool ledState = true;\n\nloop() {\n    digitalWrite(LED, ledState);\n    ledState = !ledState\n}\n// AFTER\nDigitalOut led(1);\n\nvoid loop() {\n    led.toggle();\n}\nUse case #3: analog delta\nWhat if you have an analog input and want to know if its valued changed by at least X from your latest reading? You would need an helper variable again. \nNow imagine if you have 5 analog inputs you want to track: you'll end up with 10 variables and of course you have again to always keep both in sync.\nAnalogIn conveniently provides delta() and absDelta() methods that give you the change from the previous reading and will always be in sync. Awesome!\n// BEFORE\n#define INPUT1 A1\n#define INPUT2 A2\n\nuint16_t current1, prev1;\nuint16_t current2, prev2;\n\nvoid loop() {\n    prev1 = current1;\n    current1 = analogRead(INPUT1);\n    prev2 = current2;\n    current2 = analogRead(INPUT2);\n\n    if (abs(current1 - prev1) &gt; THRESHOLD)\n        ...\n// AFTER\nAnalogIn input1(A1), input2(A2);\n\nvoid loop() {\n    input1.read();\n    input2.read();\n\n    if (input1.absDelta() &gt; THRESHOLD)\n        ...\n}\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\nL'articolo Eloquent pin management: the Pin class proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-06T17:15:13+01:00",
            "date_modified": "2019-12-24T16:41:40+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "eloquent",
                "Eloquent library"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/",
            "url": "https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/",
            "title": "Morse alphabet identification on Arduino with Machine learning",
            "content_html": "<p>In this Arduno Machine learning project we're going to identify the letters from the <a href=\"https://en.wikipedia.org/wiki/Morse_code\">Morse alphabet</a>.<br />\nIn practice, we'll translate dots (\u2022) and dashes (\u2012)  &quot;typed&quot; with a push button into meaningful characters.<br />\nIn this tutorial we're going to target an Arduino Nano board (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>\n<p><span id=\"more-194\"></span></p>\n<p><img src=\"https://i.ytimg.com/vi/L6gxfX4GrbI/maxresdefault.jpg\" alt=\"credits to https://www.youtube.com/watch?v=L6gxfX4GrbI\" /></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocrecord-sample-data\">Record sample data</a><li><a href=\"#toctrain-and-export-the-svm-classifier\">Train and export the SVM classifier</a><li><a href=\"#tocrun-the-inference\">Run the inference</a></ol></div></p>\n<h2 id=\"tocfeatures-definition\">1. Features definition</h2>\n<p>For our task we'll use a simple push button as input and a fixed number of samples taken at a fixed interval (100 ms), starting from the first detection of the button press. I chose to record 30 samples for each letter, but you can easily customize the value as per your needs. </p>\n<p>With 30 samples at 100 ms frequency, we'll have 3 seconds to &quot;type&quot; the letter and on the Serial monitor will appear a sequence of 0s and 1s, representing if the button was pressed or not; the inference procedure will translate this sequence into a letter.<br />\nAs a reference, here are a couple example of what we'll be working with.</p>\n<pre><code class=\"language-cpp\">// A (\u2022\u2012)\n0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n\n// D (\u2012\u2022\u2022)\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n\n// E (\u2022)\n0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1</code></pre>\n<h2 id=\"tocrecord-sample-data\">2. Record sample data</h2>\n<p>To the bare minimum, we'll need a push button and two wires: one to ground and the other to a digital pin. Since in the example we'll make the button an <code>INPUT_PULLUP</code>, we'll read 0 when the button is pressed and 1 when not.  </p>\n<p><img src=\"https://www.arduino.cc/en/uploads/Tutorial/PullUp_bbd.png\" alt=\"credits to https://www.arduino.cc/en/Tutorial/DigitalInputPullup\" /></p>\n<p>All we need to do is detect a press and record the following 30 samples of the digital pin:</p>\n<pre><code class=\"language-cpp\">#define IN 4\n#define NUM_SAMPLES 30\n#define INTERVAL 100\n\ndouble features[NUM_SAMPLES];\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(IN, INPUT_PULLUP);\n}\n\nvoid loop() {\n  if (digitalRead(IN) == 0) {\n    recordButtonStatus();\n    printFeatures();\n    delay(1000);\n  }\n\n  delay(10);\n}\n\nvoid recordButtonStatus() {\n  for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n    features[i] = digitalRead(IN);\n    delay(INTERVAL);\n  } \n}</code></pre>\n<pre><code class=\"language-cpp\">\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n</code></pre>\n<p>Open the Serial monitor and type a few times each letter: try to introduce some variations each time, for example waiting some more milliseconds before releasing the dash.</p>\n<div class=\"watchout\"> If you've never typed morse code before (as me), choose letters with few keystrokes and quite differentiable, otherwise you will need to be very good with the timing.</div>\n<p>Save the recordings for each letter in a file named after the letter, so you will get meaningful results later on.</p>\n<p>You may end with duplicate recordings: don't worry, that's not a problem. I'll paste my recordings for a few letters, as a reference.</p>\n<pre><code>// A (\u2022\u2012)\n0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1\n\n// D (\u2012\u2022\u2022)\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1\n\n// E (\u2022)\n0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n\n// S (\u2022\u2022\u2022)\n0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n\n// T (\u2012)\n0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1</code></pre>\n<p>If you do a good job, you should end with quite distinguible features, as show in the plot below.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-from-Morse-alphabet-identification-features.svg\" alt=\"Decision boundaries of 2 PCA components from Morse alphabet identification features\" /></p>\n<h2 id=\"toctrain-and-export-the-svm-classifier\">3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<h2 id=\"tocrun-the-inference\">4. Run the inference</h2>\n<pre><code class=\"language-cpp\">#include &quot;model.h&quot;\n\nvoid loop() {\n  if (digitalRead(IN) == 0) {\n    recordButtonStatus();\n    Serial.print(&quot;Detected letter: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(1000);\n  }\n\n  delay(10);\n}</code></pre>\n<p>Type some letter using the push button and see the identified value printed on the serial monitor.</p>\n<p>That\u2019s it: you deployed machine learning in 2 Kb! </p>\n<p><h4>Project figures</h4>\r\n<p>On my machine, the sketch targeted at the Arduino Nano (old generation) requires 12546 bytes (40%) of program space and 366 bytes (17%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question <em>Can I run machine learning on Arduino?</em> is <strong>definetly YES</strong>.<br />\n<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlMorseIdentificationExample/MicromlMorseIdentificationExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/\">Morse alphabet identification on Arduino with Machine learning</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this Arduno Machine learning project we're going to identify the letters from the Morse alphabet.\nIn practice, we'll translate dots (\u2022) and dashes (\u2012)  &quot;typed&quot; with a push button into meaningful characters.\nIn this tutorial we're going to target an Arduino Nano board (old generation), equipped with 32 kb of flash and only 2 kb of RAM.\n\n\nTable of contentsFeatures definitionRecord sample dataTrain and export the SVM classifierRun the inference\n1. Features definition\nFor our task we'll use a simple push button as input and a fixed number of samples taken at a fixed interval (100 ms), starting from the first detection of the button press. I chose to record 30 samples for each letter, but you can easily customize the value as per your needs. \nWith 30 samples at 100 ms frequency, we'll have 3 seconds to &quot;type&quot; the letter and on the Serial monitor will appear a sequence of 0s and 1s, representing if the button was pressed or not; the inference procedure will translate this sequence into a letter.\nAs a reference, here are a couple example of what we'll be working with.\n// A (\u2022\u2012)\n0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n\n// D (\u2012\u2022\u2022)\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n\n// E (\u2022)\n0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n2. Record sample data\nTo the bare minimum, we'll need a push button and two wires: one to ground and the other to a digital pin. Since in the example we'll make the button an INPUT_PULLUP, we'll read 0 when the button is pressed and 1 when not.  \n\nAll we need to do is detect a press and record the following 30 samples of the digital pin:\n#define IN 4\n#define NUM_SAMPLES 30\n#define INTERVAL 100\n\ndouble features[NUM_SAMPLES];\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(IN, INPUT_PULLUP);\n}\n\nvoid loop() {\n  if (digitalRead(IN) == 0) {\n    recordButtonStatus();\n    printFeatures();\n    delay(1000);\n  }\n\n  delay(10);\n}\n\nvoid recordButtonStatus() {\n  for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n    features[i] = digitalRead(IN);\n    delay(INTERVAL);\n  } \n}\n\r\nvoid printFeatures() {\r\n    const uint16_t numFeatures = sizeof(features) / sizeof(double);\r\n    \r\n    for (int i = 0; i &lt; numFeatures; i++) {\r\n        Serial.print(features[i]);\r\n        Serial.print(i == numFeatures - 1 ? 'n' : ',');\r\n    }\r\n}\r\n\nOpen the Serial monitor and type a few times each letter: try to introduce some variations each time, for example waiting some more milliseconds before releasing the dash.\n If you've never typed morse code before (as me), choose letters with few keystrokes and quite differentiable, otherwise you will need to be very good with the timing.\nSave the recordings for each letter in a file named after the letter, so you will get meaningful results later on.\nYou may end with duplicate recordings: don't worry, that's not a problem. I'll paste my recordings for a few letters, as a reference.\n// A (\u2022\u2012)\n0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1\n\n// D (\u2012\u2022\u2022)\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1\n\n// E (\u2022)\n0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n\n// S (\u2022\u2022\u2022)\n0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n\n// T (\u2012)\n0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\nIf you do a good job, you should end with quite distinguible features, as show in the plot below.\n\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\n4. Run the inference\n#include &quot;model.h&quot;\n\nvoid loop() {\n  if (digitalRead(IN) == 0) {\n    recordButtonStatus();\n    Serial.print(&quot;Detected letter: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(1000);\n  }\n\n  delay(10);\n}\nType some letter using the push button and see the identified value printed on the serial monitor.\nThat\u2019s it: you deployed machine learning in 2 Kb! \nProject figures\r\nOn my machine, the sketch targeted at the Arduino Nano (old generation) requires 12546 bytes (40%) of program space and 366 bytes (17%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question Can I run machine learning on Arduino? is definetly YES.\nDid you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.\n\r\nCheck the full project code on Github\nL'articolo Morse alphabet identification on Arduino with Machine learning proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-06T13:07:38+01:00",
            "date_modified": "2019-12-29T19:15:23+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/arduino-bounded-waiting/",
            "url": "https://eloquentarduino.github.io/2019/12/arduino-bounded-waiting/",
            "title": "Eloquent bounded waiting: the await construct",
            "content_html": "<p>Sometimes you may need to wait for a certain condition to become true, but you don't want to wait forever: it may be awaiting for Serial, for the Wifi to connect to a network, or the response from a SoftwareSerial peripheral. The <code>await</code> construct lets you put an upper bound to the time you're willing to wait.</p>\n<p><span id=\"more-211\"></span></p>\n<p>Most often, you see example code of this kind:</p>\n<pre><code class=\"language-cpp\">Serial.print(&quot;Attempting to connect to WiFi&quot;);\n\nwhile (WiFi.status() != WL_CONNECTED) {\n    Serial.print(&quot;.&quot;);\n    delay(500);\n}</code></pre>\n<p>If the connection doesn't succeed (maybe the AP is out of range or is down), you're stuck in an endless wait. A proper way for handling such situations is with a timeout that gets you out of the loop with an error status so you can handle the failure.<br />\n<code>await</code> is exactly this: a construct to await for a condition to become true until a timeout expires, returning true or false as a response.</p>\n<h3>Definition</h3>\n<pre><code class=\"language-cpp\">#define await(condition, timeout) await_with_interval(condition, timeout, 10)\n#define await_with_interval(condition, timeout, interval) \\\n  ([]() { \\\n    uint32_t start = millis(); \\\n    while (millis() - start &lt;= timeout) { \\\n      if (condition) return true; \\\n      delay(interval); \\\n    } \\\n  return false; })()</code></pre>\n<h3>How to use</h3>\n<p><code>await</code> needs at least two arguments:</p>\n<ol>\n<li>the condition to await for</li>\n<li>the timeout, in milliseconds</li>\n</ol>\n<pre><code>// these are for greater code readability\r\n#define Millis \r\n#define Second  *1000\r\n#define Seconds *1000\r\n</code></pre>\n<pre><code class=\"language-cpp\">bool wifiConnected = await(WiFi.status() == WL_CONNECTED, 10 Seconds)</code></pre>\n<p>The code above will wait 10 seconds for the wifi to connect: on failure, <code>wifiConnected</code> will be false and you can gently fail. </p>\n<p>You can use it for any kind of check, like waiting for <code>Serial</code>.</p>\n<pre><code class=\"language-cpp\">bool serialReady = await(Serial, 5 Seconds)\nbool serialHasCharacters = await(Serial.available(), 5 Seconds)</code></pre>\n<p>The default interval between checks is 10 milliseconds: if you need a custom delay interval you can use the more verbose <code>await_with_interval</code>:</p>\n<pre><code class=\"language-cpp\">// await WiFi for 10 seconds, check if connected every 500 millis\nbool wifiConnected = await_with_interval(WiFi.status() == WL_CONNECTED, 10 Seconds, 500 Millis)</code></pre>\n<h3>How it works</h3>\n<p>The <code>await</code> macro creates an <a href=\"http://www.cplusplus.com/articles/2LywvCM9/\">inline function</a> that loops until the timeout expires. At every loop it checks if the condition is true: if that's the case, it returns true. The inline function construct is needed to get a return value, so you can assign it to a variable or embed directly inside an <code>if</code> test. The following code sample gives you an idea of what's happening.</p>\n<pre><code class=\"language-cpp\">bool wifiConnected = await(WiFi.status() == WL_CONNECTED, 10 Seconds)\n\n// conceptually translates to\n\nbool inline_function() {\n    uint32_t start = millis();\n\n    while (millis() - start &lt;= 10000) {\n      if (WiFi.status() == WL_CONNECTED)\n        return true;\n\n      delay(10);\n    }\n\n   return false;\n}\n\nbool wifiConnected = inline_function();</code></pre>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/arduino-bounded-waiting/\">Eloquent bounded waiting: the await construct</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Sometimes you may need to wait for a certain condition to become true, but you don't want to wait forever: it may be awaiting for Serial, for the Wifi to connect to a network, or the response from a SoftwareSerial peripheral. The await construct lets you put an upper bound to the time you're willing to wait.\n\nMost often, you see example code of this kind:\nSerial.print(&quot;Attempting to connect to WiFi&quot;);\n\nwhile (WiFi.status() != WL_CONNECTED) {\n    Serial.print(&quot;.&quot;);\n    delay(500);\n}\nIf the connection doesn't succeed (maybe the AP is out of range or is down), you're stuck in an endless wait. A proper way for handling such situations is with a timeout that gets you out of the loop with an error status so you can handle the failure.\nawait is exactly this: a construct to await for a condition to become true until a timeout expires, returning true or false as a response.\nDefinition\n#define await(condition, timeout) await_with_interval(condition, timeout, 10)\n#define await_with_interval(condition, timeout, interval) \\\n  ([]() { \\\n    uint32_t start = millis(); \\\n    while (millis() - start &lt;= timeout) { \\\n      if (condition) return true; \\\n      delay(interval); \\\n    } \\\n  return false; })()\nHow to use\nawait needs at least two arguments:\n\nthe condition to await for\nthe timeout, in milliseconds\n\n// these are for greater code readability\r\n#define Millis \r\n#define Second  *1000\r\n#define Seconds *1000\r\n\nbool wifiConnected = await(WiFi.status() == WL_CONNECTED, 10 Seconds)\nThe code above will wait 10 seconds for the wifi to connect: on failure, wifiConnected will be false and you can gently fail. \nYou can use it for any kind of check, like waiting for Serial.\nbool serialReady = await(Serial, 5 Seconds)\nbool serialHasCharacters = await(Serial.available(), 5 Seconds)\nThe default interval between checks is 10 milliseconds: if you need a custom delay interval you can use the more verbose await_with_interval:\n// await WiFi for 10 seconds, check if connected every 500 millis\nbool wifiConnected = await_with_interval(WiFi.status() == WL_CONNECTED, 10 Seconds, 500 Millis)\nHow it works\nThe await macro creates an inline function that loops until the timeout expires. At every loop it checks if the condition is true: if that's the case, it returns true. The inline function construct is needed to get a return value, so you can assign it to a variable or embed directly inside an if test. The following code sample gives you an idea of what's happening.\nbool wifiConnected = await(WiFi.status() == WL_CONNECTED, 10 Seconds)\n\n// conceptually translates to\n\nbool inline_function() {\n    uint32_t start = millis();\n\n    while (millis() - start &lt;= 10000) {\n      if (WiFi.status() == WL_CONNECTED)\n        return true;\n\n      delay(10);\n    }\n\n   return false;\n}\n\nbool wifiConnected = inline_function();\nL'articolo Eloquent bounded waiting: the await construct proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-05T19:50:59+01:00",
            "date_modified": "2019-12-16T23:03:25+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "eloquent",
                "Eloquent library"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2019/12/non-blocking-arduino-code/",
            "url": "https://eloquentarduino.github.io/2019/12/non-blocking-arduino-code/",
            "title": "Eloquent non-blocking code: the Every construct",
            "content_html": "<p>The <code>every</code> construct lets you run a piace of code at regular intervals in a fluent way. If you don't need to start, stop, pause your timer, this construct is a valid alternative to more complex timer libraries already available: it only takes a time interval as argument and will execute the code block periodically.</p>\n<p><span id=\"more-209\"></span></p>\n<h3>Definition</h3>\n<pre><code class=\"language-cpp\">#define every(interval) \\\n    static uint32_t __every__##interval = millis(); \\\n    if (millis() - __every__##interval &gt;= interval &amp;&amp; (__every__##interval = millis()))</code></pre>\n<h3>How to use</h3>\n<pre><code>// these are for greater code readability\r\n#define Millis \r\n#define Second  *1000\r\n#define Seconds *1000\r\n</code></pre>\n<pre><code class=\"language-cpp\">int interval = 1 Second;\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    every(1000 Millis) {\n        Serial.println(&quot;This line is printed every 1 second&quot;);\n    }\n\n    every(2000 Millis) {\n        Serial.println(&quot;This line is printed every 2 seconds&quot;);\n    }\n\n    every(interval) {\n        interval += 1 Second;\n        Serial.print(&quot;You can have variable intervals too! &quot;);\n        Serial.print(&quot;This line will be printed again in &quot;);\n        Serial.print(interval / 1000);\n        Serial.println(&quot; seconds&quot;);\n    }\n}</code></pre>\n<h3>Caveats</h3>\n<p><code>every</code> is just a macro definition and is not a proper timer, so it has some limitations:</p>\n<ol>\n<li>you can't stop, pause or resume it: once set, it will run forever</li>\n<li>its argument must be the suffix of a valid identifier</li>\n<li>you can't use several <code>every</code> with the exact same argument: you have to put all the code that needs to happen at the same interval in the same block</li>\n</ol>\n<h4>Caveat #2</h4>\n<p>The macro works by generating a variable named like <code>__every__##argument</code></p>\n<pre><code class=\"language-cpp\">every(1) ==&gt; uint32_t __every__1;\nevery(2) ==&gt; uint32_t __every__2;\nevery(a_given_interval) ==&gt; uint32_t __every__a_given_interval;\nevery(an invalid interval) ==&gt; uint32_t __every__an invalid interval; // Syntax error\nevery(1 Second) ==&gt; uint32_t __every__1 *1000; // Syntax error</code></pre>\n<p>So every integer literal and any variable are all valid arguments. Any expression is forbidden.</p>\n<h4>Caveat #3</h4>\n<p>If you use two <code>every</code> with the exact same argument, two variables with the exact same name will be created and it will rise a compile-time error.</p>\n<p>If you can live with this limitations, <code>every</code> only needs the space of an <code>uint32_t</code> to work.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2019/12/non-blocking-arduino-code/\">Eloquent non-blocking code: the Every construct</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "The every construct lets you run a piace of code at regular intervals in a fluent way. If you don't need to start, stop, pause your timer, this construct is a valid alternative to more complex timer libraries already available: it only takes a time interval as argument and will execute the code block periodically.\n\nDefinition\n#define every(interval) \\\n    static uint32_t __every__##interval = millis(); \\\n    if (millis() - __every__##interval &gt;= interval &amp;&amp; (__every__##interval = millis()))\nHow to use\n// these are for greater code readability\r\n#define Millis \r\n#define Second  *1000\r\n#define Seconds *1000\r\n\nint interval = 1 Second;\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    every(1000 Millis) {\n        Serial.println(&quot;This line is printed every 1 second&quot;);\n    }\n\n    every(2000 Millis) {\n        Serial.println(&quot;This line is printed every 2 seconds&quot;);\n    }\n\n    every(interval) {\n        interval += 1 Second;\n        Serial.print(&quot;You can have variable intervals too! &quot;);\n        Serial.print(&quot;This line will be printed again in &quot;);\n        Serial.print(interval / 1000);\n        Serial.println(&quot; seconds&quot;);\n    }\n}\nCaveats\nevery is just a macro definition and is not a proper timer, so it has some limitations:\n\nyou can't stop, pause or resume it: once set, it will run forever\nits argument must be the suffix of a valid identifier\nyou can't use several every with the exact same argument: you have to put all the code that needs to happen at the same interval in the same block\n\nCaveat #2\nThe macro works by generating a variable named like __every__##argument\nevery(1) ==&gt; uint32_t __every__1;\nevery(2) ==&gt; uint32_t __every__2;\nevery(a_given_interval) ==&gt; uint32_t __every__a_given_interval;\nevery(an invalid interval) ==&gt; uint32_t __every__an invalid interval; // Syntax error\nevery(1 Second) ==&gt; uint32_t __every__1 *1000; // Syntax error\nSo every integer literal and any variable are all valid arguments. Any expression is forbidden.\nCaveat #3\nIf you use two every with the exact same argument, two variables with the exact same name will be created and it will rise a compile-time error.\nIf you can live with this limitations, every only needs the space of an uint32_t to work.\nL'articolo Eloquent non-blocking code: the Every construct proviene da Eloquent Arduino Blog.",
            "date_published": "2019-12-05T19:42:45+01:00",
            "date_modified": "2019-12-16T22:59:36+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "eloquent",
                "Eloquent library"
            ]
        }
    ]
}