{
    "version": "https://jsonfeed.org/version/1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://eloquentarduino.github.io/tag/camera/feed/json/ -- and add it your reader.",
    "home_page_url": "https://eloquentarduino.github.io/tag/camera/",
    "feed_url": "https://eloquentarduino.github.io/tag/camera/feed/json/",
    "title": "Eloquent Arduino Blog",
    "description": "Machine learning on Arduino, programming &amp; electronics",
    "items": [
        {
            "id": "https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/",
            "url": "https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/",
            "title": "Handwritten digit classification with Arduino and MicroML",
            "content_html": "<p>We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST.gif\" alt=\"Arduino handwritten digit classification\" /></p>\n<p><span id=\"more-931\"></span></p>\n<p>If this is the first time you're reading my blog, you may have missed that I'm on a journey to push the limits of Machine learning on embedded devices like the Arduino boards and ESP32.</p>\n<p>I started with <a href=\"/2019/12/how-to-do-gesture-identification-on-arduino/\">accelerometer data classification</a>, then did <a href=\"/2019/12/wifi-indoor-positioning-on-arduino/\">Wifi indoor positioning</a> as a proof of concept.</p>\n<p>In the last weeks, though, I undertook a more difficult path that is image classification.</p>\n<p>Image classification is where Convolutional Neural Networks really shine, but I'm here to <a href=\"/2020/01/image-recognition-with-esp32-and-arduino/\">question this settlement</a> and demostrate that it is possible to come up with much lighter alternatives.</p>\n<p>In this post we continue with the examples, replicating a &quot;benchmark&quot; dataset in Machine learning: the handwritten digits classification.</p>\n<div class=\"infobox\">\nIf you are curious about a specific image classification task you would like to see implemented, <b>let me know in the comments</b>: I'm always open to new ideas\n</div>\n<h2>The task</h2>\n<p>The objective of this example is to be able to tell what an handwritten digit is, taking as input a photo from the ESP32 camera.</p>\n<p>In particular, we have 3 handwritten numbers and the task of our model will be to distinguish which image is what number.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-examples.jpg\" alt=\"Handwritten digits example\" /></p>\n<p>I only have a single image per digit, but you're free to draw as many samples as you like: it should help improve the performance of you're classifier.</p>\n<h2>1. Feature extraction</h2>\n<p>When dealing with images, if you use a CNN this step is often overlooked: CNNs are made on purpose to handle raw pixel values, so you just throw the image in and it is handled properly.</p>\n<p>When using other types of classifiers, it could help add a bit of feature engineering to help the classifier doing its job and achieve high accuracy.</p>\n<p>But not this time.</p>\n<p>I wanted to be as &quot;light&quot; as possible in this demo, so I only took a couple steps during the feature acquisition:</p>\n<ol>\n<li>use a grayscale image</li>\n<li>downsample to a manageable size</li>\n<li>convert it to black/white with a threshold</li>\n</ol>\n<p>I would hardly call this feature engineering.</p>\n<p>This is an example of the result of this pipeline.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-feature-extraction.jpg\" alt=\"Handwritten digit feature extraction\" /></p>\n<p>The code for this pipeline is really simple and is almost the same from the example on <a href=\"/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">motion detection</a>.</p>\n<pre><code class=\"language-c\">#include &quot;esp_camera.h&quot;\n\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    15\n#define XCLK_GPIO_NUM     27\n#define SIOD_GPIO_NUM     22\n#define SIOC_GPIO_NUM     23\n#define Y9_GPIO_NUM       19\n#define Y8_GPIO_NUM       36\n#define Y7_GPIO_NUM       18\n#define Y6_GPIO_NUM       39\n#define Y5_GPIO_NUM        5\n#define Y4_GPIO_NUM       34\n#define Y3_GPIO_NUM       35\n#define Y2_GPIO_NUM       32\n#define VSYNC_GPIO_NUM    25\n#define HREF_GPIO_NUM     26\n#define PCLK_GPIO_NUM     21\n\n#define FRAME_SIZE FRAMESIZE_QQVGA\n#define WIDTH 160\n#define HEIGHT 120\n#define BLOCK_SIZE 5\n#define W (WIDTH / BLOCK_SIZE)\n#define H (HEIGHT / BLOCK_SIZE)\n#define THRESHOLD 127\n\ndouble features[H*W] = { 0 };\n\nvoid setup() {\n    Serial.begin(115200);\n    Serial.println(setup_camera(FRAME_SIZE) ? &quot;OK&quot; : &quot;ERR INIT&quot;);\n    delay(3000);\n}\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n        return;\n    }\n\n    print_features();\n    delay(3000);\n}\n\nbool setup_camera(framesize_t frameSize) {\n    camera_config_t config;\n\n    config.ledc_channel = LEDC_CHANNEL_0;\n    config.ledc_timer = LEDC_TIMER_0;\n    config.pin_d0 = Y2_GPIO_NUM;\n    config.pin_d1 = Y3_GPIO_NUM;\n    config.pin_d2 = Y4_GPIO_NUM;\n    config.pin_d3 = Y5_GPIO_NUM;\n    config.pin_d4 = Y6_GPIO_NUM;\n    config.pin_d5 = Y7_GPIO_NUM;\n    config.pin_d6 = Y8_GPIO_NUM;\n    config.pin_d7 = Y9_GPIO_NUM;\n    config.pin_xclk = XCLK_GPIO_NUM;\n    config.pin_pclk = PCLK_GPIO_NUM;\n    config.pin_vsync = VSYNC_GPIO_NUM;\n    config.pin_href = HREF_GPIO_NUM;\n    config.pin_sscb_sda = SIOD_GPIO_NUM;\n    config.pin_sscb_scl = SIOC_GPIO_NUM;\n    config.pin_pwdn = PWDN_GPIO_NUM;\n    config.pin_reset = RESET_GPIO_NUM;\n    config.xclk_freq_hz = 20000000;\n    config.pixel_format = PIXFORMAT_GRAYSCALE;\n    config.frame_size = frameSize;\n    config.jpeg_quality = 12;\n    config.fb_count = 1;\n\n    bool ok = esp_camera_init(&amp;config) == ESP_OK;\n\n    sensor_t *sensor = esp_camera_sensor_get();\n    sensor-&gt;set_framesize(sensor, frameSize);\n\n    return ok;\n}\n\nbool capture_still() {\n    camera_fb_t *frame = esp_camera_fb_get();\n\n    if (!frame)\n        return false;\n\n    // reset all the features\n    for (size_t i = 0; i &lt; H * W; i++)\n      features[i] = 0;\n\n    // for each pixel, compute the position in the downsampled image\n    for (size_t i = 0; i &lt; frame-&gt;len; i++) {\n      const uint16_t x = i % WIDTH;\n      const uint16_t y = floor(i / WIDTH);\n      const uint8_t block_x = floor(x / BLOCK_SIZE);\n      const uint8_t block_y = floor(y / BLOCK_SIZE);\n      const uint16_t j = block_y * W + block_x;\n\n      features[j] += frame-&gt;buf[i];\n    }\n\n    // apply threshold\n    for (size_t i = 0; i &lt; H * W; i++) {\n      features[i] = (features[i] / (BLOCK_SIZE * BLOCK_SIZE) &gt; THRESHOLD) ? 1 : 0;\n    }\n\n    return true;\n}\n\nvoid print_features() {\n    for (size_t i = 0; i &lt; H * W; i++) {\n        Serial.print(features[i]);\n\n        if (i != H * W - 1)\n          Serial.print(&#039;,&#039;);\n    }\n\n    Serial.println();\n}</code></pre>\n<h2>2. Samples recording</h2>\n<p>To create your own dataset, you need a collection of handwritten digits.</p>\n<p>You can do this part as you like, by using pieces of paper or a monitor. I used a tablet because it was well illuminated and I could open a bunch of tabs to keep a record of my samples.</p>\n<p>As in the <a href=\"/2020/01/image-recognition-with-esp32-and-arduino/\">apple vs orange</a>, keep in mind that you should be consistent during both the training phase and the inference phase.</p>\n<p>This is why I used tape to fix my ESP32 camera to the desk and kept the tablet in the exact same position.</p>\n<p>If you desire, you could experiment varying slightly the capturing setup during the training and see if your classifier still achieves good accuracy: this is a test I didn't make.</p>\n<h2>3. Train and export the SVM classifier</h2>\r\n\r\n<p>For a detailed guide refer to the <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn/\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a></p>\r\n\r\n<p>\r\n<pre><code class=\"language-python\">from sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)</code></pre>\r\n\r\n<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>\n<h2>4. The result</h2>\n<p>Okay, at this point you should have all the working pieces to do handwritten digit image classification on your ESP32 camera. Include your model in the sketch and run the classification.</p>\n<pre><code class=\"language-c\">#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n\n        return;\n    }\n\n    Serial.print(&quot;Number: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(3000);\n}</code></pre>\n<p>Done.</p>\n<p>You can see a demo of my results in the video below.</p>\n<div style=\"width: 788px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-931-1\" width=\"788\" height=\"443\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4?_=1\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4</a></video></div>\n<h3>Project figures</h3>\n<p>My dataset is composed of 25 training samples in total and the SVM with linear kernel produced 17 support vectors.</p>\n<p>On my M5Stick camera board, the overhead for the model is 6.8 Kb of flash and the inference takes 7ms: not that bad!</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/HandwrittenDigitClassificationExample/HandwrittenDigitClassificationExample.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/\">Handwritten digit classification with Arduino and MicroML</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera.\n\n\nIf this is the first time you're reading my blog, you may have missed that I'm on a journey to push the limits of Machine learning on embedded devices like the Arduino boards and ESP32.\nI started with accelerometer data classification, then did Wifi indoor positioning as a proof of concept.\nIn the last weeks, though, I undertook a more difficult path that is image classification.\nImage classification is where Convolutional Neural Networks really shine, but I'm here to question this settlement and demostrate that it is possible to come up with much lighter alternatives.\nIn this post we continue with the examples, replicating a &quot;benchmark&quot; dataset in Machine learning: the handwritten digits classification.\n\nIf you are curious about a specific image classification task you would like to see implemented, let me know in the comments: I'm always open to new ideas\n\nThe task\nThe objective of this example is to be able to tell what an handwritten digit is, taking as input a photo from the ESP32 camera.\nIn particular, we have 3 handwritten numbers and the task of our model will be to distinguish which image is what number.\n\nI only have a single image per digit, but you're free to draw as many samples as you like: it should help improve the performance of you're classifier.\n1. Feature extraction\nWhen dealing with images, if you use a CNN this step is often overlooked: CNNs are made on purpose to handle raw pixel values, so you just throw the image in and it is handled properly.\nWhen using other types of classifiers, it could help add a bit of feature engineering to help the classifier doing its job and achieve high accuracy.\nBut not this time.\nI wanted to be as &quot;light&quot; as possible in this demo, so I only took a couple steps during the feature acquisition:\n\nuse a grayscale image\ndownsample to a manageable size\nconvert it to black/white with a threshold\n\nI would hardly call this feature engineering.\nThis is an example of the result of this pipeline.\n\nThe code for this pipeline is really simple and is almost the same from the example on motion detection.\n#include &quot;esp_camera.h&quot;\n\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    15\n#define XCLK_GPIO_NUM     27\n#define SIOD_GPIO_NUM     22\n#define SIOC_GPIO_NUM     23\n#define Y9_GPIO_NUM       19\n#define Y8_GPIO_NUM       36\n#define Y7_GPIO_NUM       18\n#define Y6_GPIO_NUM       39\n#define Y5_GPIO_NUM        5\n#define Y4_GPIO_NUM       34\n#define Y3_GPIO_NUM       35\n#define Y2_GPIO_NUM       32\n#define VSYNC_GPIO_NUM    25\n#define HREF_GPIO_NUM     26\n#define PCLK_GPIO_NUM     21\n\n#define FRAME_SIZE FRAMESIZE_QQVGA\n#define WIDTH 160\n#define HEIGHT 120\n#define BLOCK_SIZE 5\n#define W (WIDTH / BLOCK_SIZE)\n#define H (HEIGHT / BLOCK_SIZE)\n#define THRESHOLD 127\n\ndouble features[H*W] = { 0 };\n\nvoid setup() {\n    Serial.begin(115200);\n    Serial.println(setup_camera(FRAME_SIZE) ? &quot;OK&quot; : &quot;ERR INIT&quot;);\n    delay(3000);\n}\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n        return;\n    }\n\n    print_features();\n    delay(3000);\n}\n\nbool setup_camera(framesize_t frameSize) {\n    camera_config_t config;\n\n    config.ledc_channel = LEDC_CHANNEL_0;\n    config.ledc_timer = LEDC_TIMER_0;\n    config.pin_d0 = Y2_GPIO_NUM;\n    config.pin_d1 = Y3_GPIO_NUM;\n    config.pin_d2 = Y4_GPIO_NUM;\n    config.pin_d3 = Y5_GPIO_NUM;\n    config.pin_d4 = Y6_GPIO_NUM;\n    config.pin_d5 = Y7_GPIO_NUM;\n    config.pin_d6 = Y8_GPIO_NUM;\n    config.pin_d7 = Y9_GPIO_NUM;\n    config.pin_xclk = XCLK_GPIO_NUM;\n    config.pin_pclk = PCLK_GPIO_NUM;\n    config.pin_vsync = VSYNC_GPIO_NUM;\n    config.pin_href = HREF_GPIO_NUM;\n    config.pin_sscb_sda = SIOD_GPIO_NUM;\n    config.pin_sscb_scl = SIOC_GPIO_NUM;\n    config.pin_pwdn = PWDN_GPIO_NUM;\n    config.pin_reset = RESET_GPIO_NUM;\n    config.xclk_freq_hz = 20000000;\n    config.pixel_format = PIXFORMAT_GRAYSCALE;\n    config.frame_size = frameSize;\n    config.jpeg_quality = 12;\n    config.fb_count = 1;\n\n    bool ok = esp_camera_init(&amp;config) == ESP_OK;\n\n    sensor_t *sensor = esp_camera_sensor_get();\n    sensor-&gt;set_framesize(sensor, frameSize);\n\n    return ok;\n}\n\nbool capture_still() {\n    camera_fb_t *frame = esp_camera_fb_get();\n\n    if (!frame)\n        return false;\n\n    // reset all the features\n    for (size_t i = 0; i &lt; H * W; i++)\n      features[i] = 0;\n\n    // for each pixel, compute the position in the downsampled image\n    for (size_t i = 0; i &lt; frame-&gt;len; i++) {\n      const uint16_t x = i % WIDTH;\n      const uint16_t y = floor(i / WIDTH);\n      const uint8_t block_x = floor(x / BLOCK_SIZE);\n      const uint8_t block_y = floor(y / BLOCK_SIZE);\n      const uint16_t j = block_y * W + block_x;\n\n      features[j] += frame-&gt;buf[i];\n    }\n\n    // apply threshold\n    for (size_t i = 0; i &lt; H * W; i++) {\n      features[i] = (features[i] / (BLOCK_SIZE * BLOCK_SIZE) &gt; THRESHOLD) ? 1 : 0;\n    }\n\n    return true;\n}\n\nvoid print_features() {\n    for (size_t i = 0; i &lt; H * W; i++) {\n        Serial.print(features[i]);\n\n        if (i != H * W - 1)\n          Serial.print(&#039;,&#039;);\n    }\n\n    Serial.println();\n}\n2. Samples recording\nTo create your own dataset, you need a collection of handwritten digits.\nYou can do this part as you like, by using pieces of paper or a monitor. I used a tablet because it was well illuminated and I could open a bunch of tabs to keep a record of my samples.\nAs in the apple vs orange, keep in mind that you should be consistent during both the training phase and the inference phase.\nThis is why I used tape to fix my ESP32 camera to the desk and kept the tablet in the exact same position.\nIf you desire, you could experiment varying slightly the capturing setup during the training and see if your classifier still achieves good accuracy: this is a test I didn't make.\n3. Train and export the SVM classifier\r\n\r\nFor a detailed guide refer to the tutorial\r\n\r\n\r\nfrom sklearn.svm import SVC\r\nfrom micromlgen import port\r\n\r\n# put your samples in the dataset folder\r\n# one class per file\r\n# one feature vector per line, in CSV format\r\nfeatures, classmap = load_features('dataset/')\r\nX, y = features[:, :-1], features[:, -1]\r\nclassifier = SVC(kernel='linear').fit(X, y)\r\nc_code = port(classifier, classmap=classmap)\r\nprint(c_code)\r\n\r\nAt this point you have to copy the printed code and import it in your Arduino project, in a file called model.h.\n4. The result\nOkay, at this point you should have all the working pieces to do handwritten digit image classification on your ESP32 camera. Include your model in the sketch and run the classification.\n#include &quot;model.h&quot;\n\nvoid loop() {\n    if (!capture_still()) {\n        Serial.println(&quot;Failed capture&quot;);\n        delay(2000);\n\n        return;\n    }\n\n    Serial.print(&quot;Number: &quot;);\n    Serial.println(classIdxToName(predict(features)));\n    delay(3000);\n}\nDone.\nYou can see a demo of my results in the video below.\n\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4\nProject figures\nMy dataset is composed of 25 training samples in total and the SVM with linear kernel produced 17 support vectors.\nOn my M5Stick camera board, the overhead for the model is 6.8 Kb of flash and the inference takes 7ms: not that bad!\n\r\nCheck the full project code on Github\nL'articolo Handwritten digit classification with Arduino and MicroML proviene da Eloquent Arduino Blog.",
            "date_published": "2020-02-23T11:53:03+01:00",
            "date_modified": "2020-02-23T13:11:37+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "microml",
                "Arduino Machine learning",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 6424809
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "url": "https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/",
            "title": "Apple or Orange? Image recognition with ESP32 and Arduino",
            "content_html": "<p>Do you have an ESP32 camera? </p>\n<p>Want to do image recognition directly on your ESP32, without a PC?</p>\n<p>In this post we'll look into a very basic image recognition task: <strong>distinguish apples from oranges with machine learning</strong>.</p>\n<p><img src=\"/wp-content/uploads/2020/01/Apple-vs-Orange.gif\" alt=\"Apple vs Orange\" /></p>\n<p><span id=\"more-820\"></span></p>\n<p>Image recognition is a very hot topic these days in the AI/ML landscape. <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">Convolutional Neural Networks</a> really shines in this task and can achieve almost perfect accuracy on many scenarios.</p>\n<p>Sadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.</p>\n<p>Since in this series about <a href=\"/category/programming/arduino-machine-learning/\">Machine Learning on Microcontrollers</a> we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.</p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-were-going-to-do\">What we're going to do</a><li><a href=\"#tocfeatures-definition\">Features definition</a><li><a href=\"#tocextracting-rgb-components\">Extracting RGB components</a><li><a href=\"#tocrecord-samples-image\">Record samples image</a><li><a href=\"#toctraining-the-classifier\">Training the classifier</a><li><a href=\"#tocreal-world-example\">Real world example</a><ol><li><a href=\"#tocdisclaimer\">Disclaimer</a></ol></div></p>\n<h2 id=\"tocwhat-were-going-to-do\">What we're going to do</h2>\n<p>In a previous post about <a href=\"/2019/12/color-identification-on-arduino/\">color identification with Machine learning</a>, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.</p>\n<p>Of course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.</p>\n<p>Object inference, in that case, works only if you have exactly one object for a given color.</p>\n<p>The objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.</p>\n<p>This is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.</p>\n<div class=\"watchout\">\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. <b>This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame</b>. It will classify the current image based on the samples recorded during training.\n</div>\n<p>As any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.</p>\n<h2 id=\"tocfeatures-definition\">Features definition</h2>\n<p>I have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.</p>\n<p>Those few times I used CNN, I always used the whole image as input, <em>as-is</em>. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.</p>\n<p>I don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.</p>\n<p>I said we're using the RGB components of the image. But not all of them.</p>\n<p>Even at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.</p>\n<p>We need to reduce this number  to the bare minimum.</p>\n<p>How much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?</p>\n<p>You would be surprised to know that I got 90% accuracy with an RGB image of <strong>8x6</strong>!</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Orange-and-Apple-pixelated.jpg\" alt=\"You actually need very few pixels to do image classification\" /></p>\n<p>Yes, that's all we really need to do a <em>good enough</em> classification.</p>\n<hr /><p><em>You can distinguish apples from oranges on ESP32 with 8x6 pixels only!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fimage-recognition-with-esp32-and-arduino%2F&#038;text=You%20can%20distinguish%20apples%20from%20oranges%20on%20ESP32%20with%208x6%20pixels%20only%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Of course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.</p>\n<p>You have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.</p>\n<p>So we'll stick to 8x6 images.</p>\n<p>Now, how do you compact a 160x120 image to 8x6? With <em>downsampling</em>.</p>\n<p>This is the same tecnique we've used in the post about <a href=\"/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">motion detection on ESP32</a>: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).</p>\n<p><img src=\"/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>This time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.</p>\n<p>This is the code excerpt that does the downsampling.</p>\n<pre><code class=\"language-cpp\">uint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}</code></pre>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">STAY UP TO DATE</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2 id=\"tocextracting-rgb-components\">Extracting RGB components</h2>\n<p>The ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):</p>\n<ol>\n<li><strong>grayscale</strong>: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH</li>\n<li><strong>RGB565</strong>: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2</li>\n<li><strong>JPEG</strong>: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results</li>\n</ol>\n<p>For our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.</p>\n<p><img src=\"https://s2.www.theimagingsource.com/application-1.1.29/documentation/ic_imaging_control_class/en_US/images/rgb565.gif\" alt=\"taken from https://www.theimagingsource.com/support/documentation/ic-imaging-control-cpp/PixelformatRGB565.htm\" /></p>\n<pre><code class=\"language-cpp\">config.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}</code></pre>\n<h2 id=\"tocrecord-samples-image\">Record samples image</h2>\n<p>Now that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.</p>\n<p>Before doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.</p>\n<pre><code class=\"language-cpp\">#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}</code></pre>\n<p>Now you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.</p>\n<div class=\"watchout\">\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. <br />You need to be very accurate during this phase if you want to achieve good results.<br /> I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n</div>\n<h2 id=\"toctraining-the-classifier\">Training the classifier</h2>\n<p>To train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on <a href=\"/2019/11/how-to-train-a-classifier-in-scikit-learn\">how to train a ML classifier for Arduino</a> to get the exported model.</p>\n<p>You can experiment with different classifier configurations. </p>\n<p>My features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).</p>\n<p>One odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.</p>\n<p>The model produced 13 support vectors.</p>\n<p>I did no features scaling: you could try it if classifying more than 2 classes and having poor results.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange-decision-boundaries.png\" alt=\"Apple vs Orange decision boundaries\" /></p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>If you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.</p>\n<div style=\"width: 788px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-820-2\" width=\"788\" height=\"443\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4?_=2\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4</a></video></div>\n<p></p>\n<p>The little white object you see at the bottom of the image is the camera, taped to the desk.</p>\n<p>Did you think it was possible to do simple image classification on your ESP32?</p>\n<h3 id=\"tocdisclaimer\">Disclaimer</h3>\n<p>This is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.</p>\n<p>You have to carefully craft your setup and be as consistent as possible between training and inferencing.</p>\n<p>Still, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.</p>\n<p>In the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.</p>\n<p>Now that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? </p>\n<p>Let me know in the comments, we could even try realize it together if you need some help.</p>\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/Apple_vs_Orange/Apple_vs_Orange.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/\">Apple or Orange? Image recognition with ESP32 and Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? \nWant to do image recognition directly on your ESP32, without a PC?\nIn this post we'll look into a very basic image recognition task: distinguish apples from oranges with machine learning.\n\n\nImage recognition is a very hot topic these days in the AI/ML landscape. Convolutional Neural Networks really shines in this task and can achieve almost perfect accuracy on many scenarios.\nSadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.\nSince in this series about Machine Learning on Microcontrollers we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.\nTable of contentsWhat we're going to doFeatures definitionExtracting RGB componentsRecord samples imageTraining the classifierReal world exampleDisclaimer\nWhat we're going to do\nIn a previous post about color identification with Machine learning, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.\nOf course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.\nObject inference, in that case, works only if you have exactly one object for a given color.\nThe objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.\nThis is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.\n\nIn this context, image recognition means deciding which class (from the trained ones) the current image belongs to. This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame. It will classify the current image based on the samples recorded during training.\n\nAs any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.\nFeatures definition\nI have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.\nThose few times I used CNN, I always used the whole image as input, as-is. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.\nI don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.\nI said we're using the RGB components of the image. But not all of them.\nEven at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.\nWe need to reduce this number  to the bare minimum.\nHow much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?\nYou would be surprised to know that I got 90% accuracy with an RGB image of 8x6!\n\nYes, that's all we really need to do a good enough classification.\nYou can distinguish apples from oranges on ESP32 with 8x6 pixels only!Click To Tweet\nOf course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.\nYou have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.\nSo we'll stick to 8x6 images.\nNow, how do you compact a 160x120 image to 8x6? With downsampling.\nThis is the same tecnique we've used in the post about motion detection on ESP32: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).\n\nThis time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.\nThis is the code excerpt that does the downsampling.\nuint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };\n\nvoid grab_image() {\n    for (size_t i = 0; i &lt; len; i += 2) {\n        // get r, g, b from the buffer\n        // see later\n\n        const size_t j = i / 2;\n        // transform x, y in the original image to x, y in the downsampled image\n        // by dividing by BLOCK_SIZE\n        const uint16_t x = j % WIDTH;\n        const uint16_t y = floor(j / WIDTH);\n        const uint8_t block_x = floor(x / BLOCK_SIZE);\n        const uint8_t block_y = floor(y / BLOCK_SIZE);\n\n        // average pixels in block (accumulate)\n        rgb_frame[block_y][block_x][0] += r;\n        rgb_frame[block_y][block_x][1] += g;\n        rgb_frame[block_y][block_x][2] += b;\n    }\n}\n\r\n\r\n\r\n    \r\n\tSTAY UP TO DATE\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nExtracting RGB components\nThe ESP32 camera can store the image in different formats (of our interest \u2014 there are a couple more available):\n\ngrayscale: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH\nRGB565: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2\nJPEG: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results\n\nFor our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.\n\nconfig.pixel_format = PIXFORMAT_RGB565;\n\nfor (size_t i = 0; i &lt; len; i += 2) {\n    const uint8_t high = buf[i];\n    const uint8_t low  = buf[i+1];\n    const uint16_t pixel = (high &lt;&lt; 8) | low;\n\n    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;\n    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;\n    const uint8_t b = (pixel &amp; 0b0000000000011111);\n}\nRecord samples image\nNow that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.\nBefore doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.\n#define H (HEIGHT / BLOCK_SIZE)\n#define W (WIDTH / BLOCK_SIZE)\n\nvoid linearize_features() {\n  size_t i = 0;\n  double features[H*W*3] = {0};\n\n  for (int y = 0; y &lt; H; y++) {\n    for (int x = 0; x &lt; W; x++) {\n      features[i++] = rgb_frame[y][x][0];\n      features[i++] = rgb_frame[y][x][1];\n      features[i++] = rgb_frame[y][x][2];\n    }\n  }\n\n  // print to serial\n  for (size_t i = 0; i &lt; H*W*3; i++) {\n    Serial.print(features[i]);\n    Serial.print(&#039;\\t&#039;);\n  }\n\n  Serial.println();\n}\nNow you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.\n\nImage acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. You need to be very accurate during this phase if you want to achieve good results. I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.\n\nTraining the classifier\nTo train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on how to train a ML classifier for Arduino to get the exported model.\nYou can experiment with different classifier configurations. \nMy features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).\nOne odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.\nThe model produced 13 support vectors.\nI did no features scaling: you could try it if classifying more than 2 classes and having poor results.\n\nReal world example\nIf you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4\n\nThe little white object you see at the bottom of the image is the camera, taped to the desk.\nDid you think it was possible to do simple image classification on your ESP32?\nDisclaimer\nThis is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.\nYou have to carefully craft your setup and be as consistent as possible between training and inferencing.\nStill, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.\nIn the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.\nNow that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? \nLet me know in the comments, we could even try realize it together if you need some help.\n\r\nCheck the full project code on Github\nL'articolo Apple or Orange? Image recognition with ESP32 and Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-12T11:32:08+01:00",
            "date_modified": "2020-02-10T13:32:21+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "microml",
                "Arduino Machine learning",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 1642079
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "url": "https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/",
            "title": "Motion detection with ESP32 cam only (Arduino version)",
            "content_html": "<p>Do you have an <strong>ESP32 camera</strong>? Do you want to do motion detection <em>WITHOUT ANY</em> external hardware?</p>\n<p>Here's a tutorial made just for you: <strong>30 lines of code</strong> and you will know when something changes in your video stream  \ud83c\udfa5</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.gif\" alt=\"ESP32 camera motion detection example\" /></p>\n<p><span id=\"more-779\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</a><li><a href=\"#toccant-i-use-an-external-pir\">Can't I use an external PIR?</a><ol><li><a href=\"#tocexternal-hardware\">External hardware</a><li><a href=\"#tocfield-of-view\">Field of View</a><li><a href=\"#toccold-objects\">Cold objects</a></li></ol><li><a href=\"#tocwhat-do-you-need\">What do you need?</a><li><a href=\"#tochow-does-it-work\">How does it work?</a><ol><li><a href=\"#tocdownsampling\">Downsampling</a><li><a href=\"#tocblocks-difference-threshold\">Blocks difference threshold</a><li><a href=\"#tocimage-difference-threshold\">Image difference threshold</a><li><a href=\"#toccombining-all-together\">Combining all together</a></li></ol><li><a href=\"#tocreal-world-example\">Real world example</a></ol></div></p>\n<h2 id=\"tocwhat-is-naive-motion-detection\">What is (naive) motion detection?</h2>\n<p>Quoting from Wikipedia</p>\n<blockquote>\n<p>Motion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object</p>\n</blockquote>\n<p>In this project, we're implementing what I call <em>naive</em> motion detection: that is, we're not focusing on a particular object and following its motion.</p>\n<p>We'll only detect if any considerable portion of the image changed from one frame to the next.</p>\n<p>We won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.</p>\n<h2 id=\"toccant-i-use-an-external-pir\">Can't I use an external PIR?</h2>\n<p>Several projects on the internet about motion detection with an ESP32 cam use an external <a href=\"https://en.wikipedia.org/wiki/Passive_infrared_sensor\">PIR sensor</a> to trigger the video recording.</p>\n<p>What's the problem with that approach? </p>\n<h3 id=\"tocexternal-hardware\">1. External hardware</h3>\n<p>First of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice <a href=\"https://www.banggood.com/M5CameraF-ESP32-Fish-eye-Camera-Development-Board-Module-OV2640-Mini-Fisheye-Camera-Unit-Demoboard-p-1496820.html?rmmds=search&amp;cur_warehouse=CN\">M5stick camera</a> (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.</p>\n<h3 id=\"tocfield-of-view\">2. Field of View</h3>\n<p>PIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. </p>\n<p>My camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.</p>\n<h3 id=\"toccold-objects\">3. Cold objects</h3>\n<p>PIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).</p>\n<p>But motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.</p>\n<p>A PIR sensor can't do this: video motion detection can.</p>\n<hr /><p><em>ESP32 cam pure video motion detection can detect motion due to cold objects</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fmotion-detection-with-esp32-cam-only-arduino-version%2F&#038;text=ESP32%20cam%20pure%20video%20motion%20detection%20can%20detect%20motion%20due%20to%20cold%20objects&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<h2 id=\"tocwhat-do-you-need\">What do you need?</h2>\n<p>All you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:</p>\n<ul>\n<li>ESP32 cam</li>\n<li>ESP32 eye</li>\n<li>TTGO camera</li>\n<li>... any other flavor of ESP32 camera</li>\n</ul>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-models.jpg\" alt=\"ESP32 camera models\" /></p>\n<h2 id=\"tochow-does-it-work\">How does it work?</h2>\n<p>Ok, let's go to the &quot;technical&quot; stuff.</p>\n<p>Simply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.</p>\n<p>Well, it's <em>almost</em> like this.</p>\n<p>Of course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.</p>\n<h3 id=\"tocdownsampling\">Downsampling</h3>\n<p>One super-simple and super-effective way of doing this is to <strong>work with blocks</strong>, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.</p>\n<p>This greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/Image-downsampling-example.jpg\" alt=\"Image downsampling example\" /></p>\n<p>It's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  </p>\n<p>This has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.</p>\n<p>How should you choose the scale factor?</p>\n<p>Well, it depends.</p>\n<p>It depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. </p>\n<p>If you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.</p>\n<p>Experiment with your own use case a tweak with trial-and-error.</p>\n<h3 id=\"tocblocks-difference-threshold\">Blocks difference threshold</h3>\n<p>Once we've defined the block size, we need to detect if a block changed from one frame to the next.</p>\n<p>Of course, just testing for difference (<code>current != prev</code>) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.</p>\n<p>So we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.</p>\n<p>The higher the threshold, the less sensitive the algorithm will be.</p>\n<p>In code it is calculated as</p>\n<pre><code class=\"language-cpp\">float delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;</code></pre>\n<p>which indicates the relative increment/decrement from the previous value.</p>\n<h3 id=\"tocimage-difference-threshold\">Image difference threshold</h3>\n<p>Now that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.</p>\n<p>You could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.</p>\n<p>Let's return to the 320x240 image example. With a 10x10 block, you'll be working with <code>32x24 = 768</code> blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?</p>\n<p>I don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.</p>\n<p>If you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.</p>\n<p>In code it is calculated as:</p>\n<pre><code class=\"language-cpp\">float changedBlocksPercent = changedBlocks / totalBlocks</code></pre>\n<h3 id=\"toccombining-all-together\">Combining all together</h3>\n<p>Recapping: when running the motion detection algorithm you have 3 parameters to set:</p>\n<ol>\n<li>the block size</li>\n<li>the block difference threshold</li>\n<li>the image differerence threshold</li>\n</ol>\n<p>Let's pick 3 sensible defaults: <code>block size = 10</code>, <code>block threshold = 15%</code>, <code>image threshold = 20%</code>.</p>\n<p>What does these parameters translate to in the practice?</p>\n<p>They mean that motion will be detected if <code>20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next</code>.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-example.jpg\" alt=\"ESP32 camera motion example\" /></p>\n<p>As you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.</p>\n<h2 id=\"tocreal-world-example\">Real world example</h2>\n<p>Now the fun part. I'll show you how it performs on a real-world scenario.</p>\n<p>To keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. </p>\n<p>This means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.</p>\n<div style=\"width: 652px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-779-3\" width=\"652\" height=\"604\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4?_=3\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4</a></video></div>\n<p>This is me passing my arm in front of the camera a few times.</p>\n<p>The grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.</p>\n<p>The orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.</p>\n<p>When I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. </p>\n<p>While moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.</p>\n<p>This is why I suggest you keep the <code>image difference threshold</code> to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.</p>\n<p>Do you like the grid effect of the sample video? Let me know in the comment if you want me to share it.</p>\n<p>Or even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">STAY UP TO DATE</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<hr>\r\n<p>Check the full project code on <a href=\"https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/ESP32CameraNaiveMotionDetection/ESP32CameraNaiveMotionDetection.ino\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/motion-detection-with-esp32-cam-only-arduino-version/\">Motion detection with ESP32 cam only (Arduino version)</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Do you have an ESP32 camera? Do you want to do motion detection WITHOUT ANY external hardware?\nHere's a tutorial made just for you: 30 lines of code and you will know when something changes in your video stream  \ud83c\udfa5\n\n\nTable of contentsWhat is (naive) motion detection?Can't I use an external PIR?External hardwareField of ViewCold objectsWhat do you need?How does it work?DownsamplingBlocks difference thresholdImage difference thresholdCombining all togetherReal world example\nWhat is (naive) motion detection?\nQuoting from Wikipedia\n\nMotion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object\n\nIn this project, we're implementing what I call naive motion detection: that is, we're not focusing on a particular object and following its motion.\nWe'll only detect if any considerable portion of the image changed from one frame to the next.\nWe won't identify the location of motion (that's the subject for a next project), neither what caused it. We will analyze video stream in (almost) real-time and compare frame by frame: if lots of pixels changed, we'll call it motion.\nCan't I use an external PIR?\nSeveral projects on the internet about motion detection with an ESP32 cam use an external PIR sensor to trigger the video recording.\nWhat's the problem with that approach? \n1. External hardware\nFirst of all, you need external hardware. If you're using a breadboard, no problem, you just need a couple more wires and you're good to go. But I have a nice M5stick camera (no affiliate link), that's already well packaged, so it won't be that easy to add a PIR sensor.\n2. Field of View\nPIR sensors have a limited FOV (field of view), so you will need more than one to cover the whole range of the camera. \nMy camera, for example, has fish-eye lens which give me 160\u00b0 of view. Most cheap PIR sensors have a 120\u00b0 field of view, so one will not suffice. This adds even more space to my project.\n3. Cold objects\nPIR sensors gets triggered by infrared light. Infrared light gets emitted by hot bodies (like people and animals).\nBut motion in a video stream can happen for a variety of reasons, not necessarily due to hot bodies, for example if you want to monitor a street for cars passing by.\nA PIR sensor can't do this: video motion detection can.\nESP32 cam pure video motion detection can detect motion due to cold objectsClick To Tweet\nWhat do you need?\nAll you need for this project is a board with a camera sensor. As I said, I have a M5Stick Camera with fish-eye lens, but any ESP32 based camera should work out of the box:\n\nESP32 cam\nESP32 eye\nTTGO camera\n... any other flavor of ESP32 camera\n\n\nHow does it work?\nOk, let's go to the &quot;technical&quot; stuff.\nSimply put, the algorithm counts the number of different pixels from one frame to the next: if many pixels changed, it will detect motion.\nWell, it's almost like this.\nOf course such an algorithm will be very sensitive to noise (which is quite high on these low-cost cameras). We need to mitigate false-positive triggers.\nDownsampling\nOne super-simple and super-effective way of doing this is to work with blocks, instead of pixels. A block is simply an N x N square, whose value is the average of the pixels it contains.\nThis greatly reduces sensitivity to noise, providing a more robust detection. Here's an example of what the the &quot;block-ing&quot; operation does to an image.\n\nIt's really a &quot;pixelating&quot; effect: you take the orginal image (let's say 320x240 pixels) and resize it to 10x smaller, 32x24.  \nThis has the added benefit that it's much more lightweight to work with 32x24 matrix instead of 320x240 matrix: if you want to do real-time detection, this is a MUST.\nHow should you choose the scale factor?\nWell, it depends.\nIt depends on the sensitivity you want to achieve. The higher the downsampling, the less sensitive your detection will be. \nIf you want to detect a person passing 50cm away from the camera, you can increase this number without any problem. If you want to detect a dog 10m away, you should keep it in the 5-10 range.\nExperiment with your own use case a tweak with trial-and-error.\nBlocks difference threshold\nOnce we've defined the block size, we need to detect if a block changed from one frame to the next.\nOf course, just testing for difference (current != prev) would be again too sensitive to noise. A block can change for a variety of reasons, the first of which is the bad camera quality.\nSo we instead define a percent threshold above which we can say for sure the block actually changed. A good starting point could be 10-20%, but again you need to tweak this to your needs.\nThe higher the threshold, the less sensitive the algorithm will be.\nIn code it is calculated as\nfloat delta = abs(currentBlockValue - prevBlockValue) / prevBlockValue;\nwhich indicates the relative increment/decrement from the previous value.\nImage difference threshold\nNow that we can detect if a block changed from one frame to the next, we can actually detect if the image changed.\nYou could decide to trigger motion even if a single block changed, but I suggest you to set an higher value here.\nLet's return to the 320x240 image example. With a 10x10 block, you'll be working with 32x24 = 768 blocks: will you call it &quot;motion&quot; if 1 out of 768 blocks changed value?\nI don't think so. You want something more robust. You want 50 blocks to change. Or at least 20 blocks. If you do the math, 20 blocks out of 768 is only the 2.5% of change, which is hardly noticeable.\nIf you want to be robust, don't set this threshold to a too low value. Again, tweak with real world experimenting.\nIn code it is calculated as:\nfloat changedBlocksPercent = changedBlocks / totalBlocks\nCombining all together\nRecapping: when running the motion detection algorithm you have 3 parameters to set:\n\nthe block size\nthe block difference threshold\nthe image differerence threshold\n\nLet's pick 3 sensible defaults: block size = 10, block threshold = 15%, image threshold = 20%.\nWhat does these parameters translate to in the practice?\nThey mean that motion will be detected if 20% of the image, averaged in blocks of 10x10, changed its value by at least 15% from one frame to the next.\n\nAs you can see, you don't need high-definition images to (naively) detect if something happened to the image. Large area of motion will be easily detectable, even at very low resolution.\nReal world example\nNow the fun part. I'll show you how it performs on a real-world scenario.\nTo keep it simple, I wrote a sketch that does only motion detection, not video streaming over HTTP. \nThis means you won't be able to see the original image recorded from the camera. Nevertheless, I have kept the block size to a minimum to allow for the best quality possible.\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4\nThis is me passing my arm in front of the camera a few times.\nThe grid you see represents the actual pixels used for the computation. Each cell corresponds to one pixel of the downscaled image.\nThe orange cells highlight the pixels that the algorithm sees as &quot;different&quot; from one frame to the next. As you can see, some pixels are detected even if no motion is happening. That's the noise I talked about multiple times during the post.\nWhen I move my arm in the frame, you see lots of pixels become activated, so the &quot;Motion&quot; text appears. \nWhile moving the arm, you may notice what I call the &quot;ghost&quot; effect. You actually see 2 regions of motion: one is where my arm is now, which of course changed. The other is the region where my arm was in the previous frame, which returned to its original content.\nThis is why I suggest you keep the image difference threshold to a high value: if some real motion happens, you will notice it for sure because the activated region of the image will be actually bigger than the actual object moving.\nDo you like the grid effect of the sample video? Let me know in the comment if you want me to share it.\nOr even better: subscribe to the newsletter I you will get it directly in your inbox with my next mail.\n\r\n\r\n\r\n    \r\n\tSTAY UP TO DATE\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\n\r\nCheck the full project code on Github\nL'articolo Motion detection with ESP32 cam only (Arduino version) proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-05T12:08:08+01:00",
            "date_modified": "2020-01-25T17:11:53+01:00",
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "camera",
                "esp32",
                "Computer vision"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/01/ESP32-camera-motion-detection-example.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 1673368
                }
            ]
        }
    ]
}