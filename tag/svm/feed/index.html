<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>svm &#8211; Eloquent Arduino Blog</title>
	<atom:link href="https://eloquentarduino.github.io/tag/svm/feed/" rel="self" type="application/rss+xml" />
	<link>http://eloquentarduino.github.io/</link>
	<description>Machine learning on Arduino, programming &#38; electronics</description>
	<lastBuildDate>Sun, 09 Aug 2020 14:16:51 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.6</generator>
	<item>
		<title>How to train a IRIS classification Machine learning classifier directly on your Arduino board</title>
		<link>https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sat, 28 Mar 2020 18:02:09 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1008</guid>

					<description><![CDATA[<p>In this hands-on guide about on-board SVM training we're going to see a classifier in action, training it on the Iris dataset and evaluating its performance. What we'll make In this demo project we're going to take a know dataset (iris flowers) and interactively train an SVM classifier on it, adjusting the number of samples [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/">How to train a IRIS classification Machine learning classifier directly on your Arduino board</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this hands-on guide about <a href="/2020/03/so-you-want-to-train-an-ml-classifier-directly-on-an-arduino-board">on-board SVM training</a> we're going to see a classifier in action, training it on the Iris dataset and evaluating its performance.</p>
<p><span id="more-1008"></span></p>
<h2>What we'll make</h2>
<p>In this demo project we're going to take a know dataset (<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris flowers</a>) and interactively train an SVM classifier on it, adjusting the number of samples to see the effects on both training time, inference time and accuracy.</p>
<h2>Definitions</h2>
<pre><code class="language-c">#ifdef ESP32
#define min(a, b) (a) &lt; (b) ? (a) : (b)
#define max(a, b) (a) &gt; (b) ? (a) : (b)
#define abs(x) ((x) &gt; 0 ? (x) : -(x))
#endif

#include &lt;EloquentSVMSMO.h&gt;
#include &quot;iris.h&quot;

#define TOTAL_SAMPLES (POSITIVE_SAMPLES + NEGATIVE_SAMPLES)

using namespace Eloquent::ML;

float X_train[TOTAL_SAMPLES][FEATURES_DIM];
float X_test[TOTAL_SAMPLES][FEATURES_DIM];
int y_train[TOTAL_SAMPLES];
int y_test[TOTAL_SAMPLES];
SVMSMO&lt;FEATURES_DIM&gt; classifier(linearKernel);</code></pre>
<p>First of all we need to include a couple files, namely <code>EloquentSVMSMO.h</code> for the SVM classifier and <code>iris.h</code> for the dataset.</p>
<p><code>iris.h</code> defines a couple constants:</p>
<ul>
<li><code>FEATURES_DIM</code>: the number of features each sample has (4 in this case)</li>
<li><code>POSITIVE_SAMPLES</code>: the number of samples that belong to the positive class (50)</li>
<li><code>NEGATIVE_SAMPLES</code>: the number of samples that belong to the negative class (50)</li>
</ul>
<p>The we declare the array that hold the data: <code>X_train</code> and <code>y_train</code> for the training process, <code>X_test</code> and <code>y_test</code> for the inference process.</p>
<h2>Setup</h2>
<pre><code class="language-c">void setup() {
    Serial.begin(115200);
    delay(5000);

    // configure classifier
    classifier.setC(5);
    classifier.setTol(1e-5);
    classifier.setMaxIter(10000);
}</code></pre>
<p>Here we just set a few parameters for the classifier. You could actually skip this step in this demo, since the defaults will work well. Those lines are there so you know you can tweak them, if needed.</p>
<p>Please refer to the <a href="/2020/03/how-to-train-a-color-classification-machine-learning-classifier-directly-on-your-arduino-board">demo for color classification</a> for an explanation of each parameter.</p>
<h2>Interactivity</h2>
<pre><code class="language-c">void loop() {
    int positiveSamples = readSerialNumber(&quot;How many positive samples will you use for training? &quot;, POSITIVE_SAMPLES);

    if (positiveSamples &gt; POSITIVE_SAMPLES - 1) {
        Serial.println(&quot;Too many positive samples entered. All but one will be used instead&quot;);
        positiveSamples = POSITIVE_SAMPLES - 1;
    }

    int negativeSamples = readSerialNumber(&quot;How many negative samples will you use for training? &quot;, NEGATIVE_SAMPLES);

    if (negativeSamples &gt; NEGATIVE_SAMPLES - 1) {
        Serial.println(&quot;Too many negative samples entered. All but one will be used instead&quot;);
        negativeSamples = NEGATIVE_SAMPLES - 1;
    }

    loadDataset(positiveSamples, negativeSamples);

    // ...
}

/**
 * Ask the user to enter a numeric value
 */
int readSerialNumber(String prompt, int maxAllowed) {
    Serial.print(prompt);
    Serial.print(&quot; (&quot;);
    Serial.print(maxAllowed);
    Serial.print(&quot; max) &quot;);

    while (!Serial.available()) delay(1);

    int n = Serial.readStringUntil(&#039;\n&#039;).toInt();

    Serial.println(n);

    return n;
}

/**
 * Divide training and test data
 */
void loadDataset(int positiveSamples, int negativeSamples) {
    int positiveTestSamples = POSITIVE_SAMPLES - positiveSamples;

    for (int i = 0; i &lt; positiveSamples; i++) {
        memcpy(X_train[i], X_positive[i], FEATURES_DIM);
        y_train[i] = 1;
    }

    for (int i = 0; i &lt; negativeSamples; i++) {
        memcpy(X_train[i + positiveSamples], X_negative[i], FEATURES_DIM);
        y_train[i + positiveSamples] = -1;
    }

    for (int i = 0; i &lt; positiveTestSamples; i++) {
        memcpy(X_test[i], X_positive[i + positiveSamples], FEATURES_DIM);
        y_test[i] = 1;
    }

    for (int i = 0; i &lt; NEGATIVE_SAMPLES - negativeSamples; i++) {
        memcpy(X_test[i + positiveTestSamples], X_negative[i + negativeSamples], FEATURES_DIM);
        y_test[i + positiveTestSamples] = -1;
    }
}</code></pre>
<p>The code above is a preliminary step where you're asked to enter how many samples you will use for training of both positive and negative classes.</p>
<p>This way you can have multiple run of benchmarking without the need to re-compile and re-upload the sketch.</p>
<p>It also shows that the training process can be &quot;dynamic&quot;, in the sense that you can tweak it at runtime as per your need.</p>
<h2>Training</h2>
<pre><code class="language-c">time_t start = millis();
classifier.fit(X_train, y_train, positiveSamples + negativeSamples);
Serial.print(&quot;It took &quot;);
Serial.print(millis() - start);
Serial.print(&quot;ms to train on &quot;);
Serial.print(positiveSamples + negativeSamples);
Serial.println(&quot; samples&quot;);</code></pre>
<p>Training is actually a one line operation. Here we'll also logging how much time it takes to train.</p>
<h3>Predicting</h3>
<pre><code class="language-c">void loop() {
    // ...

    int tp = 0;
    int tn = 0;
    int fp = 0;
    int fn = 0;

    start = millis();

    for (int i = 0; i &lt; TOTAL_SAMPLES - positiveSamples - negativeSamples; i++) {
        int y_pred = classifier.predict(X_train, X_test[i]);
        int y_true = y_test[i];

        if (y_pred == y_true &amp;&amp; y_pred ==  1) tp += 1;
        if (y_pred == y_true &amp;&amp; y_pred == -1) tn += 1;
        if (y_pred != y_true &amp;&amp; y_pred ==  1) fp += 1;
        if (y_pred != y_true &amp;&amp; y_pred == -1) fn += 1;
    }

    Serial.print(&quot;It took &quot;);
    Serial.print(millis() - start);
    Serial.print(&quot;ms to test on &quot;);
    Serial.print(TOTAL_SAMPLES - positiveSamples - negativeSamples);
    Serial.println(&quot; samples&quot;);

    printConfusionMatrix(tp, tn, fp, fn);
}

/**
 * Dump confusion matrix to Serial monitor
 */
void printConfusionMatrix(int tp, int tn, int fp, int fn) {
    Serial.print(&quot;Overall accuracy &quot;);
    Serial.print(100.0 * (tp + tn) / (tp + tn + fp + fn));
    Serial.println(&quot;%&quot;);
    Serial.println(&quot;Confusion matrix&quot;);
    Serial.print(&quot;          | Predicted 1 | Predicted -1 |\n&quot;);
    Serial.print(&quot;----------------------------------------\n&quot;);
    Serial.print(&quot;Actual  1 |      &quot;);
    Serial.print(tp);
    Serial.print(&quot;     |      &quot;);
    Serial.print(fn);
    Serial.print(&quot;       |\n&quot;);
    Serial.print(&quot;----------------------------------------\n&quot;);
    Serial.print(&quot;Actual -1 |      &quot;);
    Serial.print(fp);
    Serial.print(&quot;      |      &quot;);
    Serial.print(tn);
    Serial.print(&quot;       |\n&quot;);
    Serial.print(&quot;----------------------------------------\n\n\n&quot;);
}</code></pre>
<p>Finally we can run the classification on our test set and get the overall accuracy.</p>
<p>We also print the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> to double-check each class accuracy.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the (more than) monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<hr />
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/IrisClassificationTrainingExample/IrisClassificationTrainingExample.ino">Github</a> where you'll also find another dataset to test, which is characterized by a number of features much higher (30 instead of 4).</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/03/how-to-train-a-iris-classification-machine-learning-classifier-directly-on-your-arduino-board/">How to train a IRIS classification Machine learning classifier directly on your Arduino board</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Handwritten digit classification with Arduino and MicroML</title>
		<link>https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 23 Feb 2020 10:53:03 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[Computer vision]]></category>
		<category><![CDATA[camera]]></category>
		<category><![CDATA[esp32]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=931</guid>

					<description><![CDATA[<p>We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera. If this is the first time you're reading my blog, you may have missed that I'm on a [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/">Handwritten digit classification with Arduino and MicroML</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>We continue exploring the endless possibilities on the MicroML (Machine Learning for Microcontrollers) framework on Arduino and ESP32 boards: in this post we're back to image classification. In particular, we'll distinguish handwritten digits using an ESP32 camera.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST.gif" alt="Arduino handwritten digit classification" /></p>
<p><span id="more-931"></span></p>
<p>If this is the first time you're reading my blog, you may have missed that I'm on a journey to push the limits of Machine learning on embedded devices like the Arduino boards and ESP32.</p>
<p>I started with <a href="/2019/12/how-to-do-gesture-identification-on-arduino/">accelerometer data classification</a>, then did <a href="/2019/12/wifi-indoor-positioning-on-arduino/">Wifi indoor positioning</a> as a proof of concept.</p>
<p>In the last weeks, though, I undertook a more difficult path that is image classification.</p>
<p>Image classification is where Convolutional Neural Networks really shine, but I'm here to <a href="/2020/01/image-recognition-with-esp32-and-arduino/">question this settlement</a> and demostrate that it is possible to come up with much lighter alternatives.</p>
<p>In this post we continue with the examples, replicating a &quot;benchmark&quot; dataset in Machine learning: the handwritten digits classification.</p>
<div class="infobox">
If you are curious about a specific image classification task you would like to see implemented, <b>let me know in the comments</b>: I'm always open to new ideas
</div>
<h2>The task</h2>
<p>The objective of this example is to be able to tell what an handwritten digit is, taking as input a photo from the ESP32 camera.</p>
<p>In particular, we have 3 handwritten numbers and the task of our model will be to distinguish which image is what number.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-examples.jpg" alt="Handwritten digits example" /></p>
<p>I only have a single image per digit, but you're free to draw as many samples as you like: it should help improve the performance of you're classifier.</p>
<h2>1. Feature extraction</h2>
<p>When dealing with images, if you use a CNN this step is often overlooked: CNNs are made on purpose to handle raw pixel values, so you just throw the image in and it is handled properly.</p>
<p>When using other types of classifiers, it could help add a bit of feature engineering to help the classifier doing its job and achieve high accuracy.</p>
<p>But not this time.</p>
<p>I wanted to be as &quot;light&quot; as possible in this demo, so I only took a couple steps during the feature acquisition:</p>
<ol>
<li>use a grayscale image</li>
<li>downsample to a manageable size</li>
<li>convert it to black/white with a threshold</li>
</ol>
<p>I would hardly call this feature engineering.</p>
<p>This is an example of the result of this pipeline.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/mnist-feature-extraction.jpg" alt="Handwritten digit feature extraction" /></p>
<p>The code for this pipeline is really simple and is almost the same from the example on <a href="/2020/01/motion-detection-with-esp32-cam-only-arduino-version/">motion detection</a>.</p>
<pre><code class="language-c">#include &quot;esp_camera.h&quot;

#define PWDN_GPIO_NUM     -1
#define RESET_GPIO_NUM    15
#define XCLK_GPIO_NUM     27
#define SIOD_GPIO_NUM     22
#define SIOC_GPIO_NUM     23
#define Y9_GPIO_NUM       19
#define Y8_GPIO_NUM       36
#define Y7_GPIO_NUM       18
#define Y6_GPIO_NUM       39
#define Y5_GPIO_NUM        5
#define Y4_GPIO_NUM       34
#define Y3_GPIO_NUM       35
#define Y2_GPIO_NUM       32
#define VSYNC_GPIO_NUM    25
#define HREF_GPIO_NUM     26
#define PCLK_GPIO_NUM     21

#define FRAME_SIZE FRAMESIZE_QQVGA
#define WIDTH 160
#define HEIGHT 120
#define BLOCK_SIZE 5
#define W (WIDTH / BLOCK_SIZE)
#define H (HEIGHT / BLOCK_SIZE)
#define THRESHOLD 127

double features[H*W] = { 0 };

void setup() {
    Serial.begin(115200);
    Serial.println(setup_camera(FRAME_SIZE) ? &quot;OK&quot; : &quot;ERR INIT&quot;);
    delay(3000);
}

void loop() {
    if (!capture_still()) {
        Serial.println(&quot;Failed capture&quot;);
        delay(2000);
        return;
    }

    print_features();
    delay(3000);
}

bool setup_camera(framesize_t frameSize) {
    camera_config_t config;

    config.ledc_channel = LEDC_CHANNEL_0;
    config.ledc_timer = LEDC_TIMER_0;
    config.pin_d0 = Y2_GPIO_NUM;
    config.pin_d1 = Y3_GPIO_NUM;
    config.pin_d2 = Y4_GPIO_NUM;
    config.pin_d3 = Y5_GPIO_NUM;
    config.pin_d4 = Y6_GPIO_NUM;
    config.pin_d5 = Y7_GPIO_NUM;
    config.pin_d6 = Y8_GPIO_NUM;
    config.pin_d7 = Y9_GPIO_NUM;
    config.pin_xclk = XCLK_GPIO_NUM;
    config.pin_pclk = PCLK_GPIO_NUM;
    config.pin_vsync = VSYNC_GPIO_NUM;
    config.pin_href = HREF_GPIO_NUM;
    config.pin_sscb_sda = SIOD_GPIO_NUM;
    config.pin_sscb_scl = SIOC_GPIO_NUM;
    config.pin_pwdn = PWDN_GPIO_NUM;
    config.pin_reset = RESET_GPIO_NUM;
    config.xclk_freq_hz = 20000000;
    config.pixel_format = PIXFORMAT_GRAYSCALE;
    config.frame_size = frameSize;
    config.jpeg_quality = 12;
    config.fb_count = 1;

    bool ok = esp_camera_init(&amp;config) == ESP_OK;

    sensor_t *sensor = esp_camera_sensor_get();
    sensor-&gt;set_framesize(sensor, frameSize);

    return ok;
}

bool capture_still() {
    camera_fb_t *frame = esp_camera_fb_get();

    if (!frame)
        return false;

    // reset all the features
    for (size_t i = 0; i &lt; H * W; i++)
      features[i] = 0;

    // for each pixel, compute the position in the downsampled image
    for (size_t i = 0; i &lt; frame-&gt;len; i++) {
      const uint16_t x = i % WIDTH;
      const uint16_t y = floor(i / WIDTH);
      const uint8_t block_x = floor(x / BLOCK_SIZE);
      const uint8_t block_y = floor(y / BLOCK_SIZE);
      const uint16_t j = block_y * W + block_x;

      features[j] += frame-&gt;buf[i];
    }

    // apply threshold
    for (size_t i = 0; i &lt; H * W; i++) {
      features[i] = (features[i] / (BLOCK_SIZE * BLOCK_SIZE) &gt; THRESHOLD) ? 1 : 0;
    }

    return true;
}

void print_features() {
    for (size_t i = 0; i &lt; H * W; i++) {
        Serial.print(features[i]);

        if (i != H * W - 1)
          Serial.print(&#039;,&#039;);
    }

    Serial.println();
}</code></pre>
<h2>2. Samples recording</h2>
<p>To create your own dataset, you need a collection of handwritten digits.</p>
<p>You can do this part as you like, by using pieces of paper or a monitor. I used a tablet because it was well illuminated and I could open a bunch of tabs to keep a record of my samples.</p>
<p>As in the <a href="/2020/01/image-recognition-with-esp32-and-arduino/">apple vs orange</a>, keep in mind that you should be consistent during both the training phase and the inference phase.</p>
<p>This is why I used tape to fix my ESP32 camera to the desk and kept the tablet in the exact same position.</p>
<p>If you desire, you could experiment varying slightly the capturing setup during the training and see if your classifier still achieves good accuracy: this is a test I didn't make.</p>
<h2>3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<h2>4. The result</h2>
<p>Okay, at this point you should have all the working pieces to do handwritten digit image classification on your ESP32 camera. Include your model in the sketch and run the classification.</p>
<pre><code class="language-c">#include &quot;model.h&quot;

void loop() {
    if (!capture_still()) {
        Serial.println(&quot;Failed capture&quot;);
        delay(2000);

        return;
    }

    Serial.print(&quot;Number: &quot;);
    Serial.println(classIdxToName(predict(features)));
    delay(3000);
}</code></pre>
<p>Done.</p>
<p>You can see a demo of my results in the video below.</p>
<div style="width: 788px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-931-1" width="788" height="443" preload="metadata" controls="controls"><source type="video/mp4" src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4?_=1" /><a href="https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4">https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4</a></video></div>
<h3>Project figures</h3>
<p>My dataset is composed of 25 training samples in total and the SVM with linear kernel produced 17 support vectors.</p>
<p>On my M5Stick camera board, the overhead for the model is 6.8 Kb of flash and the inference takes 7ms: not that bad!</p>
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/HandwrittenDigitClassificationExample/HandwrittenDigitClassificationExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/02/handwritten-digit-classification-with-arduino-and-microml/">Handwritten digit classification with Arduino and MicroML</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://eloquentarduino.github.io/wp-content/uploads/2020/02/MNIST-mute.mp4" length="6424809" type="video/mp4" />

			</item>
		<item>
		<title>Even smaller Machine learning models for your MCU: up to -82% code size</title>
		<link>https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sat, 15 Feb 2020 16:37:32 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[rvm]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=893</guid>

					<description><![CDATA[<p>So far we've used SVM (Support Vector Machine) as our main classifier to port a Machine learning model to a microcontroller: but recently I found an interesting alternative which could be waaaay smaller, mantaining a similar accuracy. Table of contentsThe current stateA new algorithm: Relevance Vector MachinesTraining a classifierPorting to CPerformace comparisonSize comparisonDisclaimer The current [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/">Even smaller Machine learning models for your MCU: up to -82% code size</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>So far we've used SVM (Support Vector Machine) as our main classifier to port a Machine learning model to a microcontroller: but recently I found an interesting alternative which could be waaaay smaller, mantaining a similar accuracy.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/rvm.jpg" alt="RVM vs SVM support vectors" /></p>
<p><span id="more-893"></span></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocthe-current-state">The current state</a><li><a href="#toca-new-algorithm-relevance-vector-machines">A new algorithm: Relevance Vector Machines</a><li><a href="#toctraining-a-classifier">Training a classifier</a><li><a href="#tocporting-to-c">Porting to C</a><li><a href="#tocperformace-comparison">Performace comparison</a><li><a href="#tocsize-comparison">Size comparison</a><li><a href="#tocdisclaimer">Disclaimer</a></ol></div></p>
<h2 id="tocthe-current-state">The current state</h2>
<p>I chose SVM as my main focus of intereset for the MicroML framework because I knew the support vector encoding could be very memory efficient once ported to plain C. And it really is.</p>
<p>I was able to port many real-world models (gesture identification, wake word detection) to tiny microcontrollers like the old Arduino Nano (32 kb flash, 2 kb RAM).</p>
<p>The tradeoff of my implementation was to sacrifice the flash space (which is usually quite big) to save as much RAM as possible, which is usually the most limiting factor.</p>
<p>Due to this implementation, if your model grows in size (highly dimensional data or not well separable data), the generated code will still fit in the RAM, but &quot;overflow&quot; the available flash.</p>
<p>In a couple of my previous post I warned that model selection might be a required step before being able to deploy a model to a MCU, since you should first check if it fits. If not, you must train another model hoping to get fewer support vectors, since each of them contributes to the code size increase.</p>
<h2 id="toca-new-algorithm-relevance-vector-machines">A new algorithm: Relevance Vector Machines</h2>
<p>It was by chance that I came across a new algorithm that I never heard of, called <a href="https://en.wikipedia.org/wiki/Relevance_vector_machine">Relevance Vector Machine</a>. It was patented by Microsoft until last year (so maybe this is the reason you don't see it in the wild), but now it is free of use as far as I can tell.</p>
<p>Here is the <a href="https://papers.nips.cc/paper/1719-the-relevance-vector-machine.pdf">link</a> to the paper if you want to read it, it gives some insights into the development process.</p>
<p>I'm not a mathematician, so I can't describe it accurately, but in a few words it uses the same formulation of SVM (a weightened sum of kernels), applying a Bayesan model.</p>
<p>This serves in the first place to be able to get the probabilities of the classification results, which is something totally missing in SVM.</p>
<p>In the second place, the algorithm tries to learn a much more sparse representation of the support vectors, as you can see in the following picture.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/02/rvm.jpg" alt="RVM vs SVM support vectors" /></p>
<p>When I first read the paper my first tought was just &quot;wow&quot;! This is exactly what I need for my MicroML framework: a ultra-lightweight model which can still achieve high accuracy.</p>
<h2 id="toctraining-a-classifier">Training a classifier</h2>
<p>Now that I knew this algorithm, I searched for it in the <code>sklearn</code> documentation: it was not there.</p>
<p>It seems that, since it was patented, they didn't have an implementation.</p>
<p>Fortunately, there is <a href="https://github.com/AmazaspShumik/sklearn_bayes/">an implementation</a> which follows the sklearn paradigm. You have to install it:</p>
<pre><code class="language-bash">pip install Cython
pip install https://github.com/AmazaspShumik/sklearn_bayes/archive/master.zip</code></pre>
<p>Since the interface is the usual <code>fit</code> <code>predict</code>, it is super easy to train a classifier.</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
from skbayes.rvm_ard_models import RVC
import warnings

# I get tons of boring warnings during training, so turn it off
warnings.filterwarnings(&quot;ignore&quot;)

iris = load_iris()
X = iris.data
y = iris.target
clf = RVC(kernel=&#039;rbf&#039;, gamma=0.001)
clf.fit(X, y)
y_predict = clf.predict(X)</code></pre>
<p>The parameters for the constructor are similar to those of the <code>SVC</code> classifier from sklearn:</p>
<ul>
<li><code>kernel</code>: one of linear, poly, rbf</li>
<li><code>degree</code>: if <code>kernel=poly</code></li>
<li><code>gamma</code>: if <code>kernel=poly</code> or <code>kernel=rbf</code></li>
</ul>
<p>You can read <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">the docs</a> from sklearn to learn more.</p>
<h2 id="tocporting-to-c">Porting to C</h2>
<p>Now that we have a trained classifier, we have to port it to plain C that compiles on our microcontroller of choice.</p>
<p>I patched my package <code>micromlgen</code> to do the job for you, so you should install the latest version to get it working.</p>
<pre><code class="language-bash"> pip install --upgrade micromlgen</code></pre>
<p>Now the export part is almost the same as with an SVM classifier.</p>
<pre><code class="language-python"> from micromlgen import port_rvm

 clf = get_rvm_classifier()
 c_code = port_rvm(clf)
 print(c_code)</code></pre>
<p>And you're done: you have plain C code you can embed in any microcontroller.</p>
<h2 id="tocperformace-comparison">Performace comparison</h2>
<p>To test the effectiveness of this new algorithm, I applied it to the datasets I built in my previous posts, comparing side by side the size and accuracy of both SVM and RVM.</p>
<p>The results are summarized in the next table.</p>
<style>
.dataset th+th, .dataset td + td { text-align: center; }
.dataset small { display: block; font-size: 0.8em; }
.dataset .__h td {background: blanchedalmond !important}
</style>
<table class="dataset">
<thead>
<tr>
<th>Dataset</th>
<th colspan="2">SVM</th>
<th colspan="2">RVM</th>
<th colspan="2">Delta</th>
</tr>
<tr>
<th></th>
<th>Flash<small>(byte)</small></th>
<th>Acc. <small>(%)</small></th>
<th>Flash<small>(byte)</small></th>
<th>Acc. <small>(%)</small></th>
<th>Flash</th>
<th>Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGB colors</td>
<td>4584</td>
<td>100</td>
<td>3580</td>
<td>100</td>
<td>-22%</td>
<td>-0%</td>
</tr>
<tr>
<td>Accelerometer gestures<small>(linear kernel)</small></td>
<td>36888</td>
<td>92</td>
<td>7056</td>
<td>85</td>
<td>-80%</td>
<td>-7%</td>
</tr>
<tr class="__h">
<td>Accelerometer gestures<small>(gaussian kernel)</small></td>
<td>45348</td>
<td>95</td>
<td>7766</td>
<td>95</td>
<td>-82%</td>
<td>-0%</td>
</tr>
<tr>
<td>Wifi positioning</td>
<td>4641</td>
<td>100</td>
<td>3534</td>
<td>100</td>
<td>-24%</td>
<td>-0%</td>
</tr>
<tr>
<td>Wake word<small>(linear kernel)</small></td>
<td>18098</td>
<td>86</td>
<td>3602</td>
<td>53</td>
<td>-80%</td>
<td>-33%</td>
</tr>
<tr>
<td>Wake word<small>(gaussian kernel)</small></td>
<td>21788</td>
<td>90</td>
<td>4826</td>
<td>62</td>
<td>-78%</td>
<td>-28%</td>
</tr>
</tbody>
</table>
<p><small style="font-style: italic; font-size: 0.8em;">** the accuracy reported are with default parameters, without any tuning, averaged in 30 runs</small></p>
<p>As you may see, the results are quite surpising:</p>
<ul>
<li>you can achieve <strong>up to 82% space reduction</strong> on highly dimensional dataset <strong>without any loss in accuracy</strong> (accelerometer gestures with gaussian kernel)</li>
<li>sometimes you may not be able to achieve a decent accuracy (62% at most on the wake word dataset)</li>
</ul>
<p>As in any situation, you should test which one of the two algorithms works best for your use case, but there a couple of guidelines you may follow:</p>
<ul>
<li>if you need top accuracy, probably SVM can achieve slighter better performance if you have enough space</li>
<li>if you need tiny space or top speed, test if RVM achieves a satisfiable accuracy</li>
<li>if both SVM and RVM achieve comparable performace, go with RVM: it's much lighter than SVM in most cases and will run faster</li>
</ul>
<h2 id="tocsize-comparison">Size comparison</h2>
<p>As a reference, here is the codes generated for an SVM classifier and an RVM one to classify the IRIS dataset.</p>
<pre><code class="language-c">uint8_t predict_rvm(double *x) {
    double decision[3] = { 0 };
    decision[0] = -0.6190847299428206;
    decision[1] = (compute_kernel(x,  6.3, 3.3, 6.0, 2.5) - 72.33233 ) * 0.228214 + -2.3609625;
    decision[2] = (compute_kernel(x,  7.7, 2.8, 6.7, 2.0) - 81.0089166 ) * -0.29006 + -3.360963;
    uint8_t idx = 0;
    double val = decision[0];
    for (uint8_t i = 1; i &lt; 3; i++) {
        if (decision[i] &gt; val) {
            idx = i;
            val = decision[i];
        }
    }
    return idx;
}

int predict_svm(double *x) {
    double kernels[10] = { 0 };
    double decisions[3] = { 0 };
    int votes[3] = { 0 };
        kernels[0] = compute_kernel(x,   6.7  , 3.0  , 5.0  , 1.7 );
        kernels[1] = compute_kernel(x,   6.0  , 2.7  , 5.1  , 1.6 );
        kernels[2] = compute_kernel(x,   5.1  , 2.5  , 3.0  , 1.1 );
        kernels[3] = compute_kernel(x,   6.0  , 3.0  , 4.8  , 1.8 );
        kernels[4] = compute_kernel(x,   7.2  , 3.0  , 5.8  , 1.6 );
        kernels[5] = compute_kernel(x,   4.9  , 2.5  , 4.5  , 1.7 );
        kernels[6] = compute_kernel(x,   6.2  , 2.8  , 4.8  , 1.8 );
        kernels[7] = compute_kernel(x,   6.0  , 2.2  , 5.0  , 1.5 );
        kernels[8] = compute_kernel(x,   4.8  , 3.4  , 1.9  , 0.2 );
        kernels[9] = compute_kernel(x,   5.1  , 3.3  , 1.7  , 0.5 );
        decisions[0] = 20.276395502
                    + kernels[0] * 100.0
                    + kernels[1] * 100.0
                    + kernels[3] * -79.351629954
                    + kernels[4] * -49.298850195
                    + kernels[6] * -40.585178082
                    + kernels[7] * -30.764341769
        ;
        decisions[1] = -0.903345464
                    + kernels[2] * 0.743494115
                    + kernels[9] * -0.743494115
        ;
        decisions[2] = -1.507856504
                    + kernels[5] * 0.203695177
                    + kernels[8] * -0.160020702
                    + kernels[9] * -0.043674475
        ;
        votes[decisions[0] &gt; 0 ? 0 : 1] += 1;
        votes[decisions[1] &gt; 0 ? 0 : 2] += 1;
        votes[decisions[2] &gt; 0 ? 1 : 2] += 1;
                int classVal = -1;
        int classIdx = -1;
        for (int i = 0; i &lt; 3; i++) {
            if (votes[i] &gt; classVal) {
                classVal = votes[i];
                classIdx = i;
            }
        }
        return classIdx;
}</code></pre>
<p>As you can see, RVM actually only computes 2 kernels and does 2 multiplications. SVM, on the other hand, computes 10 kernels and does 13 multiplications.</p>
<p>This is a recurring pattern, so RVM is much much faster in the inference process.</p>
<h2 id="tocdisclaimer">Disclaimer</h2>
<p><code>micromlgen</code> and in particular <code>port_rvm</code> are work in progress: you may experience some glitches or it may not work in your specific case. Please report any issue <a href="https://github.com/eloquentarduino/micromlgen">on the Github repo</a>.</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/02/even-smaller-machine-learning-models-for-your-mcu/">Even smaller Machine learning models for your MCU: up to -82% code size</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Apple or Orange? Image recognition with ESP32 and Arduino</title>
		<link>https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 12 Jan 2020 10:32:08 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[Computer vision]]></category>
		<category><![CDATA[camera]]></category>
		<category><![CDATA[esp32]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=820</guid>

					<description><![CDATA[<p>Do you have an ESP32 camera? Want to do image recognition directly on your ESP32, without a PC? In this post we'll look into a very basic image recognition task: distinguish apples from oranges with machine learning. Image recognition is a very hot topic these days in the AI/ML landscape. Convolutional Neural Networks really shines [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/">Apple or Orange? Image recognition with ESP32 and Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Do you have an ESP32 camera? </p>
<p>Want to do image recognition directly on your ESP32, without a PC?</p>
<p>In this post we'll look into a very basic image recognition task: <strong>distinguish apples from oranges with machine learning</strong>.</p>
<p><img src="/wp-content/uploads/2020/01/Apple-vs-Orange.gif" alt="Apple vs Orange" /></p>
<p><span id="more-820"></span></p>
<p>Image recognition is a very hot topic these days in the AI/ML landscape. <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> really shines in this task and can achieve almost perfect accuracy on many scenarios.</p>
<p>Sadly, you can't run CNN on your ESP32, they're just too large for a microcontroller.</p>
<p>Since in this series about <a href="/category/programming/arduino-machine-learning/">Machine Learning on Microcontrollers</a> we're exploring the potential of Support Vector Machines (SVMs) at solving different classification tasks, we'll take a look into image classification too.</p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocwhat-were-going-to-do">What we're going to do</a><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocextracting-rgb-components">Extracting RGB components</a><li><a href="#tocrecord-samples-image">Record samples image</a><li><a href="#toctraining-the-classifier">Training the classifier</a><li><a href="#tocreal-world-example">Real world example</a><ol><li><a href="#tocdisclaimer">Disclaimer</a></ol></div></p>
<h2 id="tocwhat-were-going-to-do">What we're going to do</h2>
<p>In a previous post about <a href="/2019/12/color-identification-on-arduino/">color identification with Machine learning</a>, we used an Arduino to detect the object we were pointing at with a color sensor (TCS3200) by its color: if we detected yellow, for example, we knew we had a banana in front of us.</p>
<p>Of course such a process is not object recognition at all: yellow may be a banane, or a lemon, or an apple.</p>
<p>Object inference, in that case, works only if you have exactly one object for a given color.</p>
<p>The objective of this post, instead, is to investigate if we can use the MicroML framework to do simple image recognition on the images from an ESP32 camera.</p>
<p>This is much more similar to the tasks you do on your PC with CNN or any other form of NN you are comfortable with. Sure, we will still apply some restrictions to fit the problem on a microcontroller, but this is a huge step forward compared to the simple color identification.</p>
<div class="watchout">
In this context, image recognition means deciding which class (from the trained ones) the current image belongs to. <b>This algorithm can't locate interesting objects in the image, neither detect if an object is present in the frame</b>. It will classify the current image based on the samples recorded during training.
</div>
<p>As any beginning machine learning project about image classification worth of respect, our task will be to distinguish an orange from an apple.</p>
<h2 id="tocfeatures-definition">Features definition</h2>
<p>I have to admit that I rarely use NN, so I may be wrong here, but from the examples I read online it looks to me that features engineering is not a fundamental task with NN.</p>
<p>Those few times I used CNN, I always used the whole image as input, <em>as-is</em>. I didn't extracted any feature from them (e.g. color histogram): the CNN worked perfectly fine with raw images.</p>
<p>I don't think this will work best with SVM, but in this first post we're starting as simple as possible, so we'll be using the RGB components of the image as our features. In a future post, we'll introduce additional features to try to improve our results.</p>
<p>I said we're using the RGB components of the image. But not all of them.</p>
<p>Even at the lowest resolution of 160x120 pixels, a raw RGB image from the camera would generate 160x120x3 = 57600 features: way too much.</p>
<p>We need to reduce this number  to the bare minimum.</p>
<p>How much pixels do you think are necessary to get reasonable results in this task of classifying apples from oranges?</p>
<p>You would be surprised to know that I got 90% accuracy with an RGB image of <strong>8x6</strong>!</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/01/Orange-and-Apple-pixelated.jpg" alt="You actually need very few pixels to do image classification" /></p>
<p>Yes, that's all we really need to do a <em>good enough</em> classification.</p>
<hr /><p><em>You can distinguish apples from oranges on ESP32 with 8x6 pixels only!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Fimage-recognition-with-esp32-and-arduino%2F&#038;text=You%20can%20distinguish%20apples%20from%20oranges%20on%20ESP32%20with%208x6%20pixels%20only%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p>Of course this is a tradeoff: you can't expect to achieve 99% accuracy while mantaining the model size small enough to fit on a microcontroller. 90% is an acceptable accuracy for me in this context.</p>
<p>You have to keep in mind, moreover, that the features vector size grows quadratically with the image size (if you keep the aspect ratio). A raw RGB image of 8x6 generates 144 features: an image of 16x12 generates 576 features. This was already causing random crashes on my ESP32.</p>
<p>So we'll stick to 8x6 images.</p>
<p>Now, how do you compact a 160x120 image to 8x6? With <em>downsampling</em>.</p>
<p>This is the same tecnique we've used in the post about <a href="/2020/01/motion-detection-with-esp32-cam-only-arduino-version/">motion detection on ESP32</a>: we define a block size and average all the pixels inside the block to get a single value (you can refer to that post for more details).</p>
<p><img src="/wp-content/uploads/2020/01/Image-downsampling-example.jpg" alt="Image downsampling example" /></p>
<p>This time, though, we're working with RGB images instead of grayscale, so we'll repeat the exact same process 3 times, one for each channel.</p>
<p>This is the code excerpt that does the downsampling.</p>
<pre><code class="language-cpp">uint16_t rgb_frame[HEIGHT / BLOCK_SIZE][WIDTH / BLOCK_SIZE][3] = { 0 };

void grab_image() {
    for (size_t i = 0; i &lt; len; i += 2) {
        // get r, g, b from the buffer
        // see later

        const size_t j = i / 2;
        // transform x, y in the original image to x, y in the downsampled image
        // by dividing by BLOCK_SIZE
        const uint16_t x = j % WIDTH;
        const uint16_t y = floor(j / WIDTH);
        const uint8_t block_x = floor(x / BLOCK_SIZE);
        const uint8_t block_y = floor(y / BLOCK_SIZE);

        // average pixels in block (accumulate)
        rgb_frame[block_y][block_x][0] += r;
        rgb_frame[block_y][block_x][1] += g;
        rgb_frame[block_y][block_x][2] += b;
    }
}</code></pre>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the (more than) monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2 id="tocextracting-rgb-components">Extracting RGB components</h2>
<p>The ESP32 camera can store the image in different formats (of our interest â€” there are a couple more available):</p>
<ol>
<li><strong>grayscale</strong>: no color information, just the intensity is stored. The buffer has size HEIGHT*WIDTH</li>
<li><strong>RGB565</strong>: stores each RGB pixel in two bytes, with 5 bit for red, 6 for green and 5 for blue. The buffer has size HEIGHT * WIDTH * 2</li>
<li><strong>JPEG</strong>: encodes (in hardware?) the image to jpeg. The buffer has a variable length, based on the encoding results</li>
</ol>
<p>For our purpose, we'll use the RGB565 format and extract the 3 components from the 2 bytes with the following code.</p>
<p><img src="https://s2.www.theimagingsource.com/application-1.1.29/documentation/ic_imaging_control_class/en_US/images/rgb565.gif" alt="taken from https://www.theimagingsource.com/support/documentation/ic-imaging-control-cpp/PixelformatRGB565.htm" /></p>
<pre><code class="language-cpp">config.pixel_format = PIXFORMAT_RGB565;

for (size_t i = 0; i &lt; len; i += 2) {
    const uint8_t high = buf[i];
    const uint8_t low  = buf[i+1];
    const uint16_t pixel = (high &lt;&lt; 8) | low;

    const uint8_t r = (pixel &amp; 0b1111100000000000) &gt;&gt; 11;
    const uint8_t g = (pixel &amp; 0b0000011111100000) &gt;&gt; 6;
    const uint8_t b = (pixel &amp; 0b0000000000011111);
}</code></pre>
<h2 id="tocrecord-samples-image">Record samples image</h2>
<p>Now that we can grab the images from the camera, we'll need to take a few samples of each object we want to racognize.</p>
<p>Before doing so, we'll linearize the image matrix to a 1-dimensional vector, because that's what our prediction function expects.</p>
<pre><code class="language-cpp">#define H (HEIGHT / BLOCK_SIZE)
#define W (WIDTH / BLOCK_SIZE)

void linearize_features() {
  size_t i = 0;
  double features[H*W*3] = {0};

  for (int y = 0; y &lt; H; y++) {
    for (int x = 0; x &lt; W; x++) {
      features[i++] = rgb_frame[y][x][0];
      features[i++] = rgb_frame[y][x][1];
      features[i++] = rgb_frame[y][x][2];
    }
  }

  // print to serial
  for (size_t i = 0; i &lt; H*W*3; i++) {
    Serial.print(features[i]);
    Serial.print(&#039;\t&#039;);
  }

  Serial.println();
}</code></pre>
<p>Now you can setup your acquisition environment and take the samples: 15-20 of each object will do the job.</p>
<div class="watchout">
Image acquisition is a very noisy process: even keeping the camera still, you will get fluctuating values. <br />You need to be very accurate during this phase if you want to achieve good results.<br /> I suggest you immobilize your camera with tape to a flat surface or use some kind of photographic easel.
</div>
<h2 id="toctraining-the-classifier">Training the classifier</h2>
<p>To train the classifier, save the features for each object in a file, one features vector per line. Then follow the steps on <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn">how to train a ML classifier for Arduino</a> to get the exported model.</p>
<p>You can experiment with different classifier configurations. </p>
<p>My features were well distinguishable, so I had great results (100% accuracy) with any kernel (even linear).</p>
<p>One odd thing happened with the RBF kernel: I had to use an extremely low gamma value (0.0000001). Does anyone can explain me why? I usually go with a default value of 0.001.</p>
<p>The model produced 13 support vectors.</p>
<p>I did no features scaling: you could try it if classifying more than 2 classes and having poor results.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange-decision-boundaries.png" alt="Apple vs Orange decision boundaries" /></p>
<h2 id="tocreal-world-example">Real world example</h2>
<p>If you followed all the steps above, you should now have a model capable of detecting if your camera is shotting an apple or an orange, as you can see in the following video.</p>
<div style="width: 788px;" class="wp-video"><video class="wp-video-shortcode" id="video-820-2" width="788" height="443" preload="metadata" controls="controls"><source type="video/mp4" src="https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4?_=2" /><a href="https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4">https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4</a></video></div>
<p></p>
<p>The little white object you see at the bottom of the image is the camera, taped to the desk.</p>
<p>Did you think it was possible to do simple image classification on your ESP32?</p>
<h3 id="tocdisclaimer">Disclaimer</h3>
<p>This is not full-fledged object recognition: it can't label objects while you walk as Tensorflow can do, for example.</p>
<p>You have to carefully craft your setup and be as consistent as possible between training and inferencing.</p>
<p>Still, I think this is a fun proof-of-concept that can have useful applications in simple scenarios where you can live with a fixed camera and don't want to use a full Raspberry Pi.</p>
<p>In the next weeks I settled to finally try TensorFlow Lite for Microcontrollers on my ESP32, so I'll try to do a comparison between them and this example and report my results.</p>
<p>Now that you can do image classification on your ESP32, can you think of a use case you will be able to apply this code to? </p>
<p>Let me know in the comments, we could even try realize it together if you need some help.</p>
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/Apple_vs_Orange/Apple_vs_Orange.ino" target="_blank" rel="noopener noreferrer">Github</a></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/01/image-recognition-with-esp32-and-arduino/">Apple or Orange? Image recognition with ESP32 and Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://eloquentarduino.github.io/wp-content/uploads/2020/01/Apple-vs-Orange.mp4" length="1642079" type="video/mp4" />

			</item>
		<item>
		<title>Embedded Machine learning on Attiny85</title>
		<link>https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Mon, 23 Dec 2019 16:57:15 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=688</guid>

					<description><![CDATA[<p>You won't believe it, but you can run Machine learning on embedded systems like an Attiny85 (and many others Attiny)! When I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/">Embedded Machine learning on Attiny85</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>You won't believe it, but <strong>you can run Machine learning on embedded systems like an Attiny85</strong> (and many others Attiny)!</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/attiny85_ml.png" alt="ri-elaborated from https://cyaninfinite.com/miniaturize-projects-with-attiny85/" /></p>
<p><span id="more-688"></span></p>
<p>When I first run a Machine learning project on my Arduino Nano (old generation), it already felt a big achievement. I mean, that board has only 32 Kb of program space and 2 Kb of RAM and you can buy a chinese clone for around 2.50 $.</p>
<p>It already opened the path to a embedded machine learning at a new scale, given the huge amount of microcontrollers ready to become &quot;intelligent&quot;. </p>
<p>But it was not enough for me: after all, the <a href="/2019/11/how-to-create-a-classifier-for-arduino-machine-learning-projects/">MicroML generator</a> exports plain C that should run on any embedded system, not only on Arduino boards.</p>
<p>So I setup to test if I could go even smaller and run it on the #1 of tiny chips: the Attiny85.</p>
<hr /><p><em>MicroML exports plain C that could run anywhere, not only on Arduino boards.</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=MicroML%20exports%20plain%20C%20that%20could%20run%20anywhere%2C%20not%20only%20on%20Arduino%20boards.&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p>No, I couldn't.</p>
<p>The generated code makes use of a variadic function, which seems to not be supported by the Attiny compiler in the Arduino IDE.</p>
<p>So I had to come up with an alternative implementation to make it work.</p>
<p>Fortunately I already experimented with a non-variadic version when first writing the porter, so it was a matter of refreshing that algorithm and try it out.</p>
<p>Guess what? It compiled!</p>
<p>So I tried porting one my earliear tutorial (the <a href="/2019/12/how-to-do-color-identification-on-arduino/">color identification</a> one) to the Attiny and...</p>
<hr /><p><em>Boom! Machine learning on an Attiny85!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=Boom%21%20Machine%20learning%20on%20an%20Attiny85%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p>Here's a step-by-step tutorial on how you can do it too.</p>
<div class="watchout">
I strongly suggest you read <a href="/2019/12/how-to-do-color-identification-on-arduino/" target="_blank" rel="noopener noreferrer">the original tutorial</a> before following this one, because I won't go into too much details on the common steps here.
</div>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><li><a href="#toctrain-and-export-the-svm-classifier">Train and export the SVM classifier</a><li><a href="#tocrun-the-inference">Run the inference</a><li><a href="#tocproject-figures">Project figures</a></ol></div></p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>We're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be 3-dimensional, which leads to a really simple model with very high accuracy.</p>
<div class="watchout">
The Attiny85 has 8 Kb of flash and 512 bytes of RAM, so you won't be able to load any model that uses more than a few features (probably less than 10).
</div>
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<p>You must do this step on a board with a Serial interface, like an Arduino Uno / Nano / Pro Mini. See <a href="/2019/12/how-to-do-color-identification-on-arduino/" target="_blank" rel="noopener noreferrer">the original tutorial</a> for the code of this step.</p>
<h2 id="toctrain-and-export-the-svm-classifier">3. Train and export the SVM classifier</h2>
<p>This part is exactly the same as the original, except for a single parameter: you will pass <code>platform=attiny</code> to the <code>port</code> function.</p>
<pre><code class="language-python">from sklearn.svm import SVC
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features(&#039;dataset/&#039;)
X, y = features[:, :-1], features[:, -1]
classifier = SVC(kernel=&#039;linear&#039;).fit(X, y)
c_code = port(classifier, classmap=classmap, platform=&#039;attiny&#039;)
print(c_code)</code></pre>
<div class="watchout">
The Attiny mode has been implemented in version 0.8 of micromlgen: if you installed an earlier version, first update
</div>
<p>At this point you have to copy the printed code and import it in your project, in a file called <code>model.h</code>.</p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2 style="margin: 0; text-align: center">Finding this content useful?</h2>
<div class="mc-field-group">
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="join the (more than) monthly newsletter">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f0eaedd94d554cf2ee781742a_37d3496031" tabindex="-1" value=""></div>
    <div class="clear" style="position: relative; top: 8px"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<p>Since we don't have a Serial, we will blink a LED a number of times dependant on the prediction result.</p>
<pre><code class="language-cpp">#include &quot;model.h&quot;

#define LED 0

void loop() {
  readRGB();
  classify();
  delay(1000);
}

void classify() {
    for (uint8_t times = predict(features) + 1; times &gt; 0; times--) {
        digitalWrite(LED, HIGH);
        delay(10);
        digitalWrite(LED, LOW);
        delay(10);
    }
}</code></pre>
<p>Here we are: put some colored object in front of the sensor and see the LED blink.</p>
<h2 id="tocproject-figures">Project figures</h2>
<p>On my machine, the sketch requires 3434 bytes (41%) of program space and 21 bytes (4%) of RAM. This means you could actually run machine learning in even less space than what the Attiny85 provides. </p>
<p>This model in particular it's so tiny you <strong>can run in even on an Attiny45, which has only 4 Kb of flash and 256 bytes of RAM</strong>.</p>
<p>I'd like you to look at the RAM figure for a moment: <strong>21 bytes</strong>. 21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller. This is the result of the implementation I chose: the least RAM overhead possible. I challenge you to go any lower than this.</p>
<hr /><p><em>21 bytes is all the memory you need to run a Machine learning algorithm on a microcontroller</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fmachine-learning-on-attiny85%2F&#038;text=21%20bytes%20is%20all%20the%20memory%20you%20need%20to%20run%20a%20Machine%20learning%20algorithm%20on%20a%20microcontroller&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlColorIdentificationAttinyExample/MicromlColorIdentificationAttinyExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/machine-learning-on-attiny85/">Embedded Machine learning on Attiny85</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Word classification using Arduino and MicroML</title>
		<link>https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 22 Dec 2019 18:12:59 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=180</guid>

					<description><![CDATA[<p>In this Arduno Machine learning tutorial we're going to use a microphone to identify the word you speak. This is going to run on an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM. In this project the features are going to be the Fast Fourier Transform of [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/">Word classification using Arduino and MicroML</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this Arduno Machine learning tutorial we're going to use a microphone to identify the word you speak.<br />
This is going to run on an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg" alt="from https://www.udemy.com/course/learn-audio-processing-complete-engineers-course/" /></p>
<p><span id="more-180"></span></p>
<p>In this project the features are going to be the Fast Fourier Transform of 50 analog readings from a microphone, taken starting from when a loud sound is detected, sampled at intervals of 5 millis.</p>
<div class="watchout">
This tutorial is not about "Wake word" detection: it can't distinguish a known word from any other word. It can classify the word you speak among the ones you trained it to recognize!!!
</div>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><ol><li><a href="#toctranslate-the-raw-values">Translate the raw values</a><li><a href="#tocdetect-sound">Detect sound</a><li><a href="#tocrecord-the-words">Record the words</a><li><a href="#tocfast-fourier-transform">Fast Fourier Transform</a></li></ol><li><a href="#toctrain-and-export-the-classifier">Train and export the classifier</a><ol><li><a href="#tocselect-a-suitable-model">Select a suitable model</a></li></ol><li><a href="#tocrun-the-inference">Run the inference</a></ol></div></p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>The microphone we're going to use is a super simple device: it produces an analog signal (0-1024) based on the sound it detects. </p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord-Sensor.jpg" alt="from http://arduinolearning.com/code/ky038-microphone-module-and-arduino-example.php" /></p>
<p>When working with audio you almost always don't want to use raw readings, since they're hardly useful. Instead you often go with <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier Transform</a>, which extracts the frequency information from a time signal. That's going to become our features vector: let's see how in the next step.</p>
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<p>First of all, we start with raw audio data. The following plot is me saying random words.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Raw-audio.gif" alt="Raw audio stream" /></p>
<pre><code class="language-cpp">#define MIC A0
#define INTERVAL 5

void setup() {
    Serial.begin(115200);
    pinMode(MIC, INPUT);
}

void loop() {
    Serial.println(analogRead(MIC));
    delay(INTERVAL);
}</code></pre>
<h3 id="toctranslate-the-raw-values">2.1 Translate the raw values</h3>
<p>For the Fourier Transform to work, we need to provide as input an array of values both positive and negative. <code>analogRead()</code> is returning only positive values, tough, so we need to translate them.</p>
<pre><code class="language-cpp">int16_t readMic() {
    // this translated the analog value to a proper interval
    return  (analogRead(MIC) - 512) &gt;&gt; 2;
}</code></pre>
<h3 id="tocdetect-sound">2.2 Detect sound</h3>
<p>As in the <a href="/2019/12/how-to-do-gesture-identification-on-arduino/">tutorial about gesture classification</a>, we'll start recording the features when a word is beginning to be pronounced. Also in this project we'll use a threshold to detect the start of a word.</p>
<p>To do this, we first record a &quot;background&quot; sound level, that is the value produced by the sensor when we're not talking at all.</p>
<pre><code class="language-cpp">float backgroundSound = 0;

void setup() {
    Serial.begin(115200);
    pinMode(MIC, INPUT);
    calibrate();
}

void calibrate() {
    for (int i = 0; i &lt; 200; i++)
        backgroundSound += readMic();

    backgroundSound /= 200;

    Serial.print(&quot;Background sound level is &quot;);
    Serial.println(backgroundSound);
}</code></pre>
<p>At this point we can check for the starting of a word when the detected sound level exceeds tha background one by a given threshold.</p>
<pre><code class="language-cpp">// adjust as per your need
// it will depend on the sensitivity of you microphone
#define SOUND_THRESHOLD 3

void loop() {
    if (!soundDetected()) {
        delay(10);
        return;
    }
}

bool soundDetected() {
    return abs(read() - backgroundSound) &gt;= SOUND_THRESHOLD;
}</code></pre>
<h3 id="tocrecord-the-words">2.3 Record the words</h3>
<p>As for the gestures, we'll record a fixed number of readings at a fixed interval.<br />
Here a tradeoff arises: you want to have a decent number of readings to be able to accurately describe the words you want to classify, but not too much otherwise your model is going to be too large to fit in your board.</p>
<p>I made some experiments, and I got good results with 32 samples at 5 millis interval, which covers ~150 ms of speech.</p>
<div class="watchout">
The dilemma here is that the Fourier Transform to work needs a number of samples that is a power of 2. So, if you think 32 features are not enough for you, you're forced to go with at least 64: this has a REALLY bad impact on the model size.
</div>
<pre><code class="language-cpp">#define NUM_SAMPLES 32
#define INTERVAL 5

double features[NUM_SAMPLES];

void loop() {
    if (!soundDetected()) {
        delay(10);
        return;
    }

    captureWord();
    printFeatures();
    delay(1000);
}

void captureWord() {
    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {
        features[i] = readMic();
        delay(INTERVAL);
    }
}</code></pre>
<pre><code class="language-cpp">
void printFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);
    
    for (int i = 0; i &lt; numFeatures; i++) {
        Serial.print(features[i]);
        Serial.print(i == numFeatures - 1 ? 'n' : ',');
    }
}
</code></pre>
<h3 id="tocfast-fourier-transform">2.4 Fast Fourier Transform</h3>
<p>Here we are with the Fourier Transform. When implemented in software, the most widely implementation of the FT is actually called <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform (FFT)</a>, which is - as you may guess - a fast implementation of the FT. </p>
<p>Luckily for us, there exists <a href="https://github.com/kosme/arduinoFFT">a library</a> for Arduino that does FFT.</p>
<p>And is so easy to use that we only need a line to get usable results!</p>
<pre><code class="language-cpp">#include &lt;arduinoFFT.h&gt;

arduinoFFT fft;

void captureWord() {
    for (uint16_t i = 0; i &lt; NUM_SAMPLES; i++) {
        features[i] = readMic();
        delay(INTERVAL);
    }

    fft.Windowing(features, NUM_SAMPLES, FFT_WIN_TYP_HAMMING, FFT_FORWARD);
}</code></pre>
<p>You don't need to know what the <code>Windowing</code> function actually does (I don't either): what matters is that it extracts meaningful informations from our signal. Since it overwrites the features array, after calling that line we have what we need to input to our classifier.</p>
<p>At this point, record 10-15 samples for each word and save them to a file, one for each word.</p>
<div class="watchout">
After you have recorded the samples for a word, I suggest you to manually check them. It is sufficient to look at the first 3 values: if one of them seems to be clearly out of range, I suggest you to delete it. You will lose some accuracy, but your model will be smaller.
</div>
<h2 id="toctrain-and-export-the-classifier">3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<p>In this project on Machine learning we're not achieving 100% accuracy easily.<br />
Audio is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-linear-kernel-87-accuracy.png" alt="Decision boundaries of 2 PCA components of Word classification, linear kernel" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-poly-3-kernel-91-accuracy.png" alt="Decision boundaries of 2 PCA components of Word classification, poly-3 kernel" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Word-classification-rbf-kernel-94-accuracy.png" alt="Decision boundaries of 2 PCA components of Word classification, rbf kernel" /></p>
<h3 id="tocselect-a-suitable-model">2.5 Select a suitable model</h3>
<p>Here's an overview table of the 3 tests I did.</p>
<table>
<thead>
<tr>
<th>Kernel</th>
<th style="text-align: center;">No. support vectors</th>
<th style="text-align: center;">Avg. accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">87%</td>
</tr>
<tr>
<td>Poly 3</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">91%</td>
</tr>
<tr>
<td>RBF</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">94%</td>
</tr>
</tbody>
</table>
<p>Of course the one with the RBF kernel would be the most desiderable since it has a very high accuracy: 36 support vectors, tough, will produce a model too large to fit on an Arduino Nano.</p>
<p>So you're forced to pick the one with the highest accuracy that fit on your board: in my case it was the Linear kernel one.</p>
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<pre><code class="language-cpp">#include &quot;model.h&quot;

void loop() {
    if (!soundDetected()) {
        delay(10);
        return;
    }

    captureWord();
    Serial.print(&quot;You said &quot;);
    Serial.println(classIdxToName(predict(features)));

    delay(1000);
}</code></pre>
<p>And that's it: word classification through machine learning on your Arduino board! Say some word and see the classification result on the Serial monitor. </p>
<p>Here's me testing the system (English is not my language, so forgive my bad pronounce). The video quality is very low, I know, but you get the point.</p>
<div style="width: 640px;" class="wp-video"><video class="wp-video-shortcode" id="video-180-3" width="640" height="352" preload="metadata" controls="controls"><source type="video/mp4" src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4?_=3" /><a href="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4">https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4</a></video></div>
<div class="watchout">
As you can hear from the video, you should be quite accurate when pronouncing the words. I have to admit there are cases where the system totally fails to classify correctly the words. Restarting helps most of the time, so I'm suspecting there could be some kind of leak that "corrupts" the inference procedure.
</div>
<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlWakeWordIdentificationExample/MicromlWakeWordIdentificationExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/word-classification-using-arduino/">Word classification using Arduino and MicroML</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Word-classification.mp4" length="2055653" type="video/mp4" />

			</item>
		<item>
		<title>Wifi indoor positioning using Arduino and Machine Learning in 4 steps</title>
		<link>https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Fri, 20 Dec 2019 17:31:16 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=224</guid>

					<description><![CDATA[<p>In this Arduno Machine learning project we're going to use the nearby WiFi access points to locate where we are. For this project to work you will need a Wifi equipped board, such as ESP8266 or ESP32. I published an upgraded version of this project, with all the code and tools you need: check it [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/">Wifi indoor positioning using Arduino and Machine Learning in 4 steps</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this Arduno Machine learning project we're going to use the nearby WiFi access points to locate where we are. For this project to work you will need a Wifi equipped board, such as ESP8266 or ESP32.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/illustrations_ambient-wifi-site-survey2.jpg" alt="Wifi indoor positioning @ ri-elaborated from https://www.accuware.com/blog/ambient-signals-plus-video-images/" /></p>
<p><span id="more-224"></span></p>
<div class="watchout">
I published an upgraded version of this project, with all the code and tools you need: <a href="/2020/08/the-ultimate-guide-to-wifi-indoor-positioning-using-arduino-and-machine-learning/"><b>check it out</b></a></div>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocwhat-is-wifi-indoor-positioning">What is Wifi indoor positioning?</a><ol><li><a href="#tocwhat-will-we-use-it-for">What will we use it for?</a><li><a href="#tocwhat-you-need">What you need</a><li><a href="#tochow-it-works">How it works</a></li></ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><ol><li><a href="#tocenumerate-the-access-points">Enumerate the access points</a><li><a href="#toccreate-an-access-point-array">Create an access point array</a><li><a href="#tocconvert-to-features-vector">Convert to features vector</a></li></ol><li><a href="#toctrain-and-export-the-classifier">Train and export the classifier</a><li><a href="#tocrun-the-inference">Run the inference</a></ol></div></p>
<h2 id="tocwhat-is-wifi-indoor-positioning">What is Wifi indoor positioning?</h2>
<p>We all are used to GPS positioning: our device will use satellites to track our position on Earth. GPS works very well and with a very high accuracy (you can expect only a few meters of error).</p>
<p>But it suffers a problem: it needs <em>Line of Sight</em> (a clear path from your device to the satellites). If you're not in an open place, like inside a building, you're out of luck.</p>
<p>The task of detecting where you are when GPS localization is not an option is called <a href="https://en.wikipedia.org/wiki/Indoor_positioning_system">indoor positioning</a>: it could be in a building, an airport, a parking garage. </p>
<p>There are lots of different approaches to this task (Wikipedia lists more than 10 of them), each with a varying level of commitment, difficulty, cost and accuracy.</p>
<p>For this tutorial I opted for one that is both cost-efficient, easy to implement and readily available in most of the locations: <strong>WiFi indoor positioning</strong>.</p>
<h3 id="tocwhat-will-we-use-it-for">What will we use it for?</h3>
<p>In this tutorial about Machine learning on Arduino we're going to use Wifi indoor positioning to detect in which room of our house / office we are. This is the most basic task we can accomplish and will get us a feeling of level of accuracy we can achieve with such a simple setup.</p>
<p>On this basis, we'll construct more sophisticated projects in future posts.</p>
<h3 id="tocwhat-you-need">What you need</h3>
<p>To accomplish this tutorial, you really need 2 things:</p>
<ol>
<li>a WiFi equipped board (ESP8266, ESP32, Arduino MKR WiFi 1010...)</li>
<li>be in a place with a few WiFi networks around</li>
</ol>
<p>If you're doing this at home or in your office, there's a good change your neighbours have WiFi networks in their apartments you can leverage. If you live in an isolated contryside, sorry, this will not work for you.</p>
<h3 id="tochow-it-works">How it works</h3>
<p>So, how exactly does Wifi indoor positioning works in conjuction with Machine learning? </p>
<p>Let's pretend there are 5 different WiFi networks around you, like in the picture below.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Wifi1.png" alt="Wifi indoor positioning example" /></p>
<p>As you can see there are two markers on the map: each of these markers will &quot;see&quot; different networks, with different <em>signal strengths</em> (a.k.a <a href="https://en.wikipedia.org/wiki/Received_signal_strength_indication">RSSI</a>).</p>
<p>As you move around, those numbers will change: each room will be identified by the unique combination of the RSSIs.</p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>The features for this project are going to be the RSSIs (Received signal strength indication) of the known WiFi networks. If a network is out of range, it will have an RSSI equal to 0.</p>
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<p>Before actually recording the sample data to train our classifier, we need to do some preliminary work. This is because not all networks will be visible all the time:  we have to work, however, with a fixed number of features.</p>
<h3 id="tocenumerate-the-access-points">2.1 Enumerate the access points</h3>
<p>First of all we need to enumerate all the networks we will encounter during the inference process. </p>
<p>To begin, we take a &quot;reconnaissance tour&quot; of the locations we want to predict and log all the networks we detect. Load the following sketch and take note of all the networks that appear on the Serial monitor.</p>
<pre><code class="language-cpp">#include &lt;WiFi.h&gt;

void setup() {
    Serial.begin(115200);
    WiFi.mode(WIFI_STA);
    WiFi.disconnect();
}

void loop() {
  int numNetworks = WiFi.scanNetworks();

  for (int i = 0; i &lt; numNetworks; i++) {
      Serial.println(WiFi.SSID(i));

  delay(3000);
}</code></pre>
<h3 id="toccreate-an-access-point-array">2.2 Create an access point array</h3>
<p>Now that we have a bunch of SSIDs, we need to assign each SSID to a fixed index, from 0 to <code>MAX_NETWORKS</code>.</p>
<p>You can implement this part as you like, but in this demo I'll make use of a class I wrote called <code>Array</code> (you can see the <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/src/data_structures/Array.h">source code</a> and <a href="https://github.com/agrimagsrl/eloquentarduino/blob/master/examples/ArrayExample/ArrayExample.ino">example</a> on Github), which implements 2 useful functions: </p>
<ol>
<li><code>push()</code> to add an element to the array</li>
<li><code>indexOf()</code> to get the index of an element.</li>
</ol>
<p>See <a href="/2019/12/how-to-install-the-eloquent-library/">how to install the Eloquent library</a> if you don't have it already installed.<br />
At this point we populate the array with all the networks we saved from the reconnaissance tour.</p>
<pre><code class="language-cpp">#include &lt;eDataStructures.h&gt;

#define MAX_NETWORKS 10

using namespace Eloquent::DataStructures;

double features[MAX_NETWORKS];
Array&lt;String, MAX_NETWORKS&gt; knownNetworks(&quot;&quot;);

void setup() {
    Serial.begin(115200);
    WiFi.mode(WIFI_STA);
    WiFi.disconnect();

    knownNetworks.push(&quot;SSID #0&quot;);
    knownNetworks.push(&quot;SSID #1&quot;);
    knownNetworks.push(&quot;SSID #2&quot;);
    knownNetworks.push(&quot;SSID #3&quot;);
    // and so on
}</code></pre>
<h3 id="tocconvert-to-features-vector">2.3 Convert to features vector</h3>
<p>The second step is to convert the scan results into a features vector. Each feature will be the RSSI of the given SSID, in the exact order we populated the <code>knownNetworks</code> array.</p>
<p>In practice:</p>
<pre><code class="language-cpp">features[0] == RSSI of SSID #0;
features[1] == RSSI of SSID #1;
features[2] == RSSI of SSID #2;
features[3] == RSSI of SSID #3;
// and so on</code></pre>
<p>The code below will do the job.</p>
<pre><code class="language-cpp">void loop() {
    scan();
    printFeatures();
    delay(3000);
}

void scan() {
    int numNetworks = WiFi.scanNetworks();

    resetFeatures();

    // assign RSSIs to feature vector
    for (int i = 0; i &lt; numNetworks; i++) {
        String ssid = WiFi.SSID(i);
        uint16_t networkIndex = knownNetworks.indexOf(ssid);

        // only create feature if the current SSID is a known one
        if (!isnan(networkIndex))
            features[networkIndex] = WiFi.RSSI(i);
    }
}

// reset all features to 0
void resetFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);

    for (int i = 0; i &lt; numFeatues; i++)
        features[i] = 0;
}</code></pre>
<pre><code class="language-cpp">
void printFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);
    
    for (int i = 0; i &lt; numFeatures; i++) {
        Serial.print(features[i]);
        Serial.print(i == numFeatures - 1 ? 'n' : ',');
    }
}
</code></pre>
<p>Grab some recordings just staying in a location for a few seconds and save the serial output to a file; then move to the next location and repeat: 10-15 samples for each location will suffice.</p>
<p>If you do a good job, you should end with distinguible features, as show in the plot below.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-from-Wifi-indoor-positioning-features.png" alt="Decision boundaries of 2 PCA components from Wifi indoor positioning features" /></p>
<div class="watchout">
RSSIs may be a little noisy, mostly on the boundaries where weak networks may appear and disappear with a very low RSSI: this was not a problem for me, but if you're getting bad results you may filter out those low values.</p>
<pre><code class="language-cpp">
// replace
features[networkIndex] = WiFi.RSSI(i);

// with
#define MIN_RSSI -90 // adjust to your needs

features[networkIndex] = WiFi.RSSI(i) > MIN_RSSI ? WiFi.RSSI(i) : 0;
</code></pre>
</div>
<h2 id="toctrain-and-export-the-classifier">3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<pre><code class="language-cpp">#include &quot;model.h&quot;

void loop() {
    scan();
    classify();
    delay(3000);
}

void classify() {
    Serial.print(&quot;You are in &quot;);
    Serial.println(classIdxToName(predict(features)));
}</code></pre>
<p>Move around your house/office/whatever and see your location printed on the serial monitor!</p>
<p><br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlWifiIndoorPositioningExample/MicromlWifiIndoorPositioningExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/wifi-indoor-positioning-on-arduino/">Wifi indoor positioning using Arduino and Machine Learning in 4 steps</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>How to do Gesture identification through machine learning on Arduino</title>
		<link>https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Thu, 19 Dec 2019 13:25:46 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=35</guid>

					<description><![CDATA[<p>In this Arduno Machine learning project we're going to use an accelerometer sensor to identify the gestures you play. This is a remake of the project found on the Tensorflow blog. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of flash [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/">How to do Gesture identification through machine learning on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this Arduno Machine learning project we're going to use an accelerometer sensor to identify the gestures you play.<br />
This is a remake of the project found on the <a href="https://blog.tensorflow.org/2019/11/how-to-get-started-with-machine.html">Tensorflow blog</a>. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.001-gamma.png" alt="Decision boundaries, 99% accuracy" /></p>
<p><span id="more-35"></span></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><ol><li><a href="#tocread-the-imu-sensor">Read the IMU sensor</a><li><a href="#toccalibration">Calibration</a><li><a href="#tocdetect-first-motion">Detect first motion</a><li><a href="#tocrecord-features">Record features</a></li></ol><li><a href="#toctrain-and-export-the-classifier">Train and export the classifier</a><ol><li><a href="#tocselect-a-suitable-model">Select a suitable model</a></li></ol><li><a href="#tocrun-the-inference">Run the inference</a></ol></div></p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>We're going to use the accelerations along the 3 axis (X, Y, Z) coming from an <a href="https://en.wikipedia.org/wiki/Inertial_measurement_unit">IMU</a> to infer which gesture we're playing. We'll use a fixed number of recordings (<code>NUM_SAMPLES</code>) starting from the first detection of movement. </p>
<p>This means our feature vectors are going to be of dimension <code>3 * NUM_SAMPLES</code>, which can become too large to fit in the memory of the Arduino Nano. We'll start with a low value for <code>NUM_SAMPLES</code> to keep it as leaner as possible: if your classifications suffer from poor accuracy, you can increase this number.</p>
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<h3 id="tocread-the-imu-sensor">2.1 Read the IMU sensor</h3>
<p>First of all, we need to read the raw data from the IMU. This piece of code will be different based on the specific chip you use. To keep things consistent, we'll wrap the IMU logic in 2 functions: <code>imu_setup</code> and <code>imu_read</code>. </p>
<p>I'll report a couple of example implementations for the <code>MPU6050</code> and the <code>MPU9250</code> (these are the chip I have at hand). You should save whichever code you use in a file called <code>imu.h</code>. </p>
<pre><code class="language-cpp">#include &quot;Wire.h&quot;
// library from https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050
#include &quot;MPU6050.h&quot;
#define OUTPUT_READABLE_ACCELGYRO

MPU6050 imu;

void imu_setup() {
    Wire.begin();
    imu.initialize();
}

void imu_read(float *ax, float *ay, float *az) {
    int16_t _ax, _ay, _az, _gx, _gy, _gz;

    imu.getMotion6(&amp;_ax, &amp;_ay, &amp;_az, &amp;_gx, &amp;_gy, &amp;_gz);

    *ax = _ax;
    *ay = _ay;
    *az = _az;
}</code></pre>
<pre><code class="language-cpp">#include &quot;Wire.h&quot;
// library from https://github.com/bolderflight/MPU9250
#include &quot;MPU9250.h&quot;

MPU9250 imu(Wire, 0x68);

void imu_setup() {
    Wire.begin();
    imu.begin();
}

void imu_read(float *ax, float *ay, float *az) {
    imu.readSensor();

    *ax = imu.getAccelX_mss();
    *ay = imu.getAccelY_mss();
    *az = imu.getAccelZ_mss();
}</code></pre>
<p>In the main .ino file, we dump the values to Serial monitor / plotter.</p>
<pre><code class="language-cpp">#include &quot;imu.h&quot;

#define NUM_SAMPLES 30
#define NUM_AXES 3
// sometimes you may get &quot;spikes&quot; in the readings
// set a sensible value to truncate too large values
#define TRUNCATE_AT 20

double features[NUM_SAMPLES * NUM_AXES];

void setup() {
    Serial.begin(115200);
    imu_setup();
}

void loop() {
    float ax, ay, az;

    imu_read(&amp;ax, &amp;ay, &amp;az);

    ax = constrain(ax, -TRUNCATE_AT, TRUNCATE_AT);
    ay = constrain(ay, -TRUNCATE_AT, TRUNCATE_AT);
    az = constrain(az, -TRUNCATE_AT, TRUNCATE_AT);

    Serial.print(ax);
    Serial.print(&#039;\t&#039;);
    Serial.print(ay);
    Serial.print(&#039;\t&#039;);
    Serial.println(az);
}</code></pre>
<p>Open the Serial plotter and make some movement to have an idea of the range of your readings.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Raw-gestures.gif&quot;" alt="Raw IMU readings for the gestures identification project" /></p>
<h3 id="toccalibration">2.2 Calibration</h3>
<p>Due to gravity, we get a stable value of -9.8 on the Z axis at rest (you can see this in the previous image). Since I'd like to have almost 0 at rest, I created a super simple calibration procedure to remove this fixed offset from the readings.</p>
<pre><code class="language-cpp">double baseline[NUM_AXES];
double features[NUM_SAMPLES * NUM_AXES];

void setup() {
    Serial.begin(115200);
    imu_setup();
    calibrate();
}

void loop() {
    float ax, ay, az;

    imu_read(&amp;ax, &amp;ay, &amp;az);

    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);
    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);
    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);
}

void calibrate() {
    float ax, ay, az;

    for (int i = 0; i &lt; 10; i++) {
        imu_read(&amp;ax, &amp;ay, &amp;az);
        delay(100);
    }

    baseline[0] = ax;
    baseline[1] = ay;
    baseline[2] = az;
}</code></pre>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Calibrated-gestures.gif" alt="Calibrated IMU readings for the gestures identification project" /></p>
<p>Much better.</p>
<h3 id="tocdetect-first-motion">2.3 Detect first motion</h3>
<p>Now we need to check if motion is happening. To keep it simple, we'll use a naive approach that will look for an high value in the acceleration: if a threshold is exceeded, a gesture is starting. </p>
<p>If you did the calibration step, a threshold of 5 should work well. If you didn't calibrate, you have to come up with a value that suits your needs.</p>
<pre><code class="language-cpp">#include imu.h

#define ACCEL_THRESHOLD 5

void loop() {
    float ax, ay, az;

    imu_read(&amp;ax, &amp;ay, &amp;az);

    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);
    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);
    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);

    if (!motionDetected(ax, ay, az)) {
        delay(10);
        return;
    }
}

bool motionDetected(float ax, float ay, float az) {
    return (abs(ax) + abs(ay) + abs(az)) &gt; ACCEL_THRESHOLD;
}</code></pre>
<h3 id="tocrecord-features">2.4 Record features</h3>
<p>If no motion is happening, we don't take any action and keep watching. If motion is happening, we print the next <code>NUM_SAMPLES</code> readings to Serial. </p>
<pre><code class="language-cpp">void loop() {
    float ax, ay, az;

    imu_read(&amp;ax, &amp;ay, &amp;az);

    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);
    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);
    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);

    if (!motionDetected(ax, ay, az)) {
        delay(10);
        return;
    }

    recordIMU();
    printFeatures();
    delay(2000);
}

void recordIMU() {
    float ax, ay, az;

    for (int i = 0; i &lt; NUM_SAMPLES; i++) {
        imu_read(&amp;ax, &amp;ay, &amp;az);

        ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);
        ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);
        az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);

        features[i * NUM_AXES + 0] = ax;
        features[i * NUM_AXES + 1] = ay;
        features[i * NUM_AXES + 2] = az;

        delay(INTERVAL);
    }
}</code></pre>
<pre><code class="language-cpp">
void printFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);
    
    for (int i = 0; i &lt; numFeatures; i++) {
        Serial.print(features[i]);
        Serial.print(i == numFeatures - 1 ? 'n' : ',');
    }
}
</code></pre>
<p>Record 15-20 samples for each geasture and save them to a file, one for each gesture. Since we're dealing with highly dimensional data, you should collect as much samples as possible, to average out the noise.</p>
<h2 id="toctrain-and-export-the-classifier">3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<p>In this project on Machine learning, differently from the previous and simpler ones, we're not achieving 100% accuracy easily. Motion is quite noise, so you should experiment with a few params for the classifier and choose the ones that perform best. I'll showcase a few examples:</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-Linear-kernel.png" alt="Decision boundaries of 2 PCA components of Gestures features, Linear kernel" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-Polynomial-kernel.png" alt="Decision boundaries of 2 PCA components of Gestures features, Polynomial kernel" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.01-gamma.png" alt="Decision boundaries of 2 PCA components of Gestures features, RBF kernel, 0.01 gamma" /></p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-of-Gestures-features-RBF-kernel-0.001-gamma.png" alt="Decision boundaries of 2 PCA components of Gestures features, RBF kernel, 0.001 gamma" /></p>
<h3 id="tocselect-a-suitable-model">3.1 Select a suitable model</h3>
<p>Now that we selected the best model, we have to export it to C code. Here comes the culprit: not all models will fit on your board.</p>
<p>The core of SVM (Support Vector Machines) are support vectors: each trained classifier will be characterized by a certain number of them. The problem is: if there're too much, the generated code will be too large to fit in your flash.</p>
<p>For this reason, instead of selecting <em>the best</em> model on accuracy, you should make a ranking, from the best performing to the worst. For each model, starting from the top, you should import it in your Arduino project and try to compile: if it fits, fine, you're done. Otherwise you should pick the next and try again.</p>
<p>It may seem a tedious process, but keep in mind that we're trying to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash: I think this is an acceptable tradeoff.</p>
<hr /><p><em>We&#039;re fitting a model to infer a class from 90 features in 2 Kb of RAM and 32 Kb of flash!</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fhow-to-do-gesture-identification-on-arduino%2F&#038;text=We%27re%20fitting%20a%20model%20to%20infer%20a%20class%20from%2090%20features%20in%202%20Kb%20of%20RAM%20and%2032%20Kb%20of%20flash%21&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p>I'll report a few figures for different combinations I tested.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Kernel</th>
<th style="text-align: center;">C</th>
<th style="text-align: center;">Gamma</th>
<th style="text-align: center;">Degree</th>
<th style="text-align: center;">Vectors</th>
<th style="text-align: center;">Flash size</th>
<th style="text-align: center;">RAM (b)</th>
<th style="text-align: center;">Avg accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RBF</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">53 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">99%</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Poly</strong></td>
<td style="text-align: center;"><strong>100</strong></td>
<td style="text-align: center;"><strong>0.001</strong></td>
<td style="text-align: center;"><strong>2</strong></td>
<td style="text-align: center;"><strong>12</strong></td>
<td style="text-align: center;"><strong>25 Kb</strong></td>
<td style="text-align: center;"><strong>1228</strong></td>
<td style="text-align: center;"><strong>99%</strong></td>
</tr>
<tr>
<td style="text-align: left;">Poly</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">40 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">97%</td>
</tr>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">55 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">95%</td>
</tr>
<tr>
<td style="text-align: left;">RBF</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">80 Kb</td>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">95%</td>
</tr>
</tbody>
</table>
<p>As you can see, we achieved a very high accuracy on the test set for all the classifiers: only one, though, fitted on the Arduino Nano. Of course, if you use a larger board, you can deploy the others too.</p>
<div class="infobox">As a side note, take a look at the <code>RAM</code> column: all the values are equal: this is because in the implementation is independant from the number of support vectors and only depends on the number of features.</div>
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<pre><code class="language-cpp">#include &quot;model.h&quot;

void loop() {
    float ax, ay, az;

    imu_read(&amp;ax, &amp;ay, &amp;az);

    ax = constrain(ax - baseline[0], -TRUNCATE, TRUNCATE);
    ay = constrain(ay - baseline[1], -TRUNCATE, TRUNCATE);
    az = constrain(az - baseline[2], -TRUNCATE, TRUNCATE);

    if (!motionDetected(ax, ay, az)) {
        delay(10);
        return;
    }

    recordIMU();
    classify();
    delay(2000);
}

void classify() {
    Serial.print(&quot;Detected gesture: &quot;);
    Serial.println(classIdxToName(predict(features)));
}</code></pre>
<p>Here we are: it has been a long post, but now you can classify gestures with an Arduino Nano and 2 Kb of RAM. </p>
<hr /><p><em>No fancy Neural Networks, no Tensorflow, no 32-bit ARM processors: plain old SVM on plain old 8 bits with 97% accuracy.</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fhow-to-do-gesture-identification-on-arduino%2F&#038;text=No%20fancy%20Neural%20Networks%2C%20no%20Tensorflow%2C%20no%2032-bit%20ARM%20processors%3A%20plain%20old%20SVM%20on%20plain%20old%208%20bits%20with%2097%25%20accuracy.&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<p>Here's a short demo of me playing 3 gestures and getting the results on the serial monitor.</p>
<div style="width: 640px;" class="wp-video"><video class="wp-video-shortcode" id="video-35-4" width="640" height="360" preload="metadata" controls="controls"><source type="video/mp4" src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4?_=4" /><a href="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4">https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4</a></video></div>
<p><h4>Project figures</h4>
<p>On my machine, the sketch targeted at the Arduino Nano (old generation) requires 25310 bytes (82%) of program space and 1228 bytes (59%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question <em>Can I run machine learning on Arduino?</em> is <strong>definetly YES</strong>.<br />
<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlGestureIdentificationExample/MicromlGestureIdentificationExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/how-to-do-gesture-identification-on-arduino/">How to do Gesture identification through machine learning on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Gesture-identification-in-action.mp4" length="1035484" type="video/mp4" />

			</item>
		<item>
		<title>Morse alphabet identification on Arduino with Machine learning</title>
		<link>https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Fri, 06 Dec 2019 12:07:38 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=194</guid>

					<description><![CDATA[<p>In this Arduno Machine learning project we're going to identify the letters from the Morse alphabet. In practice, we'll translate dots (â€¢) and dashes (â€’) &#34;typed&#34; with a push button into meaningful characters. In this tutorial we're going to target an Arduino Nano board (old generation), equipped with 32 kb of flash and only 2 [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/">Morse alphabet identification on Arduino with Machine learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this Arduno Machine learning project we're going to identify the letters from the <a href="https://en.wikipedia.org/wiki/Morse_code">Morse alphabet</a>.<br />
In practice, we'll translate dots (â€¢) and dashes (â€’)  &quot;typed&quot; with a push button into meaningful characters.<br />
In this tutorial we're going to target an Arduino Nano board (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>
<p><span id="more-194"></span></p>
<p><img src="https://i.ytimg.com/vi/L6gxfX4GrbI/maxresdefault.jpg" alt="credits to https://www.youtube.com/watch?v=L6gxfX4GrbI" /></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><li><a href="#toctrain-and-export-the-classifier">Train and export the classifier</a><li><a href="#tocrun-the-inference">Run the inference</a></ol></div></p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>For our task we'll use a simple push button as input and a fixed number of samples taken at a fixed interval (100 ms), starting from the first detection of the button press. I chose to record 30 samples for each letter, but you can easily customize the value as per your needs. </p>
<p>With 30 samples at 100 ms frequency, we'll have 3 seconds to &quot;type&quot; the letter and on the Serial monitor will appear a sequence of 0s and 1s, representing if the button was pressed or not; the inference procedure will translate this sequence into a letter.<br />
As a reference, here are a couple example of what we'll be working with.</p>
<pre><code class="language-cpp">// A (â€¢â€’)
0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1

// D (â€’â€¢â€¢)
0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1

// E (â€¢)
0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1</code></pre>
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<p>To the bare minimum, we'll need a push button and two wires: one to ground and the other to a digital pin. Since in the example we'll make the button an <code>INPUT_PULLUP</code>, we'll read 0 when the button is pressed and 1 when not.  </p>
<p><img src="https://www.arduino.cc/en/uploads/Tutorial/PullUp_bbd.png" alt="credits to https://www.arduino.cc/en/Tutorial/DigitalInputPullup" /></p>
<p>All we need to do is detect a press and record the following 30 samples of the digital pin:</p>
<pre><code class="language-cpp">#define IN 4
#define NUM_SAMPLES 30
#define INTERVAL 100

double features[NUM_SAMPLES];

void setup() {
  Serial.begin(115200);
  pinMode(IN, INPUT_PULLUP);
}

void loop() {
  if (digitalRead(IN) == 0) {
    recordButtonStatus();
    printFeatures();
    delay(1000);
  }

  delay(10);
}

void recordButtonStatus() {
  for (int i = 0; i &lt; NUM_SAMPLES; i++) {
    features[i] = digitalRead(IN);
    delay(INTERVAL);
  } 
}</code></pre>
<pre><code class="language-cpp">
void printFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);
    
    for (int i = 0; i &lt; numFeatures; i++) {
        Serial.print(features[i]);
        Serial.print(i == numFeatures - 1 ? 'n' : ',');
    }
}
</code></pre>
<p>Open the Serial monitor and type a few times each letter: try to introduce some variations each time, for example waiting some more milliseconds before releasing the dash.</p>
<div class="watchout"> If you've never typed morse code before (as me), choose letters with few keystrokes and quite differentiable, otherwise you will need to be very good with the timing.</div>
<p>Save the recordings for each letter in a file named after the letter, so you will get meaningful results later on.</p>
<p>You may end with duplicate recordings: don't worry, that's not a problem. I'll paste my recordings for a few letters, as a reference.</p>
<pre><code>// A (â€¢â€’)
0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1
0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1

// D (â€’â€¢â€¢)
0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1

// E (â€¢)
0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1

// S (â€¢â€¢â€¢)
0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1

// T (â€’)
0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1</code></pre>
<p>If you do a good job, you should end with quite distinguible features, as show in the plot below.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-from-Morse-alphabet-identification-features.svg" alt="Decision boundaries of 2 PCA components from Morse alphabet identification features" /></p>
<h2 id="toctrain-and-export-the-classifier">3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<pre><code class="language-cpp">#include &quot;model.h&quot;

void loop() {
  if (digitalRead(IN) == 0) {
    recordButtonStatus();
    Serial.print(&quot;Detected letter: &quot;);
    Serial.println(classIdxToName(predict(features)));
    delay(1000);
  }

  delay(10);
}</code></pre>
<p>Type some letter using the push button and see the identified value printed on the serial monitor.</p>
<p>Thatâ€™s it: you deployed machine learning in 2 Kb! </p>
<p><h4>Project figures</h4>
<p>On my machine, the sketch targeted at the Arduino Nano (old generation) requires 12546 bytes (40%) of program space and 366 bytes (17%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question <em>Can I run machine learning on Arduino?</em> is <strong>definetly YES</strong>.<br />
<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlMorseIdentificationExample/MicromlMorseIdentificationExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/morse-identification-on-arduino/">Morse alphabet identification on Arduino with Machine learning</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>How to do color identification through machine learning on Arduino</title>
		<link>https://eloquentarduino.github.io/2019/12/color-identification-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Sun, 01 Dec 2019 10:35:29 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[svm]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=6</guid>

					<description><![CDATA[<p>In this Arduno Machine learning project we're going to use an RGB sensor to identify objects based on their color. This is a remake of the project found on the Tensorflow blog. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/color-identification-on-arduino/">How to do color identification through machine learning on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>In this Arduno Machine learning project we're going to use an RGB sensor to identify objects based on their color.<br />
This is a remake of the project found on the <a href="https://blog.tensorflow.org/2019/11/fruit-identification-using-arduino-and-tensorflow.html">Tensorflow blog</a>. We're going to use a lot less powerful chip in this tutorial, tough: an Arduino Nano (old generation), equipped with 32 kb of flash and only 2 kb of RAM.</p>
<p><span id="more-6"></span></p>
<p><div class="toc"><h6>Table of contents</h6><ol><li><a href="#tocfeatures-definition">Features definition</a><li><a href="#tocrecord-sample-data">Record sample data</a><li><a href="#toctrain-and-export-the-classifier">Train and export the classifier</a><li><a href="#tocrun-the-inference">Run the inference</a></ol></div></p>
<h2 id="tocfeatures-definition">1. Features definition</h2>
<p>We're going to use the RGB components of a color sensor (TCS3200 in my case) to infer which object we're pointing it at. This means our features are going to be of 3-dimensional, which leads to a really simple model with very high accuracy.</p>
<hr /><p><em>You can do color identification on Arduino using Machine learning without Neural Networks #Arduino #microml #ml #tinyml #MachineLearning #ai #svm</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2019%2F12%2Fcolor-identification-on-arduino%2F&#038;text=You%20can%20do%20color%20identification%20on%20Arduino%20using%20Machine%20learning%20without%20Neural%20Networks%20%23Arduino%20%23microml%20%23ml%20%23tinyml%20%23MachineLearning%20%23ai%20%23svm&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel="noopener noreferrer" >Click To Tweet</a><br /><hr />
<h2 id="tocrecord-sample-data">2. Record sample data</h2>
<p>We don't need any processing to get from the sensor readings to the feature vector, so the code will be straight-forward: read each component from the sensor and assign it to the features array. This part will vary based on the specific chip you have: I'll report the code for a TCS 230/3200. </p>
<pre><code class="language-cpp">#define S2 2
#define S3 3
#define sensorOut 4

double features[3];

void setup() {
  Serial.begin(115200);
  pinMode(S2, OUTPUT);
  pinMode(S3, OUTPUT);
  pinMode(sensorOut, INPUT);
}

void loop() {
  readRGB();
  printFeatures();
  delay(100);
}

int readComponent(bool s2, bool s3) {
  delay(10);
  digitalWrite(S2, s2);
  digitalWrite(S3, s3);

  return pulseIn(sensorOut, LOW);
}

void readRGB() {
  features[0] = readComponent(LOW, LOW);
  features[1] = readComponent(HIGH, HIGH);
  features[2] = readComponent(LOW, HIGH);
}</code></pre>
<pre><code class="language-cpp">
void printFeatures() {
    const uint16_t numFeatures = sizeof(features) / sizeof(double);
    
    for (int i = 0; i &lt; numFeatures; i++) {
        Serial.print(features[i]);
        Serial.print(i == numFeatures - 1 ? 'n' : ',');
    }
}
</code></pre>
<p>Open the Serial monitor and put some colored objects in front of the sensor: move the object a bit and rotate it, so the samples will include different shades of the color.</p>
<p>Save the recordings for each color in a file named after the color, so you will get meaningful results later on.</p>
<div class="watchout">
Donâ€™t forget to sample the â€œempty colorâ€ too: donâ€™t put anything in front of the sensor and let it record for a while.
</div>
<p>If you do a good job, you should end with distinguible features, as show in the contour plot below.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2019/12/Decision-boundaries-of-2-PCA-components-from-the-colors-features.png" alt="Decision boundaries of 2 PCA components from the colors features" /></p>
<h2 id="toctrain-and-export-the-classifier">3. Train and export the classifier</h2>

<p>For a detailed guide refer to the <a href="/2019/11/how-to-train-a-classifier-in-scikit-learn/" target="_blank" rel="noopener noreferrer">tutorial</a></p>

<p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from micromlgen import port

# put your samples in the dataset folder
# one class per file
# one feature vector per line, in CSV format
features, classmap = load_features('dataset/')
X, y = features[:, :-1], features[:, -1]
classifier = RandomForestClassifier(n_estimators=30, max_depth=10).fit(X, y)
c_code = port(classifier, classmap=classmap)
print(c_code)</code></pre>

<p>At this point you have to copy the printed code and import it in your Arduino project, in a file called <code>model.h</code>.</p>
<h2 id="tocrun-the-inference">4. Run the inference</h2>
<pre><code class="language-cpp">#include model.h

void loop() {
  readRGB();
  Serial.println(classIdxToName(predict(features)));
  delay(1000);
}</code></pre>
<p>Put some colored object in front of the sensor and see the identified object name printed on the serial monitor.</p>
<div class="watchout">Do you remember the "empty color"? It needs to be recorded so you will get "empty" when no object is present, otherwise you'll get unexpected predictions</div>
<p>Given the simplicity of the task, you should easily achieve near 100% accuracy for different colors (I had some troubles distinguishing orange from yellow because of the bad illumination). Just be sure to replicate the exact same setup both during training and classification.</p>
<p>Thatâ€™s it: you deployed machine learning in 2 Kb! </p>
<p><h4>Project figures</h4>
<p>On my machine, the sketch targeted at the Arduino Nano (old generation) requires 5570 bytes (18%) of program space and 266 bytes (12%) of RAM. This means you could actually run machine learning in even less space than what the Arduino Nano provides. So, the answer to the question <em>Can I run machine learning on Arduino?</em> is <strong>definetly YES</strong>.<br />
<br><p>Did you find this tutorial useful? Was is it easy to follow or did I miss something? Let me know in the comments so I can keep improving the blog.</p><br />
<hr>
<p>Check the full project code on <a href="https://github.com/eloquentarduino/EloquentArduino/blob/master/examples/MicromlColorIdentificationExample/MicromlColorIdentificationExample.ino" target="_blank" rel="noopener noreferrer">Github</a></p></p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2019/12/color-identification-on-arduino/">How to do color identification through machine learning on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
