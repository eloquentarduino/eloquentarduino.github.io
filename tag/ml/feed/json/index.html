{
    "version": "https://jsonfeed.org/version/1.1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://eloquentarduino.github.io/tag/ml/feed/json/ -- and add it your reader.",
    "home_page_url": "https://eloquentarduino.github.io/tag/ml/",
    "feed_url": "https://eloquentarduino.github.io/tag/ml/feed/json/",
    "language": "en-US",
    "title": "ml &#8211; Eloquent Arduino Blog",
    "description": "Machine learning on Arduino, programming &amp; electronics",
    "items": [
        {
            "id": "https://eloquentarduino.github.io/?p=1282",
            "url": "https://eloquentarduino.github.io/2020/08/better-word-classification-with-arduino-33-ble-sense-and-machine-learning/",
            "title": "Better word classification with Arduino Nano 33 BLE Sense and Machine Learning",
            "content_html": "<p>Let's revamp the post I wrote about <a href=\"/2019/12/word-classification-using-arduino/\">word classification using Machine Learning on Arduino</a>, this time using a proper microphone (the MP34DT05 mounted on the Arduino Nano 33 BLE Sense) instead of a chinese, analog one: will the results improve?</p>\n<div id=\"attachment_653\" style=\"width: 760px\" class=\"wp-caption alignnone\"><img aria-describedby=\"caption-attachment-653\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg\" width=\"750\" height=\"422\" class=\"size-full wp-image-653\" srcset=\"https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord.jpg 750w, https://eloquentarduino.github.io/wp-content/uploads/2019/12/WakeWord-300x169.jpg 300w\" sizes=\"(max-width: 750px) 100vw, 750px\" /><p id=\"caption-attachment-653\" class=\"wp-caption-text\">from https://www.udemy.com/course/learn-audio-processing-complete-engineers-course/</p></div>\n<p><span id=\"more-1282\"></span></p>\n<p><div class=\"toc\"><h6>Table of contents</h6><ol><li><a href=\"#tocpulse-density-modulation-a-k-a-pdm\">Pulse-density modulation (a.k.a. PDM)</a><ol><li><a href=\"#tocwheres-the-fft\">Where's the FFT?</a></li></ol><li><a href=\"#tocmachine-learning-model\">Machine learning model</a></ol></div></p>\n<h2 id=\"tocpulse-density-modulation-a-k-a-pdm\">Pulse-density modulation (a.k.a. PDM)</h2>\n<p>In the original post, I used an analog microphone to record the audio. It is for sure the easiest way to interact with audio on a microcontroller since you only need to <code>analogRead()</code> the selected pin to get a value from the sensor.</p>\n<p>This semplicity, however, comes at the cost of a nearly inexistent signal pre-processing from the sensor itself: most of the time, you will get junk - I don't want to be rude, but that's it.</p>\n<p>The microphone mounted on the Arduino Nano 33 BLE Sense (the <a href=\"https://content.arduino.cc/assets/Nano_BLE_Sense_mp34dt05-a.pdf\">MP34DT05</a>), gives you access to modulated signal much more suitable for our processing needs.</p>\n<p>The modulation used is Pulse-density: I won't try to explain you how this works since I'm not an expert in DSP and neither it is the main scope of this article (refer to <a href=\"https://en.wikipedia.org/wiki/Pulse-density_modulation\">Wikipedia</a> for some more information).</p>\n<p>What matters to us is that we can grab an array of bytes from the microphone and extract its <a href=\"https://en.wikipedia.org/wiki/Root_mean_square\">Root Mean Square</a> (a.k.a. RMS) to be used as a feature for our Machine Learning model.</p>\n<p>I had some difficulty finding examples on how to access the microphone on the Arduino Nano 33 BLE Sense board: fortunately, there's a <a href=\"https://github.com/DaleGia/nano-33-sense-serial-example\">Github repo</a> from <em>DelaGia</em> that shows how to access all the sensors of the board.</p>\n<p>I extracted the microphone part and incapsulated it in an easy to use class, so you don't really need to dig into the implementation details if you're not interested.</p>\n<p>The following code extract highlights the steps you need to access the microphone recordings.</p>\n<pre><code class=\"language-cpp\">// you&#039;ll find this file in the Github repo\n#include &quot;Mic.h&quot;\n\n// tune these constants to your needs\n#define SAMPLES 64\n#define SOUND_THRESHOLD 1500\n\nfloat features[SAMPLES];\nMic mic;\n\n/**\n *\n */\nvoid setup() {\n    Serial.begin(115200);\n    // the PDM library is asynchronous, so we need a callback\n    PDM.onReceive(onAudio);\n    mic.begin();\n}\n\n/**\n * PDM callback to update mic object\n */\nvoid onAudio() {\n    mic.update();\n}\n\n/**\n * Read given number of samples from mic\n */\nbool record() {\n    if (mic.hasData() &amp;&amp; mic.pop() &gt; SOUND_THRESHOLD) {\n        for (int i = 0; i &lt; SAMPLES; i++) {\n            while (!mic.hasData()) ; // wait for data\n\n            features[i] = mic.pop();\n        }\n\n        return true;\n    }\n\n    return false;\n}</code></pre>\n<p>The things to note in the above code are:</p>\n<ul>\n<li>the <code>PDM</code> library works asynchronously: it won't block your main code waiting for data, but will report you back when data is ready calling the callback you define. In this case, the callback just instructs the <code>mic</code> object to update</li>\n<li>since the <code>PDM</code> is asynchronous, when acquiring data you will first check if it's available: this is the case for <code>while (!mic.hasData()) ;</code>, which loops awaiting for new data</li>\n</ul>\n<p>Now that we have the acquisition logic in place, it's time for you to record some samples of the words you want to classify. I advise you to record many samples for each word, varying both the distance of your mounth from the mic and the intensity with which you speak: this will produce a more robust classification model later on.</p>\n<p>As always, save each word in a different file, one feature vector per row.</p>\n<p>Here I report an extract of the features I recorded for some words (scaled and converted to int).</p>\n<pre><code>// word &quot;yes&quot;\n43,47,41,48,60,68,67,54,40,29,17,9,6,6,10,15,10,10,8,8,8,5,7,7,12,6,8,11,7,5,3,3,13,14,15,17,15,16,10,10,16,13,8,9,14,8,0,3,3,3,8,5,3,8,11,10,12,10,11,13,8,6,5,12\n34,47,50,51,60,67,65,63,54,42,30,19,25,22,28,7,6,5,7,7,5,8,6,3,3,3,6,7,3,0,0,0,3,5,5,3,5,3,3,3,0,6,3,0,0,5,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,3,0,5\n33,41,45,47,53,61,64,59,50,41,31,17,8,9,7,9,9,7,8,8,8,10,13,8,8,7,5,5,5,3,0,0,0,0,3,8,8,3,0,3,5,5,5,7,0,0,0,0,0,5,0,3,3,5,0,0,0,5,3,0,5,7,0,9\n38,44,48,55,67,74,66,65,60,48,38,19,12,12,8,8,6,7,5,6,6,6,3,3,3,3,5,3,5,3,6,5,0,3,5,0,0,0,3,0,5,3,0,3,0,0,5,6,3,5,3,5,0,0,0,5,3,0,0,0,0,0,0,0\n\n// word &quot;no&quot;\n35,47,49,55,63,65,68,64,60,59,58,41,29,22,14,8,7,5,8,6,0,3,0,0,3,3,0,3,0,0,0,5,3,0,0,0,0,0,0,0,0,0,5,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,7,5,5,3\n42,51,55,59,68,70,73,70,71,67,67,52,52,29,16,9,6,9,3,3,5,8,8,11,10,7,8,5,3,3,0,0,0,0,0,3,3,3,6,7,7,8,9,6,8,8,7,8,6,8,8,6,8,7,6,9,5,3,5,6,5,3,3,3\n56,68,78,91,84,84,84,74,69,64,57,44,33,18,12,8,5,9,15,12,12,9,12,7,3,10,12,6,3,0,0,0,0,6,3,6,10,10,8,3,9,9,9,8,9,9,11,3,8,9,8,8,8,6,7,3,3,8,5,3,0,3,0,0\n31,42,62,69,74,81,78,78,79,74,74,68,54,44,26,15,8,8,21,10,15,12,11,8,6,5,0,0,0,0,3,5,8,3,9,5,6,9,10,6,10,12,7,15,14,5,8,7,8,9,5,8,5,6,5,0,3,6,3,5,3,5,0,0\n\n// word &quot;play&quot;\n31,195,106,60,45,44,55,51,42,36,36,33,38,37,37,29,24,21,20,20,18,19,14,10,14,12,18,11,8,11,10,12,27,8,10,6,7,6,10,6,6,6,8,6,6,3,5,6,0,0,0,0,0,7,5,3,0,5,0,0,8,8,7,5\n138,158,83,64,44,34,36,41,44,39,35,29,27,26,28,33,33,30,23,19,22,18,16,15,15,13,13,14,9,10,9,7,14,8,3,9,0,0,0,3,5,3,7,7,5,0,0,0,5,0,0,3,5,0,0,0,0,10,3,3,7,7,8,3\n89,36,27,30,22,24,38,41,37,33,32,32,32,35,34,30,25,19,18,18,15,16,14,14,12,9,8,10,9,7,3,0,0,8,7,7,6,3,5,9,8,5,7,3,5,7,6,6,3,0,3,3,0,8,8,5,6,0,0,6,0,7,8,6\n71,98,51,31,27,35,48,50,45,38,34,30,31,30,36,35,28,21,20,16,15,16,16,14,12,10,12,10,8,8,6,7,5,7,12,6,10,10,10,8,8,0,0,8,7,0,5,6,3,0,5,3,0,0,3,5,8,6,5,3,10,3,6,0\n\n// word &quot;stop&quot;\n61,93,135,157,140,148,128,92,85,64,160,75,23,28,28,22,14,8,0,3,3,5,3,6,12,202,154,100,102,94,40,54,52,40,35,22,20,19,11,18,9,7,10,12,8,7,5,7,12,14,16,12,8,6,5,11,5,7,9,10,12,21,20,16\n62,84,110,111,113,99,79,70,60,95,71,22,14,19,16,16,6,12,9,7,3,0,3,6,184,209,116,50,33,49,47,45,32,28,21,16,13,13,3,0,0,8,21,14,10,19,19,15,16,15,22,14,14,14,17,14,12,10,3,9,17,11,10,12\n94,139,168,178,165,143,100,82,128,169,34,22,26,26,22,13,8,8,3,0,3,0,5,202,191,123,119,119,64,31,71,39,35,30,19,16,14,8,19,7,8,7,6,11,18,17,13,20,24,27,29,22,23,22,25,23,21,23,21,18,17,14,14,13\n121,159,181,165,170,154,134,99,75,121,51,8,27,14,18,10,7,3,5,0,5,3,6,11,152,118,160,115,68,42,29,36,53,35,38,30,26,22,19,12,7,6,0,0,0,6,11,15,17,19,18,24,22,24,23,16,15,20,22,12,10,16,21,14</code></pre>\n<h3 id=\"tocwheres-the-fft\">Where's the FFT?</h3>\n<p>If you noticed, I didn't mention the <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\">Fast Fourier Transform</a> until now.</p>\n<p>Why?</p>\n<p>Well, it is believed (at least, I believed) that the Fourier Transform is the go-to transformation when working with audio recordings: it can extract frequency-related features useful to extract information from raw signals.</p>\n<p>For this project, I tried not to use it and I was surprised that it worked fine in spite of everything. Since I like to keep things as lean as possible, I won't apply any complex transformation to the samples.</p>\n<p>If you're having poor results, however, remember you can try to apply FFT and see if it helps improve the accuracy.</p>\n<h2 id=\"tocmachine-learning-model\">Machine learning model</h2>\n<p>Now that we have the samples, it's time to train the classifier.</p>\n<p>This step is the same as in any other tutorial I wrote so far, so I won't spend much time on this part. Among the classifiers I tried, SVM produced the best accuracy at 96% with 32 support vectors: it's not a super-tiny model, but it's quite small nevertheless.</p>\n<pre><code class=\"language-python\">import numpy as np\nfrom os.path import basename\nfrom glob import glob\nfrom sklearn.svm import SVC\nfrom micromlgen import port\n\ndef load_features(folder):\n    dataset = None\n    classmap = {}\n    for class_idx, filename in enumerate(glob(&#039;%s/*.csv&#039; % folder)):\n        class_name = basename(filename)[:-4]\n        classmap[class_idx] = class_name\n        samples = np.loadtxt(filename, dtype=float, delimiter=&#039;,&#039;)\n        labels = np.ones((len(samples), 1)) * class_idx\n        samples = np.hstack((samples, labels))\n        dataset = samples if dataset is None else np.vstack((dataset, samples))\n    return dataset, classmap\n\ndataset, classmap = load_features(&#039;64&#039;)\nX, y = dataset[:, :-1], dataset[:, -1]\nclf = SVC(kernel=&#039;poly&#039;, degree=2, gamma=0.1, C=100)\nclf.fit(X_train, y_train)\nprint(port(clf, classmap=classmap))</code></pre>\n<pre><code class=\"language-cpp\">// The produced ouput for my SVM model\n\n#pragma once\nnamespace Eloquent {\n    namespace ML {\n        namespace Port {\n            class SVM {\n            public:\n                /**\n                * Predict class for features vector\n                */\n                int predict(float *x) {\n                    float kernels[35] = { 0 };\n                    float decisions[6] = { 0 };\n                    int votes[4] = { 0 };\n                    kernels[0] = compute_kernel(x,   33.0  , 41.0  , 47.0  , 54.0  , 59.0  , 61.0  , 56.0  , 51.0  , 50.0  , 51.0  , 44.0  , 32.0  , 23.0  , 15.0  , 12.0  , 8.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[1] = compute_kernel(x,   40.0  , 50.0  , 51.0  , 60.0  , 56.0  , 57.0  , 58.0  , 53.0  , 50.0  , 45.0  , 42.0  , 34.0  , 23.0  , 16.0  , 10.0  , 7.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 14.0  , 3.0  , 8.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 3.0 );\n                    kernels[2] = compute_kernel(x,   56.0  , 68.0  , 78.0  , 91.0  , 84.0  , 84.0  , 84.0  , 74.0  , 69.0  , 64.0  , 57.0  , 44.0  , 33.0  , 18.0  , 12.0  , 8.0  , 5.0  , 9.0  , 15.0  , 12.0  , 12.0  , 9.0  , 12.0  , 7.0  , 3.0  , 10.0  , 12.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 6.0  , 3.0  , 6.0  , 10.0  , 10.0  , 8.0  , 3.0  , 9.0  , 9.0  , 9.0  , 8.0  , 9.0  , 9.0  , 11.0  , 3.0  , 8.0  , 9.0  , 8.0  , 8.0  , 8.0  , 6.0  , 7.0  , 3.0  , 3.0  , 8.0  , 5.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0 );\n                    kernels[3] = compute_kernel(x,   33.0  , 18.0  , 26.0  , 39.0  , 46.0  , 60.0  , 66.0  , 72.0  , 82.0  , 76.0  , 82.0  , 77.0  , 78.0  , 79.0  , 76.0  , 73.0  , 63.0  , 41.0  , 33.0  , 21.0  , 13.0  , 7.0  , 3.0  , 10.0  , 3.0  , 5.0  , 6.0  , 21.0  , 21.0  , 14.0  , 5.0  , 8.0  , 5.0  , 5.0  , 0.0  , 8.0  , 8.0  , 3.0  , 0.0  , 3.0  , 3.0  , 5.0  , 0.0  , 3.0  , 8.0  , 7.0  , 6.0  , 7.0  , 8.0  , 9.0  , 9.0  , 8.0  , 8.0  , 7.0  , 17.0  , 3.0  , 3.0  , 6.0  , 6.0  , 5.0  , 3.0  , 6.0  , 6.0  , 3.0 );\n                    kernels[4] = compute_kernel(x,   54.0  , 57.0  , 62.0  , 58.0  , 61.0  , 61.0  , 59.0  , 58.0  , 57.0  , 51.0  , 34.0  , 25.0  , 18.0  , 10.0  , 6.0  , 6.0  , 10.0  , 7.0  , 5.0  , 10.0  , 5.0  , 7.0  , 8.0  , 6.0  , 5.0  , 5.0  , 5.0  , 7.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 6.0  , 0.0  , 7.0  , 0.0  , 5.0  , 6.0  , 0.0  , 0.0  , 6.0  , 7.0  , 5.0  , 3.0  , 5.0  , 6.0  , 0.0  , 0.0  , 0.0  , 12.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[5] = compute_kernel(x,   49.0  , 58.0  , 68.0  , 69.0  , 72.0  , 72.0  , 75.0  , 76.0  , 73.0  , 59.0  , 59.0  , 36.0  , 19.0  , 12.0  , 12.0  , 17.0  , 12.0  , 21.0  , 9.0  , 6.0  , 8.0  , 6.0  , 7.0  , 15.0  , 14.0  , 14.0  , 10.0  , 8.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 7.0  , 10.0  , 7.0  , 6.0  , 8.0  , 12.0  , 13.0  , 15.0  , 11.0  , 16.0  , 15.0  , 13.0  , 7.0  , 6.0  , 12.0  , 5.0  , 8.0  , 8.0  , 12.0  , 8.0  , 0.0  , 3.0  , 7.0  , 10.0  , 9.0  , 8.0  , 5.0 );\n                    kernels[6] = compute_kernel(x,   43.0  , 63.0  , 76.0  , 73.0  , 67.0  , 72.0  , 64.0  , 50.0  , 31.0  , 123.0  , 95.0  , 27.0  , 17.0  , 20.0  , 12.0  , 14.0  , 11.0  , 9.0  , 6.0  , 3.0  , 3.0  , 15.0  , 156.0  , 172.0  , 69.0  , 52.0  , 47.0  , 41.0  , 18.0  , 29.0  , 46.0  , 20.0  , 22.0  , 21.0  , 0.0  , 0.0  , 6.0  , 3.0  , 7.0  , 10.0  , 10.0  , 14.0  , 13.0  , 13.0  , 11.0  , 8.0  , 18.0  , 26.0  , 19.0  , 15.0  , 15.0  , 16.0  , 17.0  , 13.0  , 11.0  , 8.0  , 20.0  , 19.0  , 10.0  , 9.0  , 10.0  , 12.0  , 9.0  , 12.0 );\n                    kernels[7] = compute_kernel(x,   66.0  , 85.0  , 105.0  , 116.0  , 118.0  , 104.0  , 102.0  , 81.0  , 58.0  , 129.0  , 222.0  , 48.0  , 80.0  , 70.0  , 50.0  , 40.0  , 19.0  , 11.0  , 3.0  , 0.0  , 0.0  , 0.0  , 7.0  , 5.0  , 15.0  , 16.0  , 9.0  , 5.0  , 25.0  , 29.0  , 43.0  , 32.0  , 28.0  , 31.0  , 21.0  , 8.0  , 0.0  , 9.0  , 8.0  , 3.0  , 3.0  , 0.0  , 0.0  , 7.0  , 11.0  , 9.0  , 26.0  , 16.0  , 13.0  , 9.0  , 20.0  , 16.0  , 16.0  , 26.0  , 18.0  , 10.0  , 7.0  , 12.0  , 10.0  , 8.0  , 7.0  , 17.0  , 15.0  , 7.0 );\n                    kernels[8] = compute_kernel(x,   64.0  , 77.0  , 90.0  , 92.0  , 93.0  , 93.0  , 83.0  , 58.0  , 112.0  , 80.0  , 34.0  , 13.0  , 16.0  , 10.0  , 5.0  , 3.0  , 7.0  , 0.0  , 0.0  , 0.0  , 8.0  , 10.0  , 50.0  , 101.0  , 44.0  , 41.0  , 16.0  , 23.0  , 19.0  , 6.0  , 13.0  , 19.0  , 25.0  , 15.0  , 8.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 10.0  , 19.0  , 8.0  , 8.0  , 10.0  , 7.0  , 8.0  , 9.0  , 14.0  , 7.0  , 9.0  , 8.0  , 10.0  , 8.0  , 7.0  , 6.0  , 6.0  , 3.0  , 7.0  , 6.0  , 3.0  , 3.0  , 3.0  , 3.0 );\n                    kernels[9] = compute_kernel(x,   33.0  , 28.0  , 20.0  , 9.0  , 5.0  , 7.0  , 7.0  , 13.0  , 130.0  , 183.0  , 212.0  , 197.0  , 190.0  , 167.0  , 151.0  , 110.0  , 83.0  , 54.0  , 67.0  , 20.0  , 23.0  , 24.0  , 16.0  , 12.0  , 7.0  , 0.0  , 3.0  , 9.0  , 0.0  , 3.0  , 5.0  , 10.0  , 79.0  , 109.0  , 80.0  , 75.0  , 38.0  , 38.0  , 29.0  , 26.0  , 29.0  , 27.0  , 26.0  , 27.0  , 22.0  , 22.0  , 15.0  , 6.0  , 0.0  , 3.0  , 12.0  , 18.0  , 21.0  , 24.0  , 27.0  , 27.0  , 25.0  , 26.0  , 25.0  , 25.0  , 27.0  , 25.0  , 22.0  , 19.0 );\n                    kernels[10] = compute_kernel(x,   36.0  , 58.0  , 70.0  , 69.0  , 62.0  , 56.0  , 52.0  , 50.0  , 26.0  , 9.0  , 3.0  , 0.0  , 3.0  , 6.0  , 3.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 7.0  , 6.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 3.0  , 0.0  , 3.0  , 5.0  , 7.0  , 5.0  , 0.0  , 0.0  , 7.0  , 9.0  , 6.0  , 5.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 8.0  , 6.0 );\n                    kernels[11] = compute_kernel(x,   99.0  , 122.0  , 127.0  , 115.0  , 110.0  , 101.0  , 88.0  , 64.0  , 51.0  , 186.0  , 73.0  , 16.0  , 25.0  , 26.0  , 22.0  , 18.0  , 12.0  , 9.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 21.0  , 14.0  , 3.0  , 11.0  , 19.0  , 34.0  , 31.0  , 33.0  , 18.0  , 15.0  , 9.0  , 19.0  , 19.0  , 16.0  , 6.0  , 0.0  , 0.0  , 0.0  , 7.0  , 12.0  , 9.0  , 10.0  , 12.0  , 14.0  , 12.0  , 13.0  , 10.0  , 10.0  , 12.0  , 9.0  , 13.0  , 13.0  , 14.0  , 8.0  , 11.0  , 8.0  , 6.0  , 3.0  , 7.0  , 3.0  , 5.0 );\n                    kernels[12] = compute_kernel(x,   51.0  , 69.0  , 82.0  , 82.0  , 78.0  , 82.0  , 71.0  , 68.0  , 50.0  , 33.0  , 58.0  , 76.0  , 28.0  , 5.0  , 12.0  , 12.0  , 6.0  , 6.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 52.0  , 90.0  , 45.0  , 30.0  , 19.0  , 12.0  , 19.0  , 19.0  , 18.0  , 7.0  , 13.0  , 16.0  , 10.0  , 7.0  , 3.0  , 0.0  , 5.0  , 6.0  , 0.0  , 11.0  , 17.0  , 14.0  , 12.0  , 16.0  , 9.0  , 11.0  , 11.0  , 18.0  , 13.0  , 8.0  , 8.0  , 10.0  , 12.0  , 8.0  , 9.0  , 8.0  , 8.0  , 5.0  , 6.0 );\n                    kernels[13] = compute_kernel(x,   55.0  , 102.0  , 134.0  , 149.0  , 145.0  , 149.0  , 148.0  , 127.0  , 94.0  , 64.0  , 108.0  , 94.0  , 37.0  , 15.0  , 22.0  , 17.0  , 17.0  , 14.0  , 13.0  , 0.0  , 8.0  , 14.0  , 9.0  , 0.0  , 6.0  , 3.0  , 7.0  , 5.0  , 6.0  , 5.0  , 5.0  , 12.0  , 8.0  , 0.0  , 10.0  , 14.0  , 8.0  , 9.0  , 0.0  , 3.0  , 3.0  , 0.0  , 5.0  , 3.0  , 8.0  , 8.0  , 3.0  , 3.0  , 3.0  , 3.0  , 8.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 12.0  , 3.0  , 0.0  , 7.0  , 5.0  , 3.0  , 0.0  , 0.0 );\n                    kernels[14] = compute_kernel(x,   75.0  , 89.0  , 116.0  , 125.0  , 124.0  , 102.0  , 109.0  , 99.0  , 80.0  , 57.0  , 136.0  , 108.0  , 55.0  , 10.0  , 20.0  , 20.0  , 16.0  , 10.0  , 8.0  , 14.0  , 6.0  , 0.0  , 3.0  , 3.0  , 6.0  , 19.0  , 8.0  , 11.0  , 3.0  , 46.0  , 33.0  , 29.0  , 26.0  , 22.0  , 12.0  , 14.0  , 21.0  , 16.0  , 18.0  , 16.0  , 13.0  , 3.0  , 0.0  , 8.0  , 6.0  , 14.0  , 10.0  , 21.0  , 21.0  , 17.0  , 16.0  , 16.0  , 19.0  , 17.0  , 16.0  , 20.0  , 14.0  , 10.0  , 14.0  , 13.0  , 12.0  , 12.0  , 10.0  , 6.0 );\n                    kernels[15] = compute_kernel(x,   43.0  , 57.0  , 63.0  , 60.0  , 64.0  , 57.0  , 55.0  , 35.0  , 18.0  , 14.0  , 13.0  , 5.0  , 3.0  , 10.0  , 5.0  , 3.0  , 3.0  , 0.0  , 7.0  , 3.0  , 5.0  , 5.0  , 0.0  , 23.0  , 8.0  , 3.0  , 3.0  , 17.0  , 6.0  , 13.0  , 10.0  , 5.0  , 0.0  , 10.0  , 7.0  , 9.0  , 8.0  , 13.0  , 6.0  , 7.0  , 5.0  , 6.0  , 3.0  , 3.0  , 5.0  , 8.0  , 3.0  , 3.0  , 5.0  , 5.0  , 5.0  , 7.0  , 8.0  , 6.0  , 6.0  , 7.0  , 6.0  , 6.0  , 5.0  , 5.0  , 5.0  , 5.0  , 5.0  , 3.0 );\n                    kernels[16] = compute_kernel(x,   31.0  , 33.0  , 27.0  , 19.0  , 14.0  , 9.0  , 3.0  , 6.0  , 16.0  , 131.0  , 186.0  , 214.0  , 206.0  , 212.0  , 192.0  , 186.0  , 147.0  , 90.0  , 157.0  , 98.0  , 34.0  , 37.0  , 35.0  , 31.0  , 17.0  , 10.0  , 7.0  , 0.0  , 3.0  , 3.0  , 5.0  , 6.0  , 10.0  , 12.0  , 22.0  , 50.0  , 48.0  , 48.0  , 61.0  , 28.0  , 26.0  , 29.0  , 30.0  , 24.0  , 26.0  , 19.0  , 26.0  , 15.0  , 26.0  , 19.0  , 17.0  , 10.0  , 3.0  , 3.0  , 0.0  , 6.0  , 12.0  , 16.0  , 16.0  , 21.0  , 25.0  , 27.0  , 27.0  , 26.0 );\n                    kernels[17] = compute_kernel(x,   50.0  , 70.0  , 80.0  , 83.0  , 79.0  , 73.0  , 80.0  , 74.0  , 54.0  , 34.0  , 16.0  , 7.0  , 3.0  , 3.0  , 3.0  , 3.0  , 6.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 10.0  , 3.0  , 7.0  , 5.0  , 0.0  , 3.0  , 3.0  , 3.0  , 3.0  , 0.0  , 5.0  , 6.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 7.0  , 3.0  , 0.0  , 3.0  , 0.0 );\n                    kernels[18] = compute_kernel(x,   38.0  , 50.0  , 52.0  , 49.0  , 44.0  , 42.0  , 43.0  , 41.0  , 41.0  , 42.0  , 42.0  , 40.0  , 35.0  , 29.0  , 23.0  , 22.0  , 20.0  , 20.0  , 18.0  , 16.0  , 16.0  , 15.0  , 11.0  , 12.0  , 15.0  , 11.0  , 13.0  , 8.0  , 5.0  , 5.0  , 3.0  , 0.0  , 0.0  , 3.0  , 7.0  , 3.0  , 8.0  , 3.0  , 0.0  , 5.0  , 6.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 6.0  , 3.0  , 8.0  , 5.0  , 7.0  , 5.0  , 8.0  , 8.0  , 7.0  , 6.0  , 10.0  , 11.0  , 3.0  , 5.0 );\n                    kernels[19] = compute_kernel(x,   33.0  , 37.0  , 34.0  , 30.0  , 25.0  , 22.0  , 37.0  , 44.0  , 40.0  , 36.0  , 33.0  , 29.0  , 27.0  , 26.0  , 28.0  , 28.0  , 28.0  , 25.0  , 20.0  , 16.0  , 14.0  , 13.0  , 12.0  , 11.0  , 10.0  , 10.0  , 10.0  , 7.0  , 6.0  , 6.0  , 6.0  , 6.0  , 5.0  , 0.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[20] = compute_kernel(x,   43.0  , 52.0  , 49.0  , 45.0  , 44.0  , 43.0  , 43.0  , 41.0  , 43.0  , 42.0  , 36.0  , 34.0  , 28.0  , 27.0  , 21.0  , 18.0  , 19.0  , 17.0  , 16.0  , 15.0  , 13.0  , 14.0  , 14.0  , 12.0  , 13.0  , 14.0  , 10.0  , 8.0  , 7.0  , 5.0  , 3.0  , 5.0  , 8.0  , 0.0  , 0.0  , 5.0  , 8.0  , 7.0  , 3.0  , 0.0  , 3.0  , 5.0  , 3.0  , 7.0  , 6.0  , 8.0  , 0.0  , 6.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 10.0  , 8.0  , 14.0  , 7.0  , 5.0  , 11.0 );\n                    kernels[21] = compute_kernel(x,   33.0  , 28.0  , 26.0  , 21.0  , 22.0  , 31.0  , 38.0  , 35.0  , 33.0  , 32.0  , 30.0  , 28.0  , 25.0  , 28.0  , 29.0  , 30.0  , 27.0  , 25.0  , 20.0  , 16.0  , 16.0  , 15.0  , 15.0  , 13.0  , 11.0  , 10.0  , 9.0  , 8.0  , 5.0  , 6.0  , 5.0  , 0.0  , 9.0  , 12.0  , 9.0  , 12.0  , 12.0  , 9.0  , 8.0  , 13.0  , 12.0  , 10.0  , 13.0  , 7.0  , 10.0  , 16.0  , 10.0  , 16.0  , 6.0  , 0.0  , 0.0  , 8.0  , 0.0  , 6.0  , 10.0  , 8.0  , 11.0  , 10.0  , 5.0  , 8.0  , 9.0  , 8.0  , 6.0  , 6.0 );\n                    kernels[22] = compute_kernel(x,   40.0  , 49.0  , 48.0  , 45.0  , 48.0  , 47.0  , 51.0  , 55.0  , 52.0  , 43.0  , 35.0  , 30.0  , 16.0  , 8.0  , 6.0  , 8.0  , 8.0  , 7.0  , 7.0  , 9.0  , 10.0  , 8.0  , 9.0  , 8.0  , 6.0  , 8.0  , 6.0  , 5.0  , 10.0  , 7.0  , 3.0  , 3.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 5.0  , 6.0  , 8.0  , 9.0  , 14.0  , 13.0  , 14.0  , 10.0  , 10.0  , 10.0  , 8.0  , 7.0  , 0.0  , 5.0  , 0.0  , 0.0  , 3.0  , 3.0  , 10.0  , 10.0  , 0.0  , 8.0  , 14.0  , 9.0  , 12.0  , 5.0  , 14.0 );\n                    kernels[23] = compute_kernel(x,   37.0  , 42.0  , 57.0  , 68.0  , 69.0  , 75.0  , 74.0  , 67.0  , 51.0  , 38.0  , 26.0  , 13.0  , 6.0  , 13.0  , 15.0  , 14.0  , 11.0  , 10.0  , 8.0  , 12.0  , 9.0  , 8.0  , 5.0  , 8.0  , 13.0  , 8.0  , 3.0  , 3.0  , 0.0  , 0.0  , 9.0  , 9.0  , 5.0  , 7.0  , 6.0  , 3.0  , 3.0  , 5.0  , 5.0  , 7.0  , 3.0  , 6.0  , 3.0  , 6.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 5.0  , 0.0  , 0.0  , 3.0  , 0.0 );\n                    kernels[24] = compute_kernel(x,   42.0  , 39.0  , 41.0  , 45.0  , 46.0  , 49.0  , 46.0  , 43.0  , 35.0  , 28.0  , 18.0  , 11.0  , 7.0  , 10.0  , 8.0  , 10.0  , 8.0  , 8.0  , 7.0  , 8.0  , 8.0  , 8.0  , 8.0  , 8.0  , 6.0  , 5.0  , 6.0  , 5.0  , 6.0  , 5.0  , 0.0  , 3.0  , 5.0  , 6.0  , 0.0  , 5.0  , 3.0  , 8.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 3.0 );\n                    kernels[25] = compute_kernel(x,   45.0  , 43.0  , 45.0  , 48.0  , 56.0  , 54.0  , 54.0  , 44.0  , 35.0  , 25.0  , 19.0  , 8.0  , 6.0  , 5.0  , 6.0  , 5.0  , 5.0  , 6.0  , 5.0  , 6.0  , 6.0  , 6.0  , 5.0  , 5.0  , 3.0  , 3.0  , 5.0  , 5.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 9.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 6.0  , 5.0  , 0.0  , 0.0 );\n                    kernels[26] = compute_kernel(x,   31.0  , 43.0  , 66.0  , 75.0  , 75.0  , 81.0  , 89.0  , 85.0  , 79.0  , 68.0  , 50.0  , 32.0  , 22.0  , 10.0  , 8.0  , 12.0  , 10.0  , 10.0  , 12.0  , 12.0  , 11.0  , 10.0  , 10.0  , 8.0  , 8.0  , 9.0  , 8.0  , 7.0  , 7.0  , 6.0  , 3.0  , 3.0  , 3.0  , 0.0  , 0.0  , 5.0  , 8.0  , 8.0  , 6.0  , 3.0  , 7.0  , 8.0  , 11.0  , 12.0  , 12.0  , 16.0  , 9.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 7.0  , 8.0  , 6.0  , 8.0  , 12.0  , 10.0  , 8.0  , 7.0  , 5.0  , 3.0  , 6.0  , 8.0 );\n                    kernels[27] = compute_kernel(x,   33.0  , 40.0  , 41.0  , 41.0  , 43.0  , 48.0  , 49.0  , 49.0  , 47.0  , 36.0  , 27.0  , 18.0  , 9.0  , 3.0  , 5.0  , 3.0  , 5.0  , 5.0  , 5.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 0.0  , 3.0  , 3.0  , 7.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[28] = compute_kernel(x,   30.0  , 47.0  , 72.0  , 81.0  , 81.0  , 79.0  , 85.0  , 88.0  , 85.0  , 86.0  , 64.0  , 45.0  , 27.0  , 15.0  , 13.0  , 14.0  , 11.0  , 18.0  , 20.0  , 24.0  , 18.0  , 17.0  , 20.0  , 15.0  , 19.0  , 14.0  , 10.0  , 10.0  , 8.0  , 6.0  , 6.0  , 3.0  , 3.0  , 0.0  , 6.0  , 7.0  , 18.0  , 15.0  , 11.0  , 12.0  , 19.0  , 20.0  , 10.0  , 8.0  , 0.0  , 0.0  , 0.0  , 6.0  , 6.0  , 9.0  , 10.0  , 9.0  , 12.0  , 12.0  , 10.0  , 12.0  , 12.0  , 3.0  , 6.0  , 7.0  , 8.0  , 10.0  , 8.0  , 5.0 );\n                    kernels[29] = compute_kernel(x,   33.0  , 31.0  , 31.0  , 33.0  , 36.0  , 36.0  , 36.0  , 32.0  , 25.0  , 19.0  , 12.0  , 5.0  , 5.0  , 10.0  , 6.0  , 6.0  , 8.0  , 6.0  , 7.0  , 6.0  , 6.0  , 6.0  , 11.0  , 6.0  , 6.0  , 6.0  , 5.0  , 3.0  , 3.0  , 0.0  , 0.0  , 6.0  , 3.0  , 5.0  , 3.0  , 10.0  , 5.0  , 6.0  , 9.0  , 3.0  , 7.0  , 6.0  , 6.0  , 8.0  , 7.0  , 0.0  , 6.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 6.0  , 6.0  , 0.0  , 5.0  , 3.0  , 3.0  , 5.0  , 8.0  , 7.0  , 0.0 );\n                    kernels[30] = compute_kernel(x,   31.0  , 48.0  , 65.0  , 60.0  , 68.0  , 81.0  , 88.0  , 91.0  , 93.0  , 79.0  , 56.0  , 32.0  , 22.0  , 12.0  , 16.0  , 13.0  , 12.0  , 13.0  , 13.0  , 13.0  , 14.0  , 11.0  , 11.0  , 14.0  , 12.0  , 10.0  , 8.0  , 8.0  , 7.0  , 5.0  , 3.0  , 0.0  , 3.0  , 5.0  , 5.0  , 7.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[31] = compute_kernel(x,   34.0  , 47.0  , 50.0  , 51.0  , 60.0  , 67.0  , 65.0  , 63.0  , 54.0  , 42.0  , 30.0  , 19.0  , 25.0  , 22.0  , 28.0  , 7.0  , 6.0  , 5.0  , 7.0  , 7.0  , 5.0  , 8.0  , 6.0  , 3.0  , 3.0  , 3.0  , 6.0  , 7.0  , 3.0  , 0.0  , 0.0  , 0.0  , 3.0  , 5.0  , 5.0  , 3.0  , 5.0  , 3.0  , 3.0  , 3.0  , 0.0  , 6.0  , 3.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 5.0 );\n                    kernels[32] = compute_kernel(x,   34.0  , 48.0  , 58.0  , 55.0  , 59.0  , 69.0  , 77.0  , 75.0  , 75.0  , 69.0  , 56.0  , 43.0  , 22.0  , 13.0  , 10.0  , 6.0  , 9.0  , 15.0  , 11.0  , 10.0  , 8.0  , 8.0  , 7.0  , 3.0  , 6.0  , 5.0  , 5.0  , 7.0  , 6.0  , 6.0  , 7.0  , 3.0  , 5.0  , 11.0  , 5.0  , 7.0  , 6.0  , 8.0  , 11.0  , 8.0  , 16.0  , 9.0  , 7.0  , 8.0  , 6.0  , 3.0  , 6.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 6.0  , 3.0  , 0.0  , 3.0  , 5.0  , 8.0  , 10.0  , 13.0  , 13.0 );\n                    kernels[33] = compute_kernel(x,   32.0  , 31.0  , 33.0  , 33.0  , 35.0  , 37.0  , 34.0  , 28.0  , 25.0  , 15.0  , 10.0  , 5.0  , 6.0  , 5.0  , 7.0  , 8.0  , 6.0  , 6.0  , 7.0  , 8.0  , 5.0  , 6.0  , 5.0  , 7.0  , 6.0  , 5.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 5.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[34] = compute_kernel(x,   44.0  , 43.0  , 48.0  , 58.0  , 59.0  , 54.0  , 55.0  , 49.0  , 48.0  , 38.0  , 26.0  , 14.0  , 8.0  , 9.0  , 14.0  , 12.0  , 7.0  , 9.0  , 10.0  , 8.0  , 9.0  , 7.0  , 6.0  , 6.0  , 6.0  , 6.0  , 6.0  , 5.0  , 6.0  , 8.0  , 6.0  , 0.0  , 0.0  , 13.0  , 8.0  , 6.0  , 17.0  , 10.0  , 8.0  , 17.0  , 13.0  , 15.0  , 13.0  , 14.0  , 8.0  , 8.0  , 11.0  , 8.0  , 8.0  , 6.0  , 5.0  , 3.0  , 5.0  , 3.0  , 8.0  , 7.0  , 6.0  , 10.0  , 9.0  , 12.0  , 13.0  , 10.0  , 7.0  , 10.0 );\n                    decisions[0] = 0.722587775297\n                                   + kernels[1] * 3.35855e-07\n                                   + kernels[2] * 1.64612e-07\n                                   + kernels[4] * 6.00056e-07\n                                   + kernels[5] * 3.5195e-08\n                                   + kernels[7] * -4.2079e-08\n                                   + kernels[8] * -4.2843e-08\n                                   + kernels[9] * -9.994e-09\n                                   + kernels[10] * -5.11065e-07\n                                   + kernels[11] * -5.979e-09\n                                   + kernels[12] * -4.4672e-08\n                                   + kernels[13] * -1.5606e-08\n                                   + kernels[14] * -1.2941e-08\n                                   + kernels[15] * -2.18903e-07\n                                   + kernels[17] * -2.31635e-07\n                            ;\n                    decisions[1] = -1.658344586719\n                                   + kernels[0] * 2.45018e-07\n                                   + kernels[1] * 4.30223e-07\n                                   + kernels[3] * 1.00277e-07\n                                   + kernels[4] * 2.16524e-07\n                                   + kernels[18] * -4.81187e-07\n                                   + kernels[20] * -5.10856e-07\n                            ;\n                    decisions[2] = -1.968607562265\n                                   + kernels[0] * 3.001833e-06\n                                   + kernels[3] * 4.5201e-08\n                                   + kernels[4] * 1.54493e-06\n                                   + kernels[5] * 2.81834e-07\n                                   + kernels[25] * -5.93581e-07\n                                   + kernels[26] * -2.89779e-07\n                                   + kernels[27] * -1.73958e-06\n                                   + kernels[28] * -1.09552e-07\n                                   + kernels[30] * -3.09126e-07\n                                   + kernels[31] * -1.294219e-06\n                                   + kernels[32] * -5.37961e-07\n                            ;\n                    decisions[3] = -0.720663029823\n                                   + kernels[6] * 1.4362e-08\n                                   + kernels[7] * 6.177e-09\n                                   + kernels[9] * 1.25e-08\n                                   + kernels[10] * 2.05478e-07\n                                   + kernels[12] * 2.501e-08\n                                   + kernels[15] * 4.363e-07\n                                   + kernels[16] * 9.147e-09\n                                   + kernels[18] * -1.82182e-07\n                                   + kernels[20] * -4.93707e-07\n                                   + kernels[21] * -3.3084e-08\n                            ;\n                    decisions[4] = -1.605747746589\n                                   + kernels[6] * 6.182e-09\n                                   + kernels[7] * 1.3853e-08\n                                   + kernels[8] * 2.12e-10\n                                   + kernels[9] * 1.1243e-08\n                                   + kernels[10] * 7.80681e-07\n                                   + kernels[15] * 8.347e-07\n                                   + kernels[17] * 1.64985e-07\n                                   + kernels[23] * -4.25014e-07\n                                   + kernels[25] * -1.134803e-06\n                                   + kernels[34] * -2.52038e-07\n                            ;\n                    decisions[5] = -0.934328303475\n                                   + kernels[19] * 3.3529e-07\n                                   + kernels[20] * 1.121946e-06\n                                   + kernels[21] * 3.44683e-07\n                                   + kernels[22] * -6.23056e-07\n                                   + kernels[24] * -1.4612e-07\n                                   + kernels[28] * -1.24025e-07\n                                   + kernels[29] * -4.31701e-07\n                                   + kernels[31] * -9.2146e-08\n                                   + kernels[33] * -3.8487e-07\n                            ;\n                    votes[decisions[0] &gt; 0 ? 0 : 1] += 1;\n                    votes[decisions[1] &gt; 0 ? 0 : 2] += 1;\n                    votes[decisions[2] &gt; 0 ? 0 : 3] += 1;\n                    votes[decisions[3] &gt; 0 ? 1 : 2] += 1;\n                    votes[decisions[4] &gt; 0 ? 1 : 3] += 1;\n                    votes[decisions[5] &gt; 0 ? 2 : 3] += 1;\n                    int val = votes[0];\n                    int idx = 0;\n\n                    for (int i = 1; i &lt; 4; i++) {\n                        if (votes[i] &gt; val) {\n                            val = votes[i];\n                            idx = i;\n                        }\n                    }\n\n                    return idx;\n                }\n\n                /**\n                * Convert class idx to readable name\n                */\n                const char* predictLabel(float *x) {\n                    switch (predict(x)) {\n                        case 0:\n                            return &quot;no&quot;;\n                        case 1:\n                            return &quot;stop&quot;;\n                        case 2:\n                            return &quot;play&quot;;\n                        case 3:\n                            return &quot;yes&quot;;\n                        default:\n                            return &quot;Houston we have a problem&quot;;\n                    }\n                }\n\n            protected:\n                /**\n                * Compute kernel between feature vector and support vector.\n                * Kernel type: poly\n                */\n                float compute_kernel(float *x, ...) {\n                    va_list w;\n                    va_start(w, 64);\n                    float kernel = 0.0;\n\n                    for (uint16_t i = 0; i &lt; 64; i++) {\n                        kernel += x[i] * va_arg(w, double);\n                    }\n\n                    return pow((0.1 * kernel) + 0.0, 2);\n                }\n            };\n        }\n    }\n}</code></pre>\n<p>Done! Deploy the sketch to your board and it should now be able to &quot;understand&quot; what you tell it!</p>\n<p>Here's a quick demo (please forgive me for the bad video quality).</p>\n<div style=\"width: 576px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-1282-1\" width=\"576\" height=\"482\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4?_=1\" /><a href=\"https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4\">https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4</a></video></div>\n<hr />\n<p>Link to the <a href=\"https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/BleSenseWordClassificationExample/BleSenseWordClassificationExample.ino\">Github repo</a></p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/08/better-word-classification-with-arduino-33-ble-sense-and-machine-learning/\">Better word classification with Arduino Nano 33 BLE Sense and Machine Learning</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Let's revamp the post I wrote about word classification using Machine Learning on Arduino, this time using a proper microphone (the MP34DT05 mounted on the Arduino Nano 33 BLE Sense) instead of a chinese, analog one: will the results improve?\nfrom https://www.udemy.com/course/learn-audio-processing-complete-engineers-course/\n\nTable of contentsPulse-density modulation (a.k.a. PDM)Where's the FFT?Machine learning model\nPulse-density modulation (a.k.a. PDM)\nIn the original post, I used an analog microphone to record the audio. It is for sure the easiest way to interact with audio on a microcontroller since you only need to analogRead() the selected pin to get a value from the sensor.\nThis semplicity, however, comes at the cost of a nearly inexistent signal pre-processing from the sensor itself: most of the time, you will get junk - I don't want to be rude, but that's it.\nThe microphone mounted on the Arduino Nano 33 BLE Sense (the MP34DT05), gives you access to modulated signal much more suitable for our processing needs.\nThe modulation used is Pulse-density: I won't try to explain you how this works since I'm not an expert in DSP and neither it is the main scope of this article (refer to Wikipedia for some more information).\nWhat matters to us is that we can grab an array of bytes from the microphone and extract its Root Mean Square (a.k.a. RMS) to be used as a feature for our Machine Learning model.\nI had some difficulty finding examples on how to access the microphone on the Arduino Nano 33 BLE Sense board: fortunately, there's a Github repo from DelaGia that shows how to access all the sensors of the board.\nI extracted the microphone part and incapsulated it in an easy to use class, so you don't really need to dig into the implementation details if you're not interested.\nThe following code extract highlights the steps you need to access the microphone recordings.\n// you&#039;ll find this file in the Github repo\n#include &quot;Mic.h&quot;\n\n// tune these constants to your needs\n#define SAMPLES 64\n#define SOUND_THRESHOLD 1500\n\nfloat features[SAMPLES];\nMic mic;\n\n/**\n *\n */\nvoid setup() {\n    Serial.begin(115200);\n    // the PDM library is asynchronous, so we need a callback\n    PDM.onReceive(onAudio);\n    mic.begin();\n}\n\n/**\n * PDM callback to update mic object\n */\nvoid onAudio() {\n    mic.update();\n}\n\n/**\n * Read given number of samples from mic\n */\nbool record() {\n    if (mic.hasData() &amp;&amp; mic.pop() &gt; SOUND_THRESHOLD) {\n        for (int i = 0; i &lt; SAMPLES; i++) {\n            while (!mic.hasData()) ; // wait for data\n\n            features[i] = mic.pop();\n        }\n\n        return true;\n    }\n\n    return false;\n}\nThe things to note in the above code are:\n\nthe PDM library works asynchronously: it won't block your main code waiting for data, but will report you back when data is ready calling the callback you define. In this case, the callback just instructs the mic object to update\nsince the PDM is asynchronous, when acquiring data you will first check if it's available: this is the case for while (!mic.hasData()) ;, which loops awaiting for new data\n\nNow that we have the acquisition logic in place, it's time for you to record some samples of the words you want to classify. I advise you to record many samples for each word, varying both the distance of your mounth from the mic and the intensity with which you speak: this will produce a more robust classification model later on.\nAs always, save each word in a different file, one feature vector per row.\nHere I report an extract of the features I recorded for some words (scaled and converted to int).\n// word &quot;yes&quot;\n43,47,41,48,60,68,67,54,40,29,17,9,6,6,10,15,10,10,8,8,8,5,7,7,12,6,8,11,7,5,3,3,13,14,15,17,15,16,10,10,16,13,8,9,14,8,0,3,3,3,8,5,3,8,11,10,12,10,11,13,8,6,5,12\n34,47,50,51,60,67,65,63,54,42,30,19,25,22,28,7,6,5,7,7,5,8,6,3,3,3,6,7,3,0,0,0,3,5,5,3,5,3,3,3,0,6,3,0,0,5,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,3,0,5\n33,41,45,47,53,61,64,59,50,41,31,17,8,9,7,9,9,7,8,8,8,10,13,8,8,7,5,5,5,3,0,0,0,0,3,8,8,3,0,3,5,5,5,7,0,0,0,0,0,5,0,3,3,5,0,0,0,5,3,0,5,7,0,9\n38,44,48,55,67,74,66,65,60,48,38,19,12,12,8,8,6,7,5,6,6,6,3,3,3,3,5,3,5,3,6,5,0,3,5,0,0,0,3,0,5,3,0,3,0,0,5,6,3,5,3,5,0,0,0,5,3,0,0,0,0,0,0,0\n\n// word &quot;no&quot;\n35,47,49,55,63,65,68,64,60,59,58,41,29,22,14,8,7,5,8,6,0,3,0,0,3,3,0,3,0,0,0,5,3,0,0,0,0,0,0,0,0,0,5,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,7,5,5,3\n42,51,55,59,68,70,73,70,71,67,67,52,52,29,16,9,6,9,3,3,5,8,8,11,10,7,8,5,3,3,0,0,0,0,0,3,3,3,6,7,7,8,9,6,8,8,7,8,6,8,8,6,8,7,6,9,5,3,5,6,5,3,3,3\n56,68,78,91,84,84,84,74,69,64,57,44,33,18,12,8,5,9,15,12,12,9,12,7,3,10,12,6,3,0,0,0,0,6,3,6,10,10,8,3,9,9,9,8,9,9,11,3,8,9,8,8,8,6,7,3,3,8,5,3,0,3,0,0\n31,42,62,69,74,81,78,78,79,74,74,68,54,44,26,15,8,8,21,10,15,12,11,8,6,5,0,0,0,0,3,5,8,3,9,5,6,9,10,6,10,12,7,15,14,5,8,7,8,9,5,8,5,6,5,0,3,6,3,5,3,5,0,0\n\n// word &quot;play&quot;\n31,195,106,60,45,44,55,51,42,36,36,33,38,37,37,29,24,21,20,20,18,19,14,10,14,12,18,11,8,11,10,12,27,8,10,6,7,6,10,6,6,6,8,6,6,3,5,6,0,0,0,0,0,7,5,3,0,5,0,0,8,8,7,5\n138,158,83,64,44,34,36,41,44,39,35,29,27,26,28,33,33,30,23,19,22,18,16,15,15,13,13,14,9,10,9,7,14,8,3,9,0,0,0,3,5,3,7,7,5,0,0,0,5,0,0,3,5,0,0,0,0,10,3,3,7,7,8,3\n89,36,27,30,22,24,38,41,37,33,32,32,32,35,34,30,25,19,18,18,15,16,14,14,12,9,8,10,9,7,3,0,0,8,7,7,6,3,5,9,8,5,7,3,5,7,6,6,3,0,3,3,0,8,8,5,6,0,0,6,0,7,8,6\n71,98,51,31,27,35,48,50,45,38,34,30,31,30,36,35,28,21,20,16,15,16,16,14,12,10,12,10,8,8,6,7,5,7,12,6,10,10,10,8,8,0,0,8,7,0,5,6,3,0,5,3,0,0,3,5,8,6,5,3,10,3,6,0\n\n// word &quot;stop&quot;\n61,93,135,157,140,148,128,92,85,64,160,75,23,28,28,22,14,8,0,3,3,5,3,6,12,202,154,100,102,94,40,54,52,40,35,22,20,19,11,18,9,7,10,12,8,7,5,7,12,14,16,12,8,6,5,11,5,7,9,10,12,21,20,16\n62,84,110,111,113,99,79,70,60,95,71,22,14,19,16,16,6,12,9,7,3,0,3,6,184,209,116,50,33,49,47,45,32,28,21,16,13,13,3,0,0,8,21,14,10,19,19,15,16,15,22,14,14,14,17,14,12,10,3,9,17,11,10,12\n94,139,168,178,165,143,100,82,128,169,34,22,26,26,22,13,8,8,3,0,3,0,5,202,191,123,119,119,64,31,71,39,35,30,19,16,14,8,19,7,8,7,6,11,18,17,13,20,24,27,29,22,23,22,25,23,21,23,21,18,17,14,14,13\n121,159,181,165,170,154,134,99,75,121,51,8,27,14,18,10,7,3,5,0,5,3,6,11,152,118,160,115,68,42,29,36,53,35,38,30,26,22,19,12,7,6,0,0,0,6,11,15,17,19,18,24,22,24,23,16,15,20,22,12,10,16,21,14\nWhere's the FFT?\nIf you noticed, I didn't mention the Fast Fourier Transform until now.\nWhy?\nWell, it is believed (at least, I believed) that the Fourier Transform is the go-to transformation when working with audio recordings: it can extract frequency-related features useful to extract information from raw signals.\nFor this project, I tried not to use it and I was surprised that it worked fine in spite of everything. Since I like to keep things as lean as possible, I won't apply any complex transformation to the samples.\nIf you're having poor results, however, remember you can try to apply FFT and see if it helps improve the accuracy.\nMachine learning model\nNow that we have the samples, it's time to train the classifier.\nThis step is the same as in any other tutorial I wrote so far, so I won't spend much time on this part. Among the classifiers I tried, SVM produced the best accuracy at 96% with 32 support vectors: it's not a super-tiny model, but it's quite small nevertheless.\nimport numpy as np\nfrom os.path import basename\nfrom glob import glob\nfrom sklearn.svm import SVC\nfrom micromlgen import port\n\ndef load_features(folder):\n    dataset = None\n    classmap = {}\n    for class_idx, filename in enumerate(glob(&#039;%s/*.csv&#039; % folder)):\n        class_name = basename(filename)[:-4]\n        classmap[class_idx] = class_name\n        samples = np.loadtxt(filename, dtype=float, delimiter=&#039;,&#039;)\n        labels = np.ones((len(samples), 1)) * class_idx\n        samples = np.hstack((samples, labels))\n        dataset = samples if dataset is None else np.vstack((dataset, samples))\n    return dataset, classmap\n\ndataset, classmap = load_features(&#039;64&#039;)\nX, y = dataset[:, :-1], dataset[:, -1]\nclf = SVC(kernel=&#039;poly&#039;, degree=2, gamma=0.1, C=100)\nclf.fit(X_train, y_train)\nprint(port(clf, classmap=classmap))\n// The produced ouput for my SVM model\n\n#pragma once\nnamespace Eloquent {\n    namespace ML {\n        namespace Port {\n            class SVM {\n            public:\n                /**\n                * Predict class for features vector\n                */\n                int predict(float *x) {\n                    float kernels[35] = { 0 };\n                    float decisions[6] = { 0 };\n                    int votes[4] = { 0 };\n                    kernels[0] = compute_kernel(x,   33.0  , 41.0  , 47.0  , 54.0  , 59.0  , 61.0  , 56.0  , 51.0  , 50.0  , 51.0  , 44.0  , 32.0  , 23.0  , 15.0  , 12.0  , 8.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[1] = compute_kernel(x,   40.0  , 50.0  , 51.0  , 60.0  , 56.0  , 57.0  , 58.0  , 53.0  , 50.0  , 45.0  , 42.0  , 34.0  , 23.0  , 16.0  , 10.0  , 7.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 14.0  , 3.0  , 8.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 3.0 );\n                    kernels[2] = compute_kernel(x,   56.0  , 68.0  , 78.0  , 91.0  , 84.0  , 84.0  , 84.0  , 74.0  , 69.0  , 64.0  , 57.0  , 44.0  , 33.0  , 18.0  , 12.0  , 8.0  , 5.0  , 9.0  , 15.0  , 12.0  , 12.0  , 9.0  , 12.0  , 7.0  , 3.0  , 10.0  , 12.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 6.0  , 3.0  , 6.0  , 10.0  , 10.0  , 8.0  , 3.0  , 9.0  , 9.0  , 9.0  , 8.0  , 9.0  , 9.0  , 11.0  , 3.0  , 8.0  , 9.0  , 8.0  , 8.0  , 8.0  , 6.0  , 7.0  , 3.0  , 3.0  , 8.0  , 5.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0 );\n                    kernels[3] = compute_kernel(x,   33.0  , 18.0  , 26.0  , 39.0  , 46.0  , 60.0  , 66.0  , 72.0  , 82.0  , 76.0  , 82.0  , 77.0  , 78.0  , 79.0  , 76.0  , 73.0  , 63.0  , 41.0  , 33.0  , 21.0  , 13.0  , 7.0  , 3.0  , 10.0  , 3.0  , 5.0  , 6.0  , 21.0  , 21.0  , 14.0  , 5.0  , 8.0  , 5.0  , 5.0  , 0.0  , 8.0  , 8.0  , 3.0  , 0.0  , 3.0  , 3.0  , 5.0  , 0.0  , 3.0  , 8.0  , 7.0  , 6.0  , 7.0  , 8.0  , 9.0  , 9.0  , 8.0  , 8.0  , 7.0  , 17.0  , 3.0  , 3.0  , 6.0  , 6.0  , 5.0  , 3.0  , 6.0  , 6.0  , 3.0 );\n                    kernels[4] = compute_kernel(x,   54.0  , 57.0  , 62.0  , 58.0  , 61.0  , 61.0  , 59.0  , 58.0  , 57.0  , 51.0  , 34.0  , 25.0  , 18.0  , 10.0  , 6.0  , 6.0  , 10.0  , 7.0  , 5.0  , 10.0  , 5.0  , 7.0  , 8.0  , 6.0  , 5.0  , 5.0  , 5.0  , 7.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 6.0  , 0.0  , 7.0  , 0.0  , 5.0  , 6.0  , 0.0  , 0.0  , 6.0  , 7.0  , 5.0  , 3.0  , 5.0  , 6.0  , 0.0  , 0.0  , 0.0  , 12.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[5] = compute_kernel(x,   49.0  , 58.0  , 68.0  , 69.0  , 72.0  , 72.0  , 75.0  , 76.0  , 73.0  , 59.0  , 59.0  , 36.0  , 19.0  , 12.0  , 12.0  , 17.0  , 12.0  , 21.0  , 9.0  , 6.0  , 8.0  , 6.0  , 7.0  , 15.0  , 14.0  , 14.0  , 10.0  , 8.0  , 5.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 7.0  , 10.0  , 7.0  , 6.0  , 8.0  , 12.0  , 13.0  , 15.0  , 11.0  , 16.0  , 15.0  , 13.0  , 7.0  , 6.0  , 12.0  , 5.0  , 8.0  , 8.0  , 12.0  , 8.0  , 0.0  , 3.0  , 7.0  , 10.0  , 9.0  , 8.0  , 5.0 );\n                    kernels[6] = compute_kernel(x,   43.0  , 63.0  , 76.0  , 73.0  , 67.0  , 72.0  , 64.0  , 50.0  , 31.0  , 123.0  , 95.0  , 27.0  , 17.0  , 20.0  , 12.0  , 14.0  , 11.0  , 9.0  , 6.0  , 3.0  , 3.0  , 15.0  , 156.0  , 172.0  , 69.0  , 52.0  , 47.0  , 41.0  , 18.0  , 29.0  , 46.0  , 20.0  , 22.0  , 21.0  , 0.0  , 0.0  , 6.0  , 3.0  , 7.0  , 10.0  , 10.0  , 14.0  , 13.0  , 13.0  , 11.0  , 8.0  , 18.0  , 26.0  , 19.0  , 15.0  , 15.0  , 16.0  , 17.0  , 13.0  , 11.0  , 8.0  , 20.0  , 19.0  , 10.0  , 9.0  , 10.0  , 12.0  , 9.0  , 12.0 );\n                    kernels[7] = compute_kernel(x,   66.0  , 85.0  , 105.0  , 116.0  , 118.0  , 104.0  , 102.0  , 81.0  , 58.0  , 129.0  , 222.0  , 48.0  , 80.0  , 70.0  , 50.0  , 40.0  , 19.0  , 11.0  , 3.0  , 0.0  , 0.0  , 0.0  , 7.0  , 5.0  , 15.0  , 16.0  , 9.0  , 5.0  , 25.0  , 29.0  , 43.0  , 32.0  , 28.0  , 31.0  , 21.0  , 8.0  , 0.0  , 9.0  , 8.0  , 3.0  , 3.0  , 0.0  , 0.0  , 7.0  , 11.0  , 9.0  , 26.0  , 16.0  , 13.0  , 9.0  , 20.0  , 16.0  , 16.0  , 26.0  , 18.0  , 10.0  , 7.0  , 12.0  , 10.0  , 8.0  , 7.0  , 17.0  , 15.0  , 7.0 );\n                    kernels[8] = compute_kernel(x,   64.0  , 77.0  , 90.0  , 92.0  , 93.0  , 93.0  , 83.0  , 58.0  , 112.0  , 80.0  , 34.0  , 13.0  , 16.0  , 10.0  , 5.0  , 3.0  , 7.0  , 0.0  , 0.0  , 0.0  , 8.0  , 10.0  , 50.0  , 101.0  , 44.0  , 41.0  , 16.0  , 23.0  , 19.0  , 6.0  , 13.0  , 19.0  , 25.0  , 15.0  , 8.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 10.0  , 19.0  , 8.0  , 8.0  , 10.0  , 7.0  , 8.0  , 9.0  , 14.0  , 7.0  , 9.0  , 8.0  , 10.0  , 8.0  , 7.0  , 6.0  , 6.0  , 3.0  , 7.0  , 6.0  , 3.0  , 3.0  , 3.0  , 3.0 );\n                    kernels[9] = compute_kernel(x,   33.0  , 28.0  , 20.0  , 9.0  , 5.0  , 7.0  , 7.0  , 13.0  , 130.0  , 183.0  , 212.0  , 197.0  , 190.0  , 167.0  , 151.0  , 110.0  , 83.0  , 54.0  , 67.0  , 20.0  , 23.0  , 24.0  , 16.0  , 12.0  , 7.0  , 0.0  , 3.0  , 9.0  , 0.0  , 3.0  , 5.0  , 10.0  , 79.0  , 109.0  , 80.0  , 75.0  , 38.0  , 38.0  , 29.0  , 26.0  , 29.0  , 27.0  , 26.0  , 27.0  , 22.0  , 22.0  , 15.0  , 6.0  , 0.0  , 3.0  , 12.0  , 18.0  , 21.0  , 24.0  , 27.0  , 27.0  , 25.0  , 26.0  , 25.0  , 25.0  , 27.0  , 25.0  , 22.0  , 19.0 );\n                    kernels[10] = compute_kernel(x,   36.0  , 58.0  , 70.0  , 69.0  , 62.0  , 56.0  , 52.0  , 50.0  , 26.0  , 9.0  , 3.0  , 0.0  , 3.0  , 6.0  , 3.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 7.0  , 6.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 3.0  , 0.0  , 3.0  , 5.0  , 7.0  , 5.0  , 0.0  , 0.0  , 7.0  , 9.0  , 6.0  , 5.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 8.0  , 6.0 );\n                    kernels[11] = compute_kernel(x,   99.0  , 122.0  , 127.0  , 115.0  , 110.0  , 101.0  , 88.0  , 64.0  , 51.0  , 186.0  , 73.0  , 16.0  , 25.0  , 26.0  , 22.0  , 18.0  , 12.0  , 9.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 21.0  , 14.0  , 3.0  , 11.0  , 19.0  , 34.0  , 31.0  , 33.0  , 18.0  , 15.0  , 9.0  , 19.0  , 19.0  , 16.0  , 6.0  , 0.0  , 0.0  , 0.0  , 7.0  , 12.0  , 9.0  , 10.0  , 12.0  , 14.0  , 12.0  , 13.0  , 10.0  , 10.0  , 12.0  , 9.0  , 13.0  , 13.0  , 14.0  , 8.0  , 11.0  , 8.0  , 6.0  , 3.0  , 7.0  , 3.0  , 5.0 );\n                    kernels[12] = compute_kernel(x,   51.0  , 69.0  , 82.0  , 82.0  , 78.0  , 82.0  , 71.0  , 68.0  , 50.0  , 33.0  , 58.0  , 76.0  , 28.0  , 5.0  , 12.0  , 12.0  , 6.0  , 6.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 52.0  , 90.0  , 45.0  , 30.0  , 19.0  , 12.0  , 19.0  , 19.0  , 18.0  , 7.0  , 13.0  , 16.0  , 10.0  , 7.0  , 3.0  , 0.0  , 5.0  , 6.0  , 0.0  , 11.0  , 17.0  , 14.0  , 12.0  , 16.0  , 9.0  , 11.0  , 11.0  , 18.0  , 13.0  , 8.0  , 8.0  , 10.0  , 12.0  , 8.0  , 9.0  , 8.0  , 8.0  , 5.0  , 6.0 );\n                    kernels[13] = compute_kernel(x,   55.0  , 102.0  , 134.0  , 149.0  , 145.0  , 149.0  , 148.0  , 127.0  , 94.0  , 64.0  , 108.0  , 94.0  , 37.0  , 15.0  , 22.0  , 17.0  , 17.0  , 14.0  , 13.0  , 0.0  , 8.0  , 14.0  , 9.0  , 0.0  , 6.0  , 3.0  , 7.0  , 5.0  , 6.0  , 5.0  , 5.0  , 12.0  , 8.0  , 0.0  , 10.0  , 14.0  , 8.0  , 9.0  , 0.0  , 3.0  , 3.0  , 0.0  , 5.0  , 3.0  , 8.0  , 8.0  , 3.0  , 3.0  , 3.0  , 3.0  , 8.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0  , 12.0  , 3.0  , 0.0  , 7.0  , 5.0  , 3.0  , 0.0  , 0.0 );\n                    kernels[14] = compute_kernel(x,   75.0  , 89.0  , 116.0  , 125.0  , 124.0  , 102.0  , 109.0  , 99.0  , 80.0  , 57.0  , 136.0  , 108.0  , 55.0  , 10.0  , 20.0  , 20.0  , 16.0  , 10.0  , 8.0  , 14.0  , 6.0  , 0.0  , 3.0  , 3.0  , 6.0  , 19.0  , 8.0  , 11.0  , 3.0  , 46.0  , 33.0  , 29.0  , 26.0  , 22.0  , 12.0  , 14.0  , 21.0  , 16.0  , 18.0  , 16.0  , 13.0  , 3.0  , 0.0  , 8.0  , 6.0  , 14.0  , 10.0  , 21.0  , 21.0  , 17.0  , 16.0  , 16.0  , 19.0  , 17.0  , 16.0  , 20.0  , 14.0  , 10.0  , 14.0  , 13.0  , 12.0  , 12.0  , 10.0  , 6.0 );\n                    kernels[15] = compute_kernel(x,   43.0  , 57.0  , 63.0  , 60.0  , 64.0  , 57.0  , 55.0  , 35.0  , 18.0  , 14.0  , 13.0  , 5.0  , 3.0  , 10.0  , 5.0  , 3.0  , 3.0  , 0.0  , 7.0  , 3.0  , 5.0  , 5.0  , 0.0  , 23.0  , 8.0  , 3.0  , 3.0  , 17.0  , 6.0  , 13.0  , 10.0  , 5.0  , 0.0  , 10.0  , 7.0  , 9.0  , 8.0  , 13.0  , 6.0  , 7.0  , 5.0  , 6.0  , 3.0  , 3.0  , 5.0  , 8.0  , 3.0  , 3.0  , 5.0  , 5.0  , 5.0  , 7.0  , 8.0  , 6.0  , 6.0  , 7.0  , 6.0  , 6.0  , 5.0  , 5.0  , 5.0  , 5.0  , 5.0  , 3.0 );\n                    kernels[16] = compute_kernel(x,   31.0  , 33.0  , 27.0  , 19.0  , 14.0  , 9.0  , 3.0  , 6.0  , 16.0  , 131.0  , 186.0  , 214.0  , 206.0  , 212.0  , 192.0  , 186.0  , 147.0  , 90.0  , 157.0  , 98.0  , 34.0  , 37.0  , 35.0  , 31.0  , 17.0  , 10.0  , 7.0  , 0.0  , 3.0  , 3.0  , 5.0  , 6.0  , 10.0  , 12.0  , 22.0  , 50.0  , 48.0  , 48.0  , 61.0  , 28.0  , 26.0  , 29.0  , 30.0  , 24.0  , 26.0  , 19.0  , 26.0  , 15.0  , 26.0  , 19.0  , 17.0  , 10.0  , 3.0  , 3.0  , 0.0  , 6.0  , 12.0  , 16.0  , 16.0  , 21.0  , 25.0  , 27.0  , 27.0  , 26.0 );\n                    kernels[17] = compute_kernel(x,   50.0  , 70.0  , 80.0  , 83.0  , 79.0  , 73.0  , 80.0  , 74.0  , 54.0  , 34.0  , 16.0  , 7.0  , 3.0  , 3.0  , 3.0  , 3.0  , 6.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 10.0  , 3.0  , 7.0  , 5.0  , 0.0  , 3.0  , 3.0  , 3.0  , 3.0  , 0.0  , 5.0  , 6.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 7.0  , 3.0  , 0.0  , 3.0  , 0.0 );\n                    kernels[18] = compute_kernel(x,   38.0  , 50.0  , 52.0  , 49.0  , 44.0  , 42.0  , 43.0  , 41.0  , 41.0  , 42.0  , 42.0  , 40.0  , 35.0  , 29.0  , 23.0  , 22.0  , 20.0  , 20.0  , 18.0  , 16.0  , 16.0  , 15.0  , 11.0  , 12.0  , 15.0  , 11.0  , 13.0  , 8.0  , 5.0  , 5.0  , 3.0  , 0.0  , 0.0  , 3.0  , 7.0  , 3.0  , 8.0  , 3.0  , 0.0  , 5.0  , 6.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 6.0  , 3.0  , 8.0  , 5.0  , 7.0  , 5.0  , 8.0  , 8.0  , 7.0  , 6.0  , 10.0  , 11.0  , 3.0  , 5.0 );\n                    kernels[19] = compute_kernel(x,   33.0  , 37.0  , 34.0  , 30.0  , 25.0  , 22.0  , 37.0  , 44.0  , 40.0  , 36.0  , 33.0  , 29.0  , 27.0  , 26.0  , 28.0  , 28.0  , 28.0  , 25.0  , 20.0  , 16.0  , 14.0  , 13.0  , 12.0  , 11.0  , 10.0  , 10.0  , 10.0  , 7.0  , 6.0  , 6.0  , 6.0  , 6.0  , 5.0  , 0.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[20] = compute_kernel(x,   43.0  , 52.0  , 49.0  , 45.0  , 44.0  , 43.0  , 43.0  , 41.0  , 43.0  , 42.0  , 36.0  , 34.0  , 28.0  , 27.0  , 21.0  , 18.0  , 19.0  , 17.0  , 16.0  , 15.0  , 13.0  , 14.0  , 14.0  , 12.0  , 13.0  , 14.0  , 10.0  , 8.0  , 7.0  , 5.0  , 3.0  , 5.0  , 8.0  , 0.0  , 0.0  , 5.0  , 8.0  , 7.0  , 3.0  , 0.0  , 3.0  , 5.0  , 3.0  , 7.0  , 6.0  , 8.0  , 0.0  , 6.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 10.0  , 8.0  , 14.0  , 7.0  , 5.0  , 11.0 );\n                    kernels[21] = compute_kernel(x,   33.0  , 28.0  , 26.0  , 21.0  , 22.0  , 31.0  , 38.0  , 35.0  , 33.0  , 32.0  , 30.0  , 28.0  , 25.0  , 28.0  , 29.0  , 30.0  , 27.0  , 25.0  , 20.0  , 16.0  , 16.0  , 15.0  , 15.0  , 13.0  , 11.0  , 10.0  , 9.0  , 8.0  , 5.0  , 6.0  , 5.0  , 0.0  , 9.0  , 12.0  , 9.0  , 12.0  , 12.0  , 9.0  , 8.0  , 13.0  , 12.0  , 10.0  , 13.0  , 7.0  , 10.0  , 16.0  , 10.0  , 16.0  , 6.0  , 0.0  , 0.0  , 8.0  , 0.0  , 6.0  , 10.0  , 8.0  , 11.0  , 10.0  , 5.0  , 8.0  , 9.0  , 8.0  , 6.0  , 6.0 );\n                    kernels[22] = compute_kernel(x,   40.0  , 49.0  , 48.0  , 45.0  , 48.0  , 47.0  , 51.0  , 55.0  , 52.0  , 43.0  , 35.0  , 30.0  , 16.0  , 8.0  , 6.0  , 8.0  , 8.0  , 7.0  , 7.0  , 9.0  , 10.0  , 8.0  , 9.0  , 8.0  , 6.0  , 8.0  , 6.0  , 5.0  , 10.0  , 7.0  , 3.0  , 3.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 5.0  , 6.0  , 8.0  , 9.0  , 14.0  , 13.0  , 14.0  , 10.0  , 10.0  , 10.0  , 8.0  , 7.0  , 0.0  , 5.0  , 0.0  , 0.0  , 3.0  , 3.0  , 10.0  , 10.0  , 0.0  , 8.0  , 14.0  , 9.0  , 12.0  , 5.0  , 14.0 );\n                    kernels[23] = compute_kernel(x,   37.0  , 42.0  , 57.0  , 68.0  , 69.0  , 75.0  , 74.0  , 67.0  , 51.0  , 38.0  , 26.0  , 13.0  , 6.0  , 13.0  , 15.0  , 14.0  , 11.0  , 10.0  , 8.0  , 12.0  , 9.0  , 8.0  , 5.0  , 8.0  , 13.0  , 8.0  , 3.0  , 3.0  , 0.0  , 0.0  , 9.0  , 9.0  , 5.0  , 7.0  , 6.0  , 3.0  , 3.0  , 5.0  , 5.0  , 7.0  , 3.0  , 6.0  , 3.0  , 6.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 5.0  , 0.0  , 0.0  , 3.0  , 0.0 );\n                    kernels[24] = compute_kernel(x,   42.0  , 39.0  , 41.0  , 45.0  , 46.0  , 49.0  , 46.0  , 43.0  , 35.0  , 28.0  , 18.0  , 11.0  , 7.0  , 10.0  , 8.0  , 10.0  , 8.0  , 8.0  , 7.0  , 8.0  , 8.0  , 8.0  , 8.0  , 8.0  , 6.0  , 5.0  , 6.0  , 5.0  , 6.0  , 5.0  , 0.0  , 3.0  , 5.0  , 6.0  , 0.0  , 5.0  , 3.0  , 8.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 3.0 );\n                    kernels[25] = compute_kernel(x,   45.0  , 43.0  , 45.0  , 48.0  , 56.0  , 54.0  , 54.0  , 44.0  , 35.0  , 25.0  , 19.0  , 8.0  , 6.0  , 5.0  , 6.0  , 5.0  , 5.0  , 6.0  , 5.0  , 6.0  , 6.0  , 6.0  , 5.0  , 5.0  , 3.0  , 3.0  , 5.0  , 5.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 9.0  , 5.0  , 5.0  , 0.0  , 0.0  , 0.0  , 5.0  , 3.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 5.0  , 6.0  , 5.0  , 0.0  , 0.0 );\n                    kernels[26] = compute_kernel(x,   31.0  , 43.0  , 66.0  , 75.0  , 75.0  , 81.0  , 89.0  , 85.0  , 79.0  , 68.0  , 50.0  , 32.0  , 22.0  , 10.0  , 8.0  , 12.0  , 10.0  , 10.0  , 12.0  , 12.0  , 11.0  , 10.0  , 10.0  , 8.0  , 8.0  , 9.0  , 8.0  , 7.0  , 7.0  , 6.0  , 3.0  , 3.0  , 3.0  , 0.0  , 0.0  , 5.0  , 8.0  , 8.0  , 6.0  , 3.0  , 7.0  , 8.0  , 11.0  , 12.0  , 12.0  , 16.0  , 9.0  , 0.0  , 0.0  , 0.0  , 3.0  , 7.0  , 7.0  , 8.0  , 6.0  , 8.0  , 12.0  , 10.0  , 8.0  , 7.0  , 5.0  , 3.0  , 6.0  , 8.0 );\n                    kernels[27] = compute_kernel(x,   33.0  , 40.0  , 41.0  , 41.0  , 43.0  , 48.0  , 49.0  , 49.0  , 47.0  , 36.0  , 27.0  , 18.0  , 9.0  , 3.0  , 5.0  , 3.0  , 5.0  , 5.0  , 5.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 3.0  , 0.0  , 3.0  , 3.0  , 7.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[28] = compute_kernel(x,   30.0  , 47.0  , 72.0  , 81.0  , 81.0  , 79.0  , 85.0  , 88.0  , 85.0  , 86.0  , 64.0  , 45.0  , 27.0  , 15.0  , 13.0  , 14.0  , 11.0  , 18.0  , 20.0  , 24.0  , 18.0  , 17.0  , 20.0  , 15.0  , 19.0  , 14.0  , 10.0  , 10.0  , 8.0  , 6.0  , 6.0  , 3.0  , 3.0  , 0.0  , 6.0  , 7.0  , 18.0  , 15.0  , 11.0  , 12.0  , 19.0  , 20.0  , 10.0  , 8.0  , 0.0  , 0.0  , 0.0  , 6.0  , 6.0  , 9.0  , 10.0  , 9.0  , 12.0  , 12.0  , 10.0  , 12.0  , 12.0  , 3.0  , 6.0  , 7.0  , 8.0  , 10.0  , 8.0  , 5.0 );\n                    kernels[29] = compute_kernel(x,   33.0  , 31.0  , 31.0  , 33.0  , 36.0  , 36.0  , 36.0  , 32.0  , 25.0  , 19.0  , 12.0  , 5.0  , 5.0  , 10.0  , 6.0  , 6.0  , 8.0  , 6.0  , 7.0  , 6.0  , 6.0  , 6.0  , 11.0  , 6.0  , 6.0  , 6.0  , 5.0  , 3.0  , 3.0  , 0.0  , 0.0  , 6.0  , 3.0  , 5.0  , 3.0  , 10.0  , 5.0  , 6.0  , 9.0  , 3.0  , 7.0  , 6.0  , 6.0  , 8.0  , 7.0  , 0.0  , 6.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 6.0  , 6.0  , 0.0  , 5.0  , 3.0  , 3.0  , 5.0  , 8.0  , 7.0  , 0.0 );\n                    kernels[30] = compute_kernel(x,   31.0  , 48.0  , 65.0  , 60.0  , 68.0  , 81.0  , 88.0  , 91.0  , 93.0  , 79.0  , 56.0  , 32.0  , 22.0  , 12.0  , 16.0  , 13.0  , 12.0  , 13.0  , 13.0  , 13.0  , 14.0  , 11.0  , 11.0  , 14.0  , 12.0  , 10.0  , 8.0  , 8.0  , 7.0  , 5.0  , 3.0  , 0.0  , 3.0  , 5.0  , 5.0  , 7.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 6.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 5.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[31] = compute_kernel(x,   34.0  , 47.0  , 50.0  , 51.0  , 60.0  , 67.0  , 65.0  , 63.0  , 54.0  , 42.0  , 30.0  , 19.0  , 25.0  , 22.0  , 28.0  , 7.0  , 6.0  , 5.0  , 7.0  , 7.0  , 5.0  , 8.0  , 6.0  , 3.0  , 3.0  , 3.0  , 6.0  , 7.0  , 3.0  , 0.0  , 0.0  , 0.0  , 3.0  , 5.0  , 5.0  , 3.0  , 5.0  , 3.0  , 3.0  , 3.0  , 0.0  , 6.0  , 3.0  , 0.0  , 0.0  , 5.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 5.0 );\n                    kernels[32] = compute_kernel(x,   34.0  , 48.0  , 58.0  , 55.0  , 59.0  , 69.0  , 77.0  , 75.0  , 75.0  , 69.0  , 56.0  , 43.0  , 22.0  , 13.0  , 10.0  , 6.0  , 9.0  , 15.0  , 11.0  , 10.0  , 8.0  , 8.0  , 7.0  , 3.0  , 6.0  , 5.0  , 5.0  , 7.0  , 6.0  , 6.0  , 7.0  , 3.0  , 5.0  , 11.0  , 5.0  , 7.0  , 6.0  , 8.0  , 11.0  , 8.0  , 16.0  , 9.0  , 7.0  , 8.0  , 6.0  , 3.0  , 6.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 6.0  , 3.0  , 0.0  , 3.0  , 5.0  , 8.0  , 10.0  , 13.0  , 13.0 );\n                    kernels[33] = compute_kernel(x,   32.0  , 31.0  , 33.0  , 33.0  , 35.0  , 37.0  , 34.0  , 28.0  , 25.0  , 15.0  , 10.0  , 5.0  , 6.0  , 5.0  , 7.0  , 8.0  , 6.0  , 6.0  , 7.0  , 8.0  , 5.0  , 6.0  , 5.0  , 7.0  , 6.0  , 5.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 3.0  , 0.0  , 0.0  , 5.0  , 3.0  , 0.0  , 3.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0  , 0.0 );\n                    kernels[34] = compute_kernel(x,   44.0  , 43.0  , 48.0  , 58.0  , 59.0  , 54.0  , 55.0  , 49.0  , 48.0  , 38.0  , 26.0  , 14.0  , 8.0  , 9.0  , 14.0  , 12.0  , 7.0  , 9.0  , 10.0  , 8.0  , 9.0  , 7.0  , 6.0  , 6.0  , 6.0  , 6.0  , 6.0  , 5.0  , 6.0  , 8.0  , 6.0  , 0.0  , 0.0  , 13.0  , 8.0  , 6.0  , 17.0  , 10.0  , 8.0  , 17.0  , 13.0  , 15.0  , 13.0  , 14.0  , 8.0  , 8.0  , 11.0  , 8.0  , 8.0  , 6.0  , 5.0  , 3.0  , 5.0  , 3.0  , 8.0  , 7.0  , 6.0  , 10.0  , 9.0  , 12.0  , 13.0  , 10.0  , 7.0  , 10.0 );\n                    decisions[0] = 0.722587775297\n                                   + kernels[1] * 3.35855e-07\n                                   + kernels[2] * 1.64612e-07\n                                   + kernels[4] * 6.00056e-07\n                                   + kernels[5] * 3.5195e-08\n                                   + kernels[7] * -4.2079e-08\n                                   + kernels[8] * -4.2843e-08\n                                   + kernels[9] * -9.994e-09\n                                   + kernels[10] * -5.11065e-07\n                                   + kernels[11] * -5.979e-09\n                                   + kernels[12] * -4.4672e-08\n                                   + kernels[13] * -1.5606e-08\n                                   + kernels[14] * -1.2941e-08\n                                   + kernels[15] * -2.18903e-07\n                                   + kernels[17] * -2.31635e-07\n                            ;\n                    decisions[1] = -1.658344586719\n                                   + kernels[0] * 2.45018e-07\n                                   + kernels[1] * 4.30223e-07\n                                   + kernels[3] * 1.00277e-07\n                                   + kernels[4] * 2.16524e-07\n                                   + kernels[18] * -4.81187e-07\n                                   + kernels[20] * -5.10856e-07\n                            ;\n                    decisions[2] = -1.968607562265\n                                   + kernels[0] * 3.001833e-06\n                                   + kernels[3] * 4.5201e-08\n                                   + kernels[4] * 1.54493e-06\n                                   + kernels[5] * 2.81834e-07\n                                   + kernels[25] * -5.93581e-07\n                                   + kernels[26] * -2.89779e-07\n                                   + kernels[27] * -1.73958e-06\n                                   + kernels[28] * -1.09552e-07\n                                   + kernels[30] * -3.09126e-07\n                                   + kernels[31] * -1.294219e-06\n                                   + kernels[32] * -5.37961e-07\n                            ;\n                    decisions[3] = -0.720663029823\n                                   + kernels[6] * 1.4362e-08\n                                   + kernels[7] * 6.177e-09\n                                   + kernels[9] * 1.25e-08\n                                   + kernels[10] * 2.05478e-07\n                                   + kernels[12] * 2.501e-08\n                                   + kernels[15] * 4.363e-07\n                                   + kernels[16] * 9.147e-09\n                                   + kernels[18] * -1.82182e-07\n                                   + kernels[20] * -4.93707e-07\n                                   + kernels[21] * -3.3084e-08\n                            ;\n                    decisions[4] = -1.605747746589\n                                   + kernels[6] * 6.182e-09\n                                   + kernels[7] * 1.3853e-08\n                                   + kernels[8] * 2.12e-10\n                                   + kernels[9] * 1.1243e-08\n                                   + kernels[10] * 7.80681e-07\n                                   + kernels[15] * 8.347e-07\n                                   + kernels[17] * 1.64985e-07\n                                   + kernels[23] * -4.25014e-07\n                                   + kernels[25] * -1.134803e-06\n                                   + kernels[34] * -2.52038e-07\n                            ;\n                    decisions[5] = -0.934328303475\n                                   + kernels[19] * 3.3529e-07\n                                   + kernels[20] * 1.121946e-06\n                                   + kernels[21] * 3.44683e-07\n                                   + kernels[22] * -6.23056e-07\n                                   + kernels[24] * -1.4612e-07\n                                   + kernels[28] * -1.24025e-07\n                                   + kernels[29] * -4.31701e-07\n                                   + kernels[31] * -9.2146e-08\n                                   + kernels[33] * -3.8487e-07\n                            ;\n                    votes[decisions[0] &gt; 0 ? 0 : 1] += 1;\n                    votes[decisions[1] &gt; 0 ? 0 : 2] += 1;\n                    votes[decisions[2] &gt; 0 ? 0 : 3] += 1;\n                    votes[decisions[3] &gt; 0 ? 1 : 2] += 1;\n                    votes[decisions[4] &gt; 0 ? 1 : 3] += 1;\n                    votes[decisions[5] &gt; 0 ? 2 : 3] += 1;\n                    int val = votes[0];\n                    int idx = 0;\n\n                    for (int i = 1; i &lt; 4; i++) {\n                        if (votes[i] &gt; val) {\n                            val = votes[i];\n                            idx = i;\n                        }\n                    }\n\n                    return idx;\n                }\n\n                /**\n                * Convert class idx to readable name\n                */\n                const char* predictLabel(float *x) {\n                    switch (predict(x)) {\n                        case 0:\n                            return &quot;no&quot;;\n                        case 1:\n                            return &quot;stop&quot;;\n                        case 2:\n                            return &quot;play&quot;;\n                        case 3:\n                            return &quot;yes&quot;;\n                        default:\n                            return &quot;Houston we have a problem&quot;;\n                    }\n                }\n\n            protected:\n                /**\n                * Compute kernel between feature vector and support vector.\n                * Kernel type: poly\n                */\n                float compute_kernel(float *x, ...) {\n                    va_list w;\n                    va_start(w, 64);\n                    float kernel = 0.0;\n\n                    for (uint16_t i = 0; i &lt; 64; i++) {\n                        kernel += x[i] * va_arg(w, double);\n                    }\n\n                    return pow((0.1 * kernel) + 0.0, 2);\n                }\n            };\n        }\n    }\n}\nDone! Deploy the sketch to your board and it should now be able to &quot;understand&quot; what you tell it!\nHere's a quick demo (please forgive me for the bad video quality).\n\nhttps://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4\n\nLink to the Github repo\nL'articolo Better word classification with Arduino Nano 33 BLE Sense and Machine Learning proviene da Eloquent Arduino Blog.",
            "date_published": "2020-08-24T19:04:57+02:00",
            "date_modified": "2020-08-24T20:27:19+02:00",
            "authors": [
                {
                    "name": "simone",
                    "url": "https://eloquentarduino.github.io/author/simone/",
                    "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "ml",
                "Arduino Machine learning"
            ],
            "attachments": [
                {
                    "url": "https://eloquentarduino.github.io/wp-content/uploads/2020/08/Arduino-Nano-33-BLE-Sense-Word-classification.mp4",
                    "mime_type": "video/mp4",
                    "size_in_bytes": 5594095
                }
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/?p=1225",
            "url": "https://eloquentarduino.github.io/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/",
            "title": "EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino",
            "content_html": "<p>Are you looking for a top-performer classifiers with a minimal amount of parameters to tune? Look no further: Gaussian Naive Bayes is what you're looking for. And thanks to EloquentML you can now port it to your microcontroller.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/08/GaussianNB.png\" alt=\"GaussianNB\" /></p>\n<p><span id=\"more-1225\"></span></p>\n<h2>(Gaussian) Naive Bayes</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\">Naive Bayes</a> classifiers are simple models based on the probability theory that can be used for classification.</p>\n<p>They originate from the assumption of independence among the input variables. Even though this assumption doesn't hold true in the vast majority of the cases, they often perform very good at many classification tasks, so they're quite popular.</p>\n<p>Gaussian Naive Bayes stack another (mostly wrong) assumption: that the variables exhibit a Gaussian probability distribution.</p>\n<p>I (and many others like me) will never understand how it is possible that so many wrong assumptions lead to such good performances!</p>\n<p>Nevertheless, what is important to us is that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\">sklearn implements GaussianNB</a>, so we easily train such a classifier.<br />\nThe most interesting part is that <code>GaussianNB</code> can be tuned with just a single parameter: <code>var_smoothing</code>.</p>\n<p>Don't ask me what it does in theory: in practice you change it and your accuracy can boost. This leads to an easy tuning process that doesn't involves expensive <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">grid search</a>.</p>\n<pre><code class=\"language-python\">import sklearn.datasets as d\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.naive_bayes import GaussianNB\n\ndef pick_best(X_train, X_test, y_train, y_test):\n    best = (None, 0)\n    for var_smoothing in range(-7, 1):\n        clf = GaussianNB(var_smoothing=pow(10, var_smoothing))\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = (y_pred == y_test).sum()\n        if accuracy &gt; best[1]:\n            best = (clf, accuracy)\n    print(&#039;best accuracy&#039;, best[1] / len(y_test))\n    return best[0]\n\niris = d.load_iris()\nX = normalize(iris.data)\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nclf = pick_best(X_train, X_test, y_train, y_test)</code></pre>\n<p>This simple procedure will train a bunch of classifiers with a different <code>var_smoothing</code> factor and pick the best performing one.</p>\n<h2>EloquentML integration</h2>\n<p>Once you have your trained classifier, porting it to C is as easy as always:</p>\n<pre><code class=\"language-python\">from micromlgen import port\n\nclf = pick_best()\nprint(port(clf))</code></pre>\n<p class=\"watchout\">Always remember to run </p>\n<pre><code>pip install --upgrade micromlgen</code></pre>\n</p>\n<p><code>port</code> is a magic method able to port many classifiers: it will automatically detect the proper converter for you.</p>\n<p>What does the exported code looks like?</p>\n<pre><code class=\"language-cpp\">#pragma once\nnamespace Eloquent {\n    namespace ML {\n        namespace Port {\n            class GaussianNB {\n                public:\n                    /**\n                    * Predict class for features vector\n                    */\n                    int predict(float *x) {\n                        float votes[3] = { 0.0f };\n                        float theta[4] = { 0 };\n                        float sigma[4] = { 0 };\n                        theta[0] = 0.801139789889; theta[1] = 0.54726920354; theta[2] = 0.234408773313; theta[3] = 0.039178084094;\n                        sigma[0] = 0.000366881742; sigma[1] = 0.000907992556; sigma[2] = 0.000740960787; sigma[3] = 0.000274925514;\n                        votes[0] = 0.333333333333 - gauss(x, theta, sigma);\n                        theta[0] = 0.748563871324; theta[1] = 0.349390892644; theta[2] = 0.536186138345; theta[3] = 0.166747384117;\n                        sigma[0] = 0.000529727082; sigma[1] = 0.000847956504; sigma[2] = 0.000690057342; sigma[3] = 0.000311828658;\n                        votes[1] = 0.333333333333 - gauss(x, theta, sigma);\n                        theta[0] = 0.704497203305; theta[1] = 0.318862439835; theta[2] = 0.593755956917; theta[3] = 0.217288784452;\n                        sigma[0] = 0.000363782089; sigma[1] = 0.000813846722; sigma[2] = 0.000415475678; sigma[3] = 0.000758478249;\n                        votes[2] = 0.333333333333 - gauss(x, theta, sigma);\n                        // return argmax of votes\n                        uint8_t classIdx = 0;\n                        float maxVotes = votes[0];\n\n                        for (uint8_t i = 1; i &lt; 3; i++) {\n                            if (votes[i] &gt; maxVotes) {\n                                classIdx = i;\n                                maxVotes = votes[i];\n                            }\n                        }\n\n                        return classIdx;\n                    }\n\n                protected:\n                    /**\n                    * Compute gaussian value\n                    */\n                    float gauss(float *x, float *theta, float *sigma) {\n                        float gauss = 0.0f;\n\n                        for (uint16_t i = 0; i &lt; 4; i++) {\n                            gauss += log(sigma[i]);\n                            gauss += pow(x[i] - theta[i], 2) / sigma[i];\n                        }\n\n                        return gauss;\n                    }\n                };\n            }\n        }\n    }</code></pre>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<p>As you can see, we need a couple of &quot;weight vectors&quot;:</p>\n<ul>\n<li><code>theta</code> is the mean of each feature</li>\n<li><code>sigma</code> is the standard deviation</li>\n</ul>\n<p>The computation is quite thin: just a couple of operations; the class with the highest score is then selected.</p>\n<h2>Benchmarks</h2>\n<p>Following there's a recap of a couple benchmarks I run on an Arduino Nano 33 Ble Sense.</p>\n<table>\n<thead>\n<tr>\n<th>Classifier</th>\n<th>Dataset</th>\n<th style=\"text-align: center;\">Flash</th>\n<th style=\"text-align: center;\">RAM</th>\n<th style=\"text-align: center;\">Execution time</th>\n<th style=\"text-align: center;\">Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GaussianNB</td>\n<td>Iris (150x4)</td>\n<td style=\"text-align: center;\">82 kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">65 ms</td>\n<td style=\"text-align: center;\">97%</td>\n</tr>\n<tr>\n<td>LinearSVC</td>\n<td>Iris (150x4)</td>\n<td style=\"text-align: center;\">83 Kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">76 ms</td>\n<td style=\"text-align: center;\">99%</td>\n</tr>\n<tr>\n<td>GaussianNB</td>\n<td>Breast cancer (80x40)</td>\n<td style=\"text-align: center;\">90 Kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">160 ms</td>\n<td style=\"text-align: center;\">77%</td>\n</tr>\n<tr>\n<td>LinearSVC</td>\n<td>Breast cancer (80x40)</td>\n<td style=\"text-align: center;\">112 Kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">378 ms</td>\n<td style=\"text-align: center;\">73%</td>\n</tr>\n<tr>\n<td>GaussianNB</td>\n<td>Wine (100x13)</td>\n<td style=\"text-align: center;\">85 Kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">130 ms</td>\n<td style=\"text-align: center;\">97%</td>\n</tr>\n<tr>\n<td>LinearSVC</td>\n<td>Wine (100x13)</td>\n<td style=\"text-align: center;\">89 Kb</td>\n<td style=\"text-align: center;\">42 Kb</td>\n<td style=\"text-align: center;\">125 ms</td>\n<td style=\"text-align: center;\">99%</td>\n</tr>\n</tbody>\n</table>\n<p>We can see that the accuracy is on par with a linear SVM, reaching up to 97% on some datasets. Its semplicity shines with high-dimensional datasets (breast cancer) where execution time is half of the LinearSVC: I can see this pattern repeating with other real-world, medium-sized datasets.</p>\n<hr />\n<p>This is it, you can find the example project on <a href=\"https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/GaussianNBClassificationExample/GaussianNBClassificationExample.ino\">Github</a>.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/\">EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "Are you looking for a top-performer classifiers with a minimal amount of parameters to tune? Look no further: Gaussian Naive Bayes is what you're looking for. And thanks to EloquentML you can now port it to your microcontroller.\n\n\n(Gaussian) Naive Bayes\nNaive Bayes classifiers are simple models based on the probability theory that can be used for classification.\nThey originate from the assumption of independence among the input variables. Even though this assumption doesn't hold true in the vast majority of the cases, they often perform very good at many classification tasks, so they're quite popular.\nGaussian Naive Bayes stack another (mostly wrong) assumption: that the variables exhibit a Gaussian probability distribution.\nI (and many others like me) will never understand how it is possible that so many wrong assumptions lead to such good performances!\nNevertheless, what is important to us is that sklearn implements GaussianNB, so we easily train such a classifier.\nThe most interesting part is that GaussianNB can be tuned with just a single parameter: var_smoothing.\nDon't ask me what it does in theory: in practice you change it and your accuracy can boost. This leads to an easy tuning process that doesn't involves expensive grid search.\nimport sklearn.datasets as d\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.naive_bayes import GaussianNB\n\ndef pick_best(X_train, X_test, y_train, y_test):\n    best = (None, 0)\n    for var_smoothing in range(-7, 1):\n        clf = GaussianNB(var_smoothing=pow(10, var_smoothing))\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = (y_pred == y_test).sum()\n        if accuracy &gt; best[1]:\n            best = (clf, accuracy)\n    print(&#039;best accuracy&#039;, best[1] / len(y_test))\n    return best[0]\n\niris = d.load_iris()\nX = normalize(iris.data)\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nclf = pick_best(X_train, X_test, y_train, y_test)\nThis simple procedure will train a bunch of classifiers with a different var_smoothing factor and pick the best performing one.\nEloquentML integration\nOnce you have your trained classifier, porting it to C is as easy as always:\nfrom micromlgen import port\n\nclf = pick_best()\nprint(port(clf))\nAlways remember to run \npip install --upgrade micromlgen\n\nport is a magic method able to port many classifiers: it will automatically detect the proper converter for you.\nWhat does the exported code looks like?\n#pragma once\nnamespace Eloquent {\n    namespace ML {\n        namespace Port {\n            class GaussianNB {\n                public:\n                    /**\n                    * Predict class for features vector\n                    */\n                    int predict(float *x) {\n                        float votes[3] = { 0.0f };\n                        float theta[4] = { 0 };\n                        float sigma[4] = { 0 };\n                        theta[0] = 0.801139789889; theta[1] = 0.54726920354; theta[2] = 0.234408773313; theta[3] = 0.039178084094;\n                        sigma[0] = 0.000366881742; sigma[1] = 0.000907992556; sigma[2] = 0.000740960787; sigma[3] = 0.000274925514;\n                        votes[0] = 0.333333333333 - gauss(x, theta, sigma);\n                        theta[0] = 0.748563871324; theta[1] = 0.349390892644; theta[2] = 0.536186138345; theta[3] = 0.166747384117;\n                        sigma[0] = 0.000529727082; sigma[1] = 0.000847956504; sigma[2] = 0.000690057342; sigma[3] = 0.000311828658;\n                        votes[1] = 0.333333333333 - gauss(x, theta, sigma);\n                        theta[0] = 0.704497203305; theta[1] = 0.318862439835; theta[2] = 0.593755956917; theta[3] = 0.217288784452;\n                        sigma[0] = 0.000363782089; sigma[1] = 0.000813846722; sigma[2] = 0.000415475678; sigma[3] = 0.000758478249;\n                        votes[2] = 0.333333333333 - gauss(x, theta, sigma);\n                        // return argmax of votes\n                        uint8_t classIdx = 0;\n                        float maxVotes = votes[0];\n\n                        for (uint8_t i = 1; i &lt; 3; i++) {\n                            if (votes[i] &gt; maxVotes) {\n                                classIdx = i;\n                                maxVotes = votes[i];\n                            }\n                        }\n\n                        return classIdx;\n                    }\n\n                protected:\n                    /**\n                    * Compute gaussian value\n                    */\n                    float gauss(float *x, float *theta, float *sigma) {\n                        float gauss = 0.0f;\n\n                        for (uint16_t i = 0; i &lt; 4; i++) {\n                            gauss += log(sigma[i]);\n                            gauss += pow(x[i] - theta[i], 2) / sigma[i];\n                        }\n\n                        return gauss;\n                    }\n                };\n            }\n        }\n    }\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nAs you can see, we need a couple of &quot;weight vectors&quot;:\n\ntheta is the mean of each feature\nsigma is the standard deviation\n\nThe computation is quite thin: just a couple of operations; the class with the highest score is then selected.\nBenchmarks\nFollowing there's a recap of a couple benchmarks I run on an Arduino Nano 33 Ble Sense.\n\n\n\nClassifier\nDataset\nFlash\nRAM\nExecution time\nAccuracy\n\n\n\n\nGaussianNB\nIris (150x4)\n82 kb\n42 Kb\n65 ms\n97%\n\n\nLinearSVC\nIris (150x4)\n83 Kb\n42 Kb\n76 ms\n99%\n\n\nGaussianNB\nBreast cancer (80x40)\n90 Kb\n42 Kb\n160 ms\n77%\n\n\nLinearSVC\nBreast cancer (80x40)\n112 Kb\n42 Kb\n378 ms\n73%\n\n\nGaussianNB\nWine (100x13)\n85 Kb\n42 Kb\n130 ms\n97%\n\n\nLinearSVC\nWine (100x13)\n89 Kb\n42 Kb\n125 ms\n99%\n\n\n\nWe can see that the accuracy is on par with a linear SVM, reaching up to 97% on some datasets. Its semplicity shines with high-dimensional datasets (breast cancer) where execution time is half of the LinearSVC: I can see this pattern repeating with other real-world, medium-sized datasets.\n\nThis is it, you can find the example project on Github.\nL'articolo EloquentML grows its family of classifiers: Gaussian Naive Bayes on Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-08-02T10:44:36+02:00",
            "date_modified": "2020-08-02T11:36:42+02:00",
            "authors": [
                {
                    "name": "simone",
                    "url": "https://eloquentarduino.github.io/author/simone/",
                    "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "microml",
                "ml",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/?p=1079",
            "url": "https://eloquentarduino.github.io/2020/04/incremental-multiclass-classification-on-microcontrollers-one-vs-one/",
            "title": "Incremental multiclass classification on microcontrollers: One vs One",
            "content_html": "<p>In earlier posts I showed you can run incremental binary classification on your microcontroller with Stochastic Gradient Descent or Passive-Aggressive classifier. Now it is time to upgrade your toolbelt with a new item: One-vs-One multiclass classifier.</p>\n<p><span id=\"more-1079\"></span></p>\n<h2>One vs One</h2>\n<p>Many classifiers are, by nature, binary: they can only distinguish the positive class from the negative one. Many of real-world problems, however, are multiclass: you have 3 or more possible outcomes to distinguish from.</p>\n<p>There are a couple of ways to achieve this:</p>\n<ol>\n<li><strong>One vs All</strong>: if your classifier is able to output a confidence score of its prediction, for N classes you train N classifiers, each able to recognize a single class. During inference, you pick the &quot;most confident&quot; one.</li>\n<li><strong>One vs One</strong>: for N classes, you train N * (N-1) / 2 classifiers, one for each couple of classes. During inference, each classifier makes a prediction and you pick the class with the highest number of votes.</li>\n</ol>\n<p>Since SGD and Passive-Aggressive don't output a confidence score, I implemented the One vs One algorithm to tackle the multiclass classification problem on microcontrollers.</p>\n<p>Actually, One vs One is not a new type of classifier: it is really a &quot;coordinator&quot; class that sorts which samples go to which classifier. You can still choose your own classifier type to use.</p>\n<p>As SGD and Passive-Aggressive, OneVsOne implements the classifier interface, so you will use the well known <code>fitOne</code> and <code>predict</code> methods.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2>Example code</h2>\n<pre><code class=\"language-cpp\">// Esp32 has some problems with min/max\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n// you will actually need only one of SGD or PassiveAggressive\n#include &quot;EloquentSGD.h&quot;\n#include &quot;EloquentPassiveAggressive.h&quot;\n#include &quot;EloquentOneVsOne.h&quot;\n#include &quot;EloquentAccuracyScorer.h&quot;\n// this file defines NUM_FEATURES, NUM_CLASSES, TRAIN_SAMPLES and TEST_SAMPLES\n#include &quot;dataset.h&quot;\n\nusing namespace Eloquent::ML;\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(3000);\n}\n\nvoid loop() {\n  AccuracyScorer scorer;\n  // OneVsOne needs the actual classifier class, the number of features and the number of classes\n  OneVsOne&lt;SGD&lt;FEATURES_DIM&gt;, FEATURES_DIM, NUM_CLASSES&gt; clf;\n\n  // clf.set() propagates the configuration to the actual classifiers\n  // if a parameter does not exists on the classifier, it does nothing\n  // in this example, alpha and momentum refer to SGD, C to Passive-Aggressive\n  clf.set(&quot;alpha&quot;, 1);\n  clf.set(&quot;momentum&quot;, 0.7);\n  clf.set(&quot;C&quot;, 0.1);\n\n  // fit\n  // I noticed that repeating the training a few times over the same dataset increases performance  to a certain extent: if you re-train it too much, performance will decay\n  for (unsigned int i = 0; i &lt; TRAIN_SAMPLES * 5; i++) {\n      clf.fitOne(X_train[i % TRAIN_SAMPLES], y_train[i % TRAIN_SAMPLES]);\n  }\n\n  // predict\n  for (int i = 0; i &lt; TEST_SAMPLES; i++) {\n      int y_true = y_test[i];\n      int y_pred = clf.predict(X_test[i]);\n\n      Serial.print(&quot;Predicted &quot;);\n      Serial.print(y_pred);\n      Serial.print(&quot; vs &quot;);\n      Serial.println(y_true);\n      scorer.scoreOne(y_true, y_pred);\n  }\n\n  Serial.print(&quot;Accuracy = &quot;);\n  Serial.print(scorer.accuracy() * 100);\n  Serial.print(&quot; out of &quot;);\n  Serial.print(scorer.support());\n  Serial.println(&quot; samples&quot;);\n  delay(30000);\n}</code></pre>\n<p>If you refer to the previous posts on <a href=\"/2020/04/stochastic-gradient-descent-on-your-microcontroller/\">SGD</a> and <a href=\"/2020/04/passive-aggressive-classifier-for-embedded-devices/\">Passive-Aggressive</a>, you'll notice that you would be able to replace one with the other and your code will change by <strong>1 single line only</strong>. This let's you experiment to find the best configuration for your project without hassle.</p>\n<h2>Accuracy</h2>\n<p>Well, accuracy vary.</p>\n<p>In my tests, I couldn't get predictable accuracy on all datasets. I couldn't even get acceptable accuracy on the Iris dataset (60% max). But I got 90% accuracy on the Digits dataset from scikit-learn with 6 classes.</p>\n<p>You have to experiment. Try Passive-Aggressive with many <code>C</code> values. If it doesn't work, try SGD with varying <code>momentum</code> and <code>alpha</code>. Try to repeat the training over the dataset 5, 10 times.</p>\n<p>In a next post I'll report my benchmarks so you can see what works for you and what not.<br />\nThis is an emerging field for me, so I will need time to master it.</p>\n<hr />\n<p>As always, you can find the examle on <a href=\"https://github.com/eloquentarduino/EloquentMicroML/blob/master/examples/OvOExample/OvOExample.ino\">Github</a> with a the dataset to experiment with.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/04/incremental-multiclass-classification-on-microcontrollers-one-vs-one/\">Incremental multiclass classification on microcontrollers: One vs One</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In earlier posts I showed you can run incremental binary classification on your microcontroller with Stochastic Gradient Descent or Passive-Aggressive classifier. Now it is time to upgrade your toolbelt with a new item: One-vs-One multiclass classifier.\n\nOne vs One\nMany classifiers are, by nature, binary: they can only distinguish the positive class from the negative one. Many of real-world problems, however, are multiclass: you have 3 or more possible outcomes to distinguish from.\nThere are a couple of ways to achieve this:\n\nOne vs All: if your classifier is able to output a confidence score of its prediction, for N classes you train N classifiers, each able to recognize a single class. During inference, you pick the &quot;most confident&quot; one.\nOne vs One: for N classes, you train N * (N-1) / 2 classifiers, one for each couple of classes. During inference, each classifier makes a prediction and you pick the class with the highest number of votes.\n\nSince SGD and Passive-Aggressive don't output a confidence score, I implemented the One vs One algorithm to tackle the multiclass classification problem on microcontrollers.\nActually, One vs One is not a new type of classifier: it is really a &quot;coordinator&quot; class that sorts which samples go to which classifier. You can still choose your own classifier type to use.\nAs SGD and Passive-Aggressive, OneVsOne implements the classifier interface, so you will use the well known fitOne and predict methods.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nExample code\n// Esp32 has some problems with min/max\n#define min(a, b) (a) &lt; (b) ? (a) : (b)\n#define max(a, b) (a) &gt; (b) ? (a) : (b)\n// you will actually need only one of SGD or PassiveAggressive\n#include &quot;EloquentSGD.h&quot;\n#include &quot;EloquentPassiveAggressive.h&quot;\n#include &quot;EloquentOneVsOne.h&quot;\n#include &quot;EloquentAccuracyScorer.h&quot;\n// this file defines NUM_FEATURES, NUM_CLASSES, TRAIN_SAMPLES and TEST_SAMPLES\n#include &quot;dataset.h&quot;\n\nusing namespace Eloquent::ML;\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(3000);\n}\n\nvoid loop() {\n  AccuracyScorer scorer;\n  // OneVsOne needs the actual classifier class, the number of features and the number of classes\n  OneVsOne&lt;SGD&lt;FEATURES_DIM&gt;, FEATURES_DIM, NUM_CLASSES&gt; clf;\n\n  // clf.set() propagates the configuration to the actual classifiers\n  // if a parameter does not exists on the classifier, it does nothing\n  // in this example, alpha and momentum refer to SGD, C to Passive-Aggressive\n  clf.set(&quot;alpha&quot;, 1);\n  clf.set(&quot;momentum&quot;, 0.7);\n  clf.set(&quot;C&quot;, 0.1);\n\n  // fit\n  // I noticed that repeating the training a few times over the same dataset increases performance  to a certain extent: if you re-train it too much, performance will decay\n  for (unsigned int i = 0; i &lt; TRAIN_SAMPLES * 5; i++) {\n      clf.fitOne(X_train[i % TRAIN_SAMPLES], y_train[i % TRAIN_SAMPLES]);\n  }\n\n  // predict\n  for (int i = 0; i &lt; TEST_SAMPLES; i++) {\n      int y_true = y_test[i];\n      int y_pred = clf.predict(X_test[i]);\n\n      Serial.print(&quot;Predicted &quot;);\n      Serial.print(y_pred);\n      Serial.print(&quot; vs &quot;);\n      Serial.println(y_true);\n      scorer.scoreOne(y_true, y_pred);\n  }\n\n  Serial.print(&quot;Accuracy = &quot;);\n  Serial.print(scorer.accuracy() * 100);\n  Serial.print(&quot; out of &quot;);\n  Serial.print(scorer.support());\n  Serial.println(&quot; samples&quot;);\n  delay(30000);\n}\nIf you refer to the previous posts on SGD and Passive-Aggressive, you'll notice that you would be able to replace one with the other and your code will change by 1 single line only. This let's you experiment to find the best configuration for your project without hassle.\nAccuracy\nWell, accuracy vary.\nIn my tests, I couldn't get predictable accuracy on all datasets. I couldn't even get acceptable accuracy on the Iris dataset (60% max). But I got 90% accuracy on the Digits dataset from scikit-learn with 6 classes.\nYou have to experiment. Try Passive-Aggressive with many C values. If it doesn't work, try SGD with varying momentum and alpha. Try to repeat the training over the dataset 5, 10 times.\nIn a next post I'll report my benchmarks so you can see what works for you and what not.\nThis is an emerging field for me, so I will need time to master it.\n\nAs always, you can find the examle on Github with a the dataset to experiment with.\nL'articolo Incremental multiclass classification on microcontrollers: One vs One proviene da Eloquent Arduino Blog.",
            "date_published": "2020-04-26T10:01:14+02:00",
            "date_modified": "2020-04-26T11:52:29+02:00",
            "authors": [
                {
                    "name": "simone",
                    "url": "https://eloquentarduino.github.io/author/simone/",
                    "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "incremental-learning",
                "microml",
                "ml",
                "Arduino Machine learning"
            ]
        },
        {
            "id": "https://eloquentarduino.github.io/?p=864",
            "url": "https://eloquentarduino.github.io/2020/01/easy-tinyml-on-esp32-and-arduino/",
            "title": "Easy Tensorflow TinyML on ESP32 and Arduino",
            "content_html": "<p>In this post I will show you how to easily deploy your Tensorflow Lite model to an ESP32 using the Arduino IDE <strong>without any compilation stuff</strong>.</p>\n<p><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/01/tf-arduino-esp.png\" alt=\"tf arduino esp\" /></p>\n<p><span id=\"more-864\"></span></p>\n<p>So I finally settled on giving a try to TinyML, which is a way to deploy Tensorflow Lite models to microcontrollers.<br />\nAs a first step, I downloaded the free chapters from <a href=\"https://tinymlbook.com/\">the TinyML book website</a> and rapidly skimmed through them.</p>\n<p>Let me say that, even if it starts from &quot;too beginner&quot; level for me (they explain why you need to use the arrow instead of the point to access a pointer's property), it is a very well written book. They uncover every single aspect you may encounter during your first steps and give a very sound introduction to the general topic of training, validating and testing a dataset on a model.</p>\n<p>If I will go on with this TinyML stuff, I'll probably buy a copy: I strongly recommend you to at least read the free sample.</p>\n<p>Once done reading the 6 chapters, I wanted to try the described tutorial on my ESP32. Sadly, it is not mentioned in the supported boards on the book, so I had to solve it by myself.</p>\n<p>In this post I'm going to make a sort of recap of my learnings about the steps you need to follow to implement TF models to a microcontroller and introduce you to a tiny library I wrote for the purpose of facilitating the deployment in the Arduino IDE: <a href=\"https://github.com/eloquentarduino/EloquentTinyML\">EloquentTinyML</a>.</p>\n<h2>Building our first model</h2>\n<p>First of all, we need a model to deploy.</p>\n<p>The book guides us on building a neural network capable of predicting the sine value of a given number, in the range from 0 to Pi (3.14).</p>\n<p>It's an easy model to get started (the &quot;Hello world&quot; of machine learning, according to the authors), so we'll stick with it.</p>\n<p>I won't go into too much details about generating data and training the classifier, because I suppose you already know that part if you want to port Tensorflow on a microcontroller.</p>\n<p>Here's the code from the book.</p>\n<pre><code class=\"language-python\">import math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef get_model():\n    SAMPLES = 1000\n    np.random.seed(1337)\n    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n    # shuffle and add noise\n    np.random.shuffle(x_values)\n    y_values = np.sin(x_values)\n    y_values += 0.1 * np.random.randn(*y_values.shape)\n\n    # split into train, validation, test\n    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n    # create a NN with 2 layers of 16 neurons\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, activation=&#039;relu&#039;, input_shape=(1,)))\n    model.add(layers.Dense(16, activation=&#039;relu&#039;))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=&#039;rmsprop&#039;, loss=&#039;mse&#039;, metrics=[&#039;mae&#039;])\n    model.fit(x_train, y_train, epochs=200, batch_size=16,\n                        validation_data=(x_validate, y_validate))\n    return model</code></pre>\n<h2>Exporting the model</h2>\n<p>Now that we have a model, we need to convert it into a form ready to be deployed on our microcontroller. This is actually just an array of bytes that the TF interpreter will read to recreate the model.</p>\n<pre><code class=\"language-python\">model = get_model()\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(&quot;sine_model_quantized.tflite&quot;, &quot;wb&quot;).write(tflite_model)</code></pre>\n<p>Then you have to convert to a C array in the command line.</p>\n<pre><code class=\"language-bash\">xxd -i sine_model_quantized.tflite &gt; sine_model_quantized.cc</code></pre>\n<p>This is copy-paste code that hardly would change, so, for ease my development cycle, I wrapped this little snippet in a tiny package you can use: it's called <code>tinymlgen</code>.</p>\n<pre><code class=\"language-bash\">pip install tinymlgen</code></pre>\n<pre><code class=\"language-python\">from tinymlgen import port\n\nmodel = get_model()\nc_code = port(model, pretty_print=True)\nprint(c_code)</code></pre>\n<p>I point you to the <a href=\"https://github.com/eloquentarduino/tinymlgen\">Github repo</a> for a couple more options you can configure. </p>\n<p>Using this package, you don't have to open a terminal and use the <code>xxd</code> program to get a usable result.</p>\n<!-- Begin Mailchimp Signup Form -->\r\n<div id=\"mc_embed_signup\">\r\n<form action=\"https://github.us4.list-manage.com/subscribe/post?u=f0eaedd94d554cf2ee781742a&id=37d3496031\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\r\n    <div id=\"mc_embed_signup_scroll\">\r\n\t<h2 style=\"margin: 0; text-align: center\">Finding this content useful?</h2>\r\n<div class=\"mc-field-group\">\r\n\t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\" placeholder=\"join the monthly newsletter\">\r\n</div>\r\n\t<div id=\"mce-responses\" class=\"clear\">\r\n\t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\r\n\t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\r\n\t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\r\n    <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_f0eaedd94d554cf2ee781742a_37d3496031\" tabindex=\"-1\" value=\"\"></div>\r\n    <div class=\"clear\" style=\"position: relative; top: 8px\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\r\n    </div>\r\n</form>\r\n</div>\r\n\r\n<!--End mc_embed_signup-->\n<h2>Use the model</h2>\n<p>Now it is finally the time we deploy the model on our microcontroller. </p>\n<p>This part can be tricky, actually, if you don't have one of the supported boards in the book (Arduino Nano 33, SparkFun Edge or STM32F746G Discovery kit). </p>\n<p>I tried just setting &quot;ESP32&quot; as my target in the Arduino IDE and I got tons of errors.</p>\n<p>Luckily for us, a man called Wezley Sherman wrote a tutorial on <a href=\"https://towardsdatascience.com/tensorflow-meet-the-esp32-3ac36d7f32c7\">how to get a TinyML project to compile using the PlatformIO environment</a>. He saved me the effort to try to fix all the broken import errors on my own.</p>\n<p>Since I could get the project to compile using PlatformIO (which I don't use in my everyday tinkering), I settled to get the project to compile in the Arduino IDE.</p>\n<p>Fortunately, it was not difficult at all, so I can finally bring you this library that does all the heavy lifting for you.</p>\n<p>Thanks to the library, you won't need to download the full Tensorflow Lite framework and compile it on your own machine: it has been already done for you.</p>\n<p>As an added bonus, I created a wrapper class that incapsulates all the boring repetitive stuff, so you can focus solely on the application logic.</p>\n<p>Install the library from the library manager in the Arduino IDE: search for &quot;EloquentTinyML&quot;, or from <a href=\"https://github.com/eloquentarduino/EloquentTinyML\">Github</a> first.</p>\n<pre><code class=\"language-bash\">git clone https://github.com/eloquentarduino/EloquentTinyML.git</code></pre>\n<hr /><p><em>#EloquentTinyML escapes you from compiling Tensforflow on your own machine</em><br /><a href='https://twitter.com/intent/tweet?url=http%3A%2F%2Feloquent.blog%2F2020%2F01%2Feasy-tinyml-on-esp32-and-arduino%2F&#038;text=%23EloquentTinyML%20escapes%20you%20from%20compiling%20Tensforflow%20on%20your%20own%20machine&#038;via=EloquentArduino&#038;related=EloquentArduino' target='_blank' rel=\"noopener noreferrer\" >Click To Tweet</a><br /><hr />\n<p>Here is an example on how you use it.</p>\n<pre><code class=\"language-cpp\">#include &quot;EloquentTinyML.h&quot;\n// sine_model.h contains the array you exported from the previous step\n// with either xxd or tinymlgen\n#include &quot;sine_model.h&quot;\n\n#define NUMBER_OF_INPUTS 1\n#define NUMBER_OF_OUTPUTS 1\n// in future projects you may need to tweak this value\n// it&#039;s a trial and error process\n#define TENSOR_ARENA_SIZE 2*1024\n\nEloquent::TinyML::TfLite&lt;NUMBER_OF_INPUTS, NUMBER_OF_OUTPUTS, TENSOR_ARENA_SIZE&gt; ml(sine_model);\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    // pick up a random x and predict its sine\n    float x = 3.14 * random(100) / 100;\n    float y = sin(x);\n    float input[1] = { x };\n    float predicted = ml.predict(input);\n\n    Serial.print(&quot;sin(&quot;);\n    Serial.print(x);\n    Serial.print(&quot;) = &quot;);\n    Serial.print(y);\n    Serial.print(&quot;\\t predicted: &quot;);\n    Serial.println(predicted);\n    delay(1000);\n}</code></pre>\n<p>Does it look easy to use? I bet so.</p>\n<p>For simple cases like this example where you have a single output, the <code>predict</code> method returns that output so you can esaily assign it to a variable.</p>\n<p>If this is not the case and you expect multiple output from your model, you have to declare an output array.</p>\n<pre><code class=\"language-cpp\">float input[10] = { ... };\nfloat output[5] = { 0 };\n\nml.predict(input, output);</code></pre>\n<p>You will find the complete code on <a href=\"https://github.com/eloquentarduino/EloquentTinyML/blob/master/examples/SineExample/SineExample.ino\">Github</a>, with the <code>sine_model.h</code> file too.</p>\n<h2>Wrapping up</h2>\n<p>I hoped this post helped you kickstart your next TinyML project on your ESP32.</p>\n<p>It served me as a foundation for the next experiments I'm willing to do on this platform which is really in its early stages, so needs a lot of investigation about its capabilities.</p>\n<p>I plan to do a comparison with my MicroML framework when I get more experience in both, so staty tuned for the upcoming updates.</p>\n<h2>Disclaimer</h2>\n<p>I tested the library on both Ubuntu 18.04 and Windows 10 64 bit: if you are on a different platform and get compiling errors, please let me know in the comments so I can fix them.</p>\n<p>L'articolo <a rel=\"nofollow\" href=\"https://eloquentarduino.github.io/2020/01/easy-tinyml-on-esp32-and-arduino/\">Easy Tensorflow TinyML on ESP32 and Arduino</a> proviene da <a rel=\"nofollow\" href=\"http://eloquentarduino.github.io/\">Eloquent Arduino Blog</a>.</p>\n",
            "content_text": "In this post I will show you how to easily deploy your Tensorflow Lite model to an ESP32 using the Arduino IDE without any compilation stuff.\n\n\nSo I finally settled on giving a try to TinyML, which is a way to deploy Tensorflow Lite models to microcontrollers.\nAs a first step, I downloaded the free chapters from the TinyML book website and rapidly skimmed through them.\nLet me say that, even if it starts from &quot;too beginner&quot; level for me (they explain why you need to use the arrow instead of the point to access a pointer's property), it is a very well written book. They uncover every single aspect you may encounter during your first steps and give a very sound introduction to the general topic of training, validating and testing a dataset on a model.\nIf I will go on with this TinyML stuff, I'll probably buy a copy: I strongly recommend you to at least read the free sample.\nOnce done reading the 6 chapters, I wanted to try the described tutorial on my ESP32. Sadly, it is not mentioned in the supported boards on the book, so I had to solve it by myself.\nIn this post I'm going to make a sort of recap of my learnings about the steps you need to follow to implement TF models to a microcontroller and introduce you to a tiny library I wrote for the purpose of facilitating the deployment in the Arduino IDE: EloquentTinyML.\nBuilding our first model\nFirst of all, we need a model to deploy.\nThe book guides us on building a neural network capable of predicting the sine value of a given number, in the range from 0 to Pi (3.14).\nIt's an easy model to get started (the &quot;Hello world&quot; of machine learning, according to the authors), so we'll stick with it.\nI won't go into too much details about generating data and training the classifier, because I suppose you already know that part if you want to port Tensorflow on a microcontroller.\nHere's the code from the book.\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef get_model():\n    SAMPLES = 1000\n    np.random.seed(1337)\n    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n    # shuffle and add noise\n    np.random.shuffle(x_values)\n    y_values = np.sin(x_values)\n    y_values += 0.1 * np.random.randn(*y_values.shape)\n\n    # split into train, validation, test\n    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n    # create a NN with 2 layers of 16 neurons\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, activation=&#039;relu&#039;, input_shape=(1,)))\n    model.add(layers.Dense(16, activation=&#039;relu&#039;))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=&#039;rmsprop&#039;, loss=&#039;mse&#039;, metrics=[&#039;mae&#039;])\n    model.fit(x_train, y_train, epochs=200, batch_size=16,\n                        validation_data=(x_validate, y_validate))\n    return model\nExporting the model\nNow that we have a model, we need to convert it into a form ready to be deployed on our microcontroller. This is actually just an array of bytes that the TF interpreter will read to recreate the model.\nmodel = get_model()\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = converter.convert()\n\n# Save the model to disk\nopen(&quot;sine_model_quantized.tflite&quot;, &quot;wb&quot;).write(tflite_model)\nThen you have to convert to a C array in the command line.\nxxd -i sine_model_quantized.tflite &gt; sine_model_quantized.cc\nThis is copy-paste code that hardly would change, so, for ease my development cycle, I wrapped this little snippet in a tiny package you can use: it's called tinymlgen.\npip install tinymlgen\nfrom tinymlgen import port\n\nmodel = get_model()\nc_code = port(model, pretty_print=True)\nprint(c_code)\nI point you to the Github repo for a couple more options you can configure. \nUsing this package, you don't have to open a terminal and use the xxd program to get a usable result.\n\r\n\r\n\r\n    \r\n\tFinding this content useful?\r\n\r\n\t\r\n\r\n\t\r\n\t\t\r\n\t\t\r\n\t    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\nUse the model\nNow it is finally the time we deploy the model on our microcontroller. \nThis part can be tricky, actually, if you don't have one of the supported boards in the book (Arduino Nano 33, SparkFun Edge or STM32F746G Discovery kit). \nI tried just setting &quot;ESP32&quot; as my target in the Arduino IDE and I got tons of errors.\nLuckily for us, a man called Wezley Sherman wrote a tutorial on how to get a TinyML project to compile using the PlatformIO environment. He saved me the effort to try to fix all the broken import errors on my own.\nSince I could get the project to compile using PlatformIO (which I don't use in my everyday tinkering), I settled to get the project to compile in the Arduino IDE.\nFortunately, it was not difficult at all, so I can finally bring you this library that does all the heavy lifting for you.\nThanks to the library, you won't need to download the full Tensorflow Lite framework and compile it on your own machine: it has been already done for you.\nAs an added bonus, I created a wrapper class that incapsulates all the boring repetitive stuff, so you can focus solely on the application logic.\nInstall the library from the library manager in the Arduino IDE: search for &quot;EloquentTinyML&quot;, or from Github first.\ngit clone https://github.com/eloquentarduino/EloquentTinyML.git\n#EloquentTinyML escapes you from compiling Tensforflow on your own machineClick To Tweet\nHere is an example on how you use it.\n#include &quot;EloquentTinyML.h&quot;\n// sine_model.h contains the array you exported from the previous step\n// with either xxd or tinymlgen\n#include &quot;sine_model.h&quot;\n\n#define NUMBER_OF_INPUTS 1\n#define NUMBER_OF_OUTPUTS 1\n// in future projects you may need to tweak this value\n// it&#039;s a trial and error process\n#define TENSOR_ARENA_SIZE 2*1024\n\nEloquent::TinyML::TfLite&lt;NUMBER_OF_INPUTS, NUMBER_OF_OUTPUTS, TENSOR_ARENA_SIZE&gt; ml(sine_model);\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    // pick up a random x and predict its sine\n    float x = 3.14 * random(100) / 100;\n    float y = sin(x);\n    float input[1] = { x };\n    float predicted = ml.predict(input);\n\n    Serial.print(&quot;sin(&quot;);\n    Serial.print(x);\n    Serial.print(&quot;) = &quot;);\n    Serial.print(y);\n    Serial.print(&quot;\\t predicted: &quot;);\n    Serial.println(predicted);\n    delay(1000);\n}\nDoes it look easy to use? I bet so.\nFor simple cases like this example where you have a single output, the predict method returns that output so you can esaily assign it to a variable.\nIf this is not the case and you expect multiple output from your model, you have to declare an output array.\nfloat input[10] = { ... };\nfloat output[5] = { 0 };\n\nml.predict(input, output);\nYou will find the complete code on Github, with the sine_model.h file too.\nWrapping up\nI hoped this post helped you kickstart your next TinyML project on your ESP32.\nIt served me as a foundation for the next experiments I'm willing to do on this platform which is really in its early stages, so needs a lot of investigation about its capabilities.\nI plan to do a comparison with my MicroML framework when I get more experience in both, so staty tuned for the upcoming updates.\nDisclaimer\nI tested the library on both Ubuntu 18.04 and Windows 10 64 bit: if you are on a different platform and get compiling errors, please let me know in the comments so I can fix them.\nL'articolo Easy Tensorflow TinyML on ESP32 and Arduino proviene da Eloquent Arduino Blog.",
            "date_published": "2020-01-25T20:36:29+01:00",
            "date_modified": "2020-06-03T19:10:18+02:00",
            "authors": [
                {
                    "name": "simone",
                    "url": "https://eloquentarduino.github.io/author/simone/",
                    "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "simone",
                "url": "https://eloquentarduino.github.io/author/simone/",
                "avatar": "http://1.gravatar.com/avatar/d670eb91ca3b1135f213ffad83cb8de4?s=512&d=mm&r=g"
            },
            "tags": [
                "ml",
                "tinyml",
                "Arduino Machine learning"
            ]
        }
    ]
}