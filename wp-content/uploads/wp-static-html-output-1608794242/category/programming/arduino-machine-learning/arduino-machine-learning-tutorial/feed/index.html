<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Arduino Machine Learning tutorial &#8211; Eloquent Arduino Blog</title>
	<atom:link href="https://eloquentarduino.github.io/category/programming/arduino-machine-learning/arduino-machine-learning-tutorial/feed/" rel="self" type="application/rss+xml" />
	<link>http://eloquentarduino.github.io/</link>
	<description>Machine learning on Arduino, programming &#38; electronics</description>
	<lastBuildDate>Thu, 10 Dec 2020 11:26:23 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.6</generator>
	<item>
		<title>Decision Tree, Random Forest and XGBoost on Arduino</title>
		<link>https://eloquentarduino.github.io/2020/10/decision-tree-random-forest-and-xgboost-on-arduino/</link>
		
		<dc:creator><![CDATA[simone]]></dc:creator>
		<pubDate>Mon, 19 Oct 2020 17:31:02 +0000</pubDate>
				<category><![CDATA[Arduino Machine learning]]></category>
		<category><![CDATA[Arduino Machine Learning tutorial]]></category>
		<category><![CDATA[microml]]></category>
		<category><![CDATA[ml]]></category>
		<guid isPermaLink="false">https://eloquentarduino.github.io/?p=1264</guid>

					<description><![CDATA[<p>You will be surprised by how much accuracy you can achieve in just a few kylobytes of resources: Decision Tree, Random Forest and XGBoost (Extreme Gradient Boosting) are now available on your microcontrollers: highly RAM-optmized implementations for super-fast classification on embedded devices. Decision Tree Decision Tree is without doubt one of the most well-known classification [&#8230;]</p>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/10/decision-tree-random-forest-and-xgboost-on-arduino/">Decision Tree, Random Forest and XGBoost on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>You will be surprised by how much accuracy you can achieve in just a few kylobytes of resources: <strong>Decision Tree, Random Forest and XGBoost (Extreme Gradient Boosting)</strong> are now available on your microcontrollers: highly RAM-optmized implementations for super-fast classification on embedded devices.</p>
<p><img src="https://eloquentarduino.github.io/wp-content/uploads/2020/08/DecisionTree.png" alt="DecisionTree" /></p>
<p><span id="more-1264"></span></p>
<h2>Decision Tree</h2>
<p><strong>Decision Tree</strong> is without doubt one of the most well-known classification algorithms out there. It is so simple to understand that it was probably the first classifier you encountered in any Machine Learning course.</p>
<p>I won't go into the details of how a Decision Tree classifier trains and selects the splits for the input features: here I will explain how a RAM-efficient porting of such a classifier is implemented.</p>
<p>To an introduction visit <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Wikipedia</a>; for a more in-depth guide visit <a href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html">KDNuggets</a>.</p>
<p>Since we're willing to sacrifice program space (a.k.a flash) in favor of memory (a.k.a RAM), because RAM is the most scarce resource in the vast majority of microcontrollers, the smart way to port a Decision Tree classifier from Python to C is &quot;hard-coding&quot; the splits in code, without keeping any reference to them into variables.</p>
<p>Here's what it looks like for a Decision tree that classifies the Iris dataset.</p>
<p>As you can see, we're using <strong>0 bytes of RAM</strong> to get the classification result, since no variable is being allocated. On the other side, the program space will grow almost linearly with the number of splits.</p>
<p>Since program space is often much greater than RAM on microcontrollers, this implementation exploits its abundance to be able to deploy larger models. How much large? It will depend on the flash size available: many new generations board (Arduino Nano 33 BLE Sense, ESP32, ST Nucleus...) have 1 Mb of flash, which will hold tens of thousands of splits. </p>
<h2>Random Forest</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a></strong> is just many Decision Trees joined together in a voting scheme. The core idea is that of <em>&quot;the wisdom of the corwd&quot;</em>, such that if many trees vote for a given class (having being trained on different subsets of the training set), that class is probably the true class.</p>
<p><a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">Towards Data Science</a> has a more detailed guide on Random Forest and how it balances the trees with the<em>bagging</em> tecnique.</p>
<p>As easy as Decision Trees, Random Forest gets the exact same implementation with <strong>0 bytes of RAM</strong> required (it actually needs as many bytes as the number of classes to store the votes, but that's really negligible): it just hard-codes all its composing trees.</p>
<h2>XGBoost (Extreme Gradient Boosting)</h2>
<p>Extreme Gradient Boosting is <em>&quot;Gradient Boosting on steroids&quot;</em> and has gained much attention from the Machine learning community due to its top results in many data competitions.</p>
<ol>
<li>&quot;gradient boosting&quot; refers to the process of chaining a number of trees so that each tree tries to learn from the errors of the previous</li>
<li>&quot;extreme&quot; refers to many software and hardware optimizations that greatly reduce the time it takes to train the model</li>
</ol>
<p>You can read <a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">the original paper about XGBoost here</a>. For a discursive description head to <a href="https://www.kdnuggets.com/2019/05/xgboost-algorithm.html">KDNuggets</a>, if you want some more math refer to <a href="https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5">this blog post on Medium</a>.</p>
<h2>Porting to plain C</h2>
<p>If you followed my earlier posts on <a href="/2020/08/eloquentml-grows-its-family-of-classifiers-gaussian-naive-bayes-on-arduino/">Gaussian Naive Bayes</a>, <a href="https://eloquentarduino.github.io/2020/07/sefr-a-fast-linear-time-classifier-for-ultra-low-power-devices/">SEFR</a>, <a href="/2020/02/even-smaller-machine-learning-models-for-your-mcu/">Relevant Vector Machine</a> and <a href="/2019/11/you-can-run-machine-learning-on-arduino/">Support Vector Machines</a>, you already know how to port these new classifiers.</p>
<p>If you're new, you will need a couple things:</p>
<ol>
<li>install the <a href="https://pypi.org/project/micromlgen/">micromlgen</a> package with </li>
</ol>
<pre><code class="language-bash">pip install micromlgen</code></pre>
<ol start="2">
<li>(optionally, if you want to use Extreme Gradient Boosting) install the <a href="https://pypi.org/project/xgboost/">xgboost</a> package with </li>
</ol>
<pre><code class="language-bash">pip install xgboost</code></pre>
<ol start="3">
<li>use the <code>micromlgen.port</code> function to generate your plain C code</li>
</ol>
<pre><code class="language-python">from micromlgen import port
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

clf = DecisionTreeClassifier()
X, y = load_iris(return_X_y=True)
clf.fit(X, y)
print(port(clf))</code></pre>
<p>You can then copy-past the C code and import it in your sketch.</p>
<h2>Using in the Arduino sketch</h2>
<p>Once you have the classifier code, create a new project named <code>TreeClassifierExample</code> and copy the classifier code into a file named <code>DecisionTree.h</code> (or <code>RandomForest.h</code> or <code>XGBoost.h</code> depending on the model you chose).</p>
<p>The copy the following to the main ino file.</p>
<pre><code class="language-cpp">#include &quot;DecisionTree.h&quot;

Eloquent::ML::Port::DecisionTree clf;

void setup() {
    Serial.begin(115200);
    Serial.println(&quot;Begin&quot;);
}

void loop() {
    float irisSample[4] = {6.2, 2.8, 4.8, 1.8};

    Serial.print(&quot;Predicted label (you should see &#039;2&#039;: &quot;);
    Serial.println(clf.predict(irisSample));
    delay(1000);
}</code></pre>
<h2>Bechmarks</h2>
<p>How do the 3 classifiers compare against each other?</p>
<p>We will evaluate a few keypoints:</p>
<ul>
<li>training time</li>
<li>accuracy</li>
<li>needed RAM</li>
<li>needed Flash</li>
</ul>
<p>for each classifier on a variety of datasets. I will report the results for RAM and Flash on the Arduino Nano old generation, so you should consider more the relative figures than the absolute ones.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th style="text-align: center;">Classifier</th>
<th style="text-align: center;">Training <br />time (s)</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">RAM <br />(bytes)</th>
<th style="text-align: center;">Flash <br />(bytes)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gas Sensor Array Drift Dataset </strong></td>
<td style="text-align: center;">Decision Tree</td>
<td style="text-align: center;">1,6</td>
<td style="text-align: center;">0.781 ± 0.12</td>
<td style="text-align: center;">290</td>
<td style="text-align: center;">5722</td>
</tr>
<tr>
<td>13910 samples x 128 features</td>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.865 ± 0.083</td>
<td style="text-align: center;">290</td>
<td style="text-align: center;">6438</td>
</tr>
<tr>
<td>6 classes</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">18,8</td>
<td style="text-align: center;"><strong>0.878 ± 0.074</strong></td>
<td style="text-align: center;">290</td>
<td style="text-align: center;">6506</td>
</tr>
<tr>
<td><strong>Gesture Phase Segmentation Dataset</strong></td>
<td style="text-align: center;">Decision Tree</td>
<td style="text-align: center;">0,1</td>
<td style="text-align: center;">0.943 ± 0.005</td>
<td style="text-align: center;">290</td>
<td style="text-align: center;">5638</td>
</tr>
<tr>
<td>10000 samples x 19 features</td>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">0,7</td>
<td style="text-align: center;"><strong>0.970 ± 0.004</strong></td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">6466</td>
</tr>
<tr>
<td>5 classes</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">18,9</td>
<td style="text-align: center;">0.969 ± 0.003</td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">6536</td>
</tr>
<tr>
<td><strong>Drive Diagnosis Dataset</strong></td>
<td style="text-align: center;">Decision Tree</td>
<td style="text-align: center;">0,6</td>
<td style="text-align: center;">0.946 ± 0.005</td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">5850</td>
</tr>
<tr>
<td>10000 samples x 48 features</td>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">2,6</td>
<td style="text-align: center;"><strong>0.983 ± 0.003</strong></td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">6526</td>
</tr>
<tr>
<td>11 classes</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">68,9</td>
<td style="text-align: center;">0.977 ± 0.005</td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">6698</td>
</tr>
</tbody>
</table>
<p>* <em>all datasets are taken from the <a href="https://archive.ics.uci.edu/ml//datasets.php">UCI Machine Learning datasets archive</a></em></p>
<p>I'm collecting more data for a complete benchmark, but in the meantime you can see that both Random Forest and XGBoost are on par: if not that XGBoost takes 5 to 25 times longer to train.</p>
<p>I've never used XGBoost, so I may be missing some tuning parameters, but for now Random Forest remains my favourite classifier.</p>
<h2>Code listings</h2>
<pre><code class="language-cpp">// example IRIS dataset classification with Decision Tree
int predict(float *x) {
  if (x[3] &lt;= 0.800000011920929) {
      return 0;
  }
  else {
      if (x[3] &lt;= 1.75) {
          if (x[2] &lt;= 4.950000047683716) {
              if (x[0] &lt;= 5.049999952316284) {
                  return 1;
              }
              else {
                  return 1;
              }
          }
          else {
              return 2;
          }
      }
      else {
          if (x[2] &lt;= 4.950000047683716) {
              return 2;
          }
          else {
              return 2;
          }
      }
  }
}</code></pre>
<pre><code class="language-cpp">// example IRIS dataset classification with Random Forest of 3 trees

int predict(float *x) {
  uint16_t votes[3] = { 0 };

  // tree #1
  if (x[0] &lt;= 5.450000047683716) {
      if (x[1] &lt;= 2.950000047683716) {
          votes[1] += 1;
      }
      else {
          votes[0] += 1;
      }
  }
  else {
      if (x[0] &lt;= 6.049999952316284) {
          if (x[3] &lt;= 1.699999988079071) {
              if (x[2] &lt;= 3.549999952316284) {
                  votes[0] += 1;
              }
              else {
                  votes[1] += 1;
              }
          }
          else {
              votes[2] += 1;
          }
      }
      else {
          if (x[3] &lt;= 1.699999988079071) {
              if (x[3] &lt;= 1.449999988079071) {
                  if (x[0] &lt;= 6.1499998569488525) {
                      votes[1] += 1;
                  }
                  else {
                      votes[1] += 1;
                  }
              }
              else {
                  votes[1] += 1;
              }
          }
          else {
              votes[2] += 1;
          }
      }
  }

  // tree #2
  if (x[0] &lt;= 5.549999952316284) {
      if (x[2] &lt;= 2.449999988079071) {
          votes[0] += 1;
      }
      else {
          if (x[2] &lt;= 3.950000047683716) {
              votes[1] += 1;
          }
          else {
              votes[1] += 1;
          }
      }
  }
  else {
      if (x[3] &lt;= 1.699999988079071) {
          if (x[1] &lt;= 2.649999976158142) {
              if (x[3] &lt;= 1.25) {
                  votes[1] += 1;
              }
              else {
                  votes[1] += 1;
              }
          }
          else {
              if (x[2] &lt;= 4.1499998569488525) {
                  votes[1] += 1;
              }
              else {
                  if (x[0] &lt;= 6.75) {
                      votes[1] += 1;
                  }
                  else {
                      votes[1] += 1;
                  }
              }
          }
      }
      else {
          if (x[0] &lt;= 6.0) {
              votes[2] += 1;
          }
          else {
              votes[2] += 1;
          }
      }
  }

  // tree #3
  if (x[3] &lt;= 1.75) {
      if (x[2] &lt;= 2.449999988079071) {
          votes[0] += 1;
      }
      else {
          if (x[2] &lt;= 4.8500001430511475) {
              if (x[0] &lt;= 5.299999952316284) {
                  votes[1] += 1;
              }
              else {
                  votes[1] += 1;
              }
          }
          else {
              votes[1] += 1;
          }
      }
  }
  else {
      if (x[0] &lt;= 5.950000047683716) {
          votes[2] += 1;
      }
      else {
          votes[2] += 1;
      }
  }

  // return argmax of votes
  uint8_t classIdx = 0;
  float maxVotes = votes[0];

  for (uint8_t i = 1; i &lt; 3; i++) {
      if (votes[i] &gt; maxVotes) {
          classIdx = i;
          maxVotes = votes[i];
      }
  }

  return classIdx;
}</code></pre>
<p>L'articolo <a rel="nofollow" href="https://eloquentarduino.github.io/2020/10/decision-tree-random-forest-and-xgboost-on-arduino/">Decision Tree, Random Forest and XGBoost on Arduino</a> proviene da <a rel="nofollow" href="http://eloquentarduino.github.io/">Eloquent Arduino Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
